# complete_enhanced_lba.py - å®Œæ•´ç‰ˆå¢å¼·LBAåˆ†æå™¨ï¼Œè‡ªå‹•éæ¿¾ä½å“è³ªæ•¸æ“š

import numpy as np
import pandas as pd
import pymc as pm
import arviz as az
import time
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

@dataclass
class LBAConfig:
    """LBAæ¨¡å‹é…ç½®é¡"""
    # å›ºå®šåƒæ•¸
    threshold: float = 0.8
    start_var: float = 0.2
    ndt: float = 0.15
    noise: float = 0.3
    
    # MCMCé…ç½®
    draws: int = 600
    tune: int = 1000
    chains: int = 4
    cores: int = 1
    target_accept: float = 0.95
    max_treedepth: int = 12
    
    # æ•¸æ“šéæ¿¾é…ç½®
    min_accuracy: float = 0.5      # æœ€ä½æ­£ç¢ºç‡é–€æª» 50%
    min_trials: int = 50           # æœ€ä½è©¦é©—æ•¸
    max_rt: float = 2.5           # æœ€å¤§åæ‡‰æ™‚é–“
    min_rt: float = 0.2           # æœ€å°åæ‡‰æ™‚é–“
    
    def get_mcmc_config(self) -> Dict:
        return {
            'draws': self.draws,
            'tune': self.tune,
            'chains': self.chains,
            'cores': self.cores,
            'target_accept': self.target_accept,
            'max_treedepth': self.max_treedepth,
            'init': 'adapt_diag',
            'progressbar': True,
            'return_inferencedata': True,
            'random_seed': [42, 43, 44, 45]
        }

class DataFilter:
    """æ•¸æ“šéæ¿¾å™¨"""
    
    def __init__(self, config: LBAConfig):
        self.config = config
        self.filtered_subjects = []
    
    def filter_subjects(self, df: pd.DataFrame) -> Tuple[List[int], List[Dict]]:
        """éæ¿¾å—è©¦è€…ï¼Œè¿”å›åˆæ ¼çš„å—è©¦è€…IDåˆ—è¡¨å’Œéæ¿¾è©³æƒ…"""
        
        print(f"ğŸ” é–‹å§‹æ•¸æ“šéæ¿¾...")
        print(f"   éæ¿¾æ¢ä»¶: æ­£ç¢ºç‡â‰¥{self.config.min_accuracy:.0%}, è©¦é©—æ•¸â‰¥{self.config.min_trials}")
        
        valid_subjects = []
        filtered_details = []
        
        for subject_id in df['participant'].unique():
            subject_df = df[df['participant'] == subject_id].copy()
            
            # è¨ˆç®—åŸºæœ¬çµ±è¨ˆ
            n_trials = len(subject_df)
            
            # è¨ˆç®—æ­£ç¢ºç‡
            accuracy = self._calculate_accuracy(subject_df)
            
            # æª¢æŸ¥RTç¯„åœ
            valid_rt_ratio = self._check_rt_range(subject_df)
            
            # æ‡‰ç”¨éæ¿¾è¦å‰‡
            filter_result = self._apply_filters(subject_id, n_trials, accuracy, valid_rt_ratio)
            
            if filter_result['passed']:
                valid_subjects.append(subject_id)
                print(f"   âœ… å—è©¦è€… {subject_id}: {n_trials} trials, æ­£ç¢ºç‡ {accuracy:.1%}")
            else:
                filtered_details.append(filter_result)
                print(f"   âŒ å—è©¦è€… {subject_id}: {filter_result['reason']}")
        
        print(f"\nğŸ“Š éæ¿¾çµæœ:")
        print(f"   ç¸½å—è©¦è€…: {df['participant'].nunique()}")
        print(f"   åˆæ ¼å—è©¦è€…: {len(valid_subjects)}")
        print(f"   éæ¿¾å—è©¦è€…: {len(filtered_details)}")
        
        return valid_subjects, filtered_details
    
    def _calculate_accuracy(self, subject_df: pd.DataFrame) -> float:
        """è¨ˆç®—å—è©¦è€…æ•´é«”æ­£ç¢ºç‡"""
        
        # æ˜ å°„åˆºæ¿€å’Œé¸æ“‡
        stimulus_map = {0: [1, 0], 1: [1, 1], 2: [0, 0], 3: [0, 1]}
        choice_map = {0: [1, 0], 1: [1, 1], 2: [0, 0], 3: [0, 1]}
        
        correct_count = 0
        total_count = len(subject_df)
        
        for _, row in subject_df.iterrows():
            stimulus = int(row['Stimulus'])
            choice = int(row['Response'])
            
            stim_left, stim_right = stimulus_map[stimulus]
            choice_left, choice_right = choice_map[choice]
            
            # å…©é‚Šéƒ½å°æ‰ç®—æ­£ç¢º
            if stim_left == choice_left and stim_right == choice_right:
                correct_count += 1
        
        return correct_count / total_count if total_count > 0 else 0.0
    
    def _check_rt_range(self, subject_df: pd.DataFrame) -> float:
        """æª¢æŸ¥RTç¯„åœçš„æœ‰æ•ˆæ€§"""
        
        rt_values = subject_df['RT'].values
        valid_rt = np.sum((rt_values >= self.config.min_rt) & (rt_values <= self.config.max_rt))
        return valid_rt / len(rt_values) if len(rt_values) > 0 else 0.0
    
    def _apply_filters(self, subject_id: int, n_trials: int, accuracy: float, valid_rt_ratio: float) -> Dict:
        """æ‡‰ç”¨éæ¿¾è¦å‰‡"""
        
        # æª¢æŸ¥æ­£ç¢ºç‡
        if accuracy < self.config.min_accuracy:
            return {
                'subject_id': subject_id,
                'passed': False,
                'reason': f'æ­£ç¢ºç‡éä½ ({accuracy:.1%} < {self.config.min_accuracy:.0%})',
                'accuracy': accuracy,
                'n_trials': n_trials
            }
        
        # æª¢æŸ¥è©¦é©—æ•¸
        if n_trials < self.config.min_trials:
            return {
                'subject_id': subject_id,
                'passed': False,
                'reason': f'è©¦é©—æ•¸ä¸è¶³ ({n_trials} < {self.config.min_trials})',
                'accuracy': accuracy,
                'n_trials': n_trials
            }
        
        # æª¢æŸ¥RTç¯„åœ
        if valid_rt_ratio < 0.9:  # è‡³å°‘90%çš„RTåœ¨åˆç†ç¯„åœå…§
            return {
                'subject_id': subject_id,
                'passed': False,
                'reason': f'RTç¯„åœç•°å¸¸ (æœ‰æ•ˆæ¯”ä¾‹: {valid_rt_ratio:.1%})',
                'accuracy': accuracy,
                'n_trials': n_trials
            }
        
        return {
            'subject_id': subject_id,
            'passed': True,
            'reason': 'é€šéæ‰€æœ‰éæ¿¾æ¢ä»¶',
            'accuracy': accuracy,
            'n_trials': n_trials
        }

class SimpleLBAModel:
    """ç°¡åŒ–çš„LBAæ¨¡å‹ï¼Œå°ˆæ³¨æ–¼ç©©å®šæ€§"""
    
    def __init__(self, config: LBAConfig, model_type: str = 'minimal'):
        self.config = config
        self.model_type = model_type
        self._model = None
    
    def build_model(self, data: Dict) -> pm.Model:
        """æ§‹å»ºPyMCæ¨¡å‹"""
        
        if self._model is not None:
            return self._model
        
        print(f"ğŸ”§ æ§‹å»º {self.model_type} æ¨¡å‹")
        
        with pm.Model() as model:
            if self.model_type == 'minimal':
                params = self._build_minimal_params()
            else:  # constrained
                params = self._build_constrained_params()
            
            # è¨ˆç®—ä¼¼ç„¶
            left_ll = self._compute_lba_likelihood(
                data['left_choices'], data['left_stimuli'], data['rt'],
                params['left_drift_match'], params['left_drift_mismatch']
            )
            
            right_ll = self._compute_lba_likelihood(
                data['right_choices'], data['right_stimuli'], data['rt'],
                params['right_drift_match'], params['right_drift_mismatch']
            )
            
            pm.Potential('left_likelihood', left_ll)
            pm.Potential('right_likelihood', right_ll)
        
        self._model = model
        return model
    
    def _build_minimal_params(self) -> Dict:
        """æ§‹å»ºæœ€å°åƒæ•¸é›†"""
        
        left_drift_match = pm.Gamma('left_drift_match', alpha=2.0, beta=1.5)
        left_drift_mismatch = pm.Gamma('left_drift_mismatch', alpha=1.5, beta=3.0)
        right_drift_match = pm.Gamma('right_drift_match', alpha=2.0, beta=1.5)
        right_drift_mismatch = pm.Gamma('right_drift_mismatch', alpha=1.5, beta=3.0)
        
        # è»Ÿç´„æŸ
        pm.Potential('left_ordering', 
            pm.math.log(1 + pm.math.exp(3.0 * (left_drift_match - left_drift_mismatch - 0.2))))
        pm.Potential('right_ordering',
            pm.math.log(1 + pm.math.exp(3.0 * (right_drift_match - right_drift_mismatch - 0.2))))
        
        return {
            'left_drift_match': left_drift_match,
            'left_drift_mismatch': left_drift_mismatch,
            'right_drift_match': right_drift_match,
            'right_drift_mismatch': right_drift_mismatch
        }
    
    def _build_constrained_params(self) -> Dict:
        """æ§‹å»ºå¼·ç´„æŸåƒæ•¸"""
        
        # å°æ•¸ç©ºé–“åƒæ•¸
        log_left_match = pm.Normal('log_left_match', mu=0.4, sigma=0.4)
        log_left_mismatch = pm.Normal('log_left_mismatch', mu=-0.6, sigma=0.3)
        log_right_match = pm.Normal('log_right_match', mu=0.4, sigma=0.4)
        log_right_mismatch = pm.Normal('log_right_mismatch', mu=-0.6, sigma=0.3)
        
        # è®Šæ›åˆ°æ­£å€¼ä¸¦å¼·åˆ¶é †åº
        left_drift_mismatch_base = pm.math.exp(log_left_mismatch)
        right_drift_mismatch_base = pm.math.exp(log_right_mismatch)
        
        left_drift_match_base = left_drift_mismatch_base + pm.math.exp(log_left_match) + 0.15
        right_drift_match_base = right_drift_mismatch_base + pm.math.exp(log_right_match) + 0.15
        
        # å°ç¨±æ€§ç´„æŸ
        symmetry_weight = 0.3
        mean_match = (left_drift_match_base + right_drift_match_base) / 2
        mean_mismatch = (left_drift_mismatch_base + right_drift_mismatch_base) / 2
        
        left_drift_match = pm.Deterministic('left_drift_match',
            symmetry_weight * mean_match + (1 - symmetry_weight) * left_drift_match_base)
        left_drift_mismatch = pm.Deterministic('left_drift_mismatch',
            symmetry_weight * mean_mismatch + (1 - symmetry_weight) * left_drift_mismatch_base)
        
        right_drift_match = pm.Deterministic('right_drift_match',
            symmetry_weight * mean_match + (1 - symmetry_weight) * right_drift_match_base)
        right_drift_mismatch = pm.Deterministic('right_drift_mismatch',
            symmetry_weight * mean_mismatch + (1 - symmetry_weight) * right_drift_mismatch_base)
        
        return {
            'left_drift_match': left_drift_match,
            'left_drift_mismatch': left_drift_mismatch,
            'right_drift_match': right_drift_match,
            'right_drift_mismatch': right_drift_mismatch
        }
    
    def _compute_lba_likelihood(self, decisions, stimuli, rt, drift_match, drift_mismatch):
        """è¨ˆç®—LBAä¼¼ç„¶"""
        
        from pytensor.tensor import erf
        
        # å›ºå®šåƒæ•¸
        threshold = self.config.threshold
        start_var = self.config.start_var
        ndt = self.config.ndt
        noise = self.config.noise
        
        # åƒæ•¸å®‰å…¨åŒ–
        drift_match_safe = pm.math.clip(drift_match, 0.12, 6.0)
        drift_mismatch_safe = pm.math.clip(drift_mismatch, 0.08, 4.0)
        
        # æ±ºç­–æ™‚é–“
        decision_time = pm.math.clip(rt - ndt, 0.05, 3.0)
        
        # åŒ¹é…æ€§åˆ¤æ–·
        stimulus_match = pm.math.eq(decisions, stimuli)
        
        # æ¼‚ç§»ç‡åˆ†é…
        v_chosen = pm.math.where(stimulus_match, drift_match_safe, drift_mismatch_safe)
        v_unchosen = pm.math.where(stimulus_match, drift_mismatch_safe, drift_match_safe)
        
        # LBAå¯†åº¦è¨ˆç®—
        sqrt_t = pm.math.sqrt(decision_time)
        
        z1_chosen = pm.math.clip(
            (v_chosen * decision_time - threshold) / (noise * sqrt_t), -4.0, 4.0)
        z2_chosen = pm.math.clip(
            (v_chosen * decision_time - start_var) / (noise * sqrt_t), -4.0, 4.0)
        z1_unchosen = pm.math.clip(
            (v_unchosen * decision_time - threshold) / (noise * sqrt_t), -4.0, 4.0)
        
        # æ­£æ…‹å‡½æ•¸
        def safe_normal_cdf(x):
            x_safe = pm.math.clip(x, -4.0, 4.0)
            return 0.5 * (1 + erf(x_safe / pm.math.sqrt(2)))
        
        def safe_normal_pdf(x):
            x_safe = pm.math.clip(x, -4.0, 4.0)
            return pm.math.exp(-0.5 * x_safe**2) / pm.math.sqrt(2 * np.pi)
        
        # Winnerå¯†åº¦
        chosen_cdf_term = safe_normal_cdf(z1_chosen) - safe_normal_cdf(z2_chosen)
        chosen_pdf_term = (safe_normal_pdf(z1_chosen) - safe_normal_pdf(z2_chosen)) / (noise * sqrt_t)
        chosen_cdf_term = pm.math.maximum(chosen_cdf_term, 1e-8)
        
        chosen_likelihood = pm.math.maximum(
            (v_chosen / start_var) * chosen_cdf_term + chosen_pdf_term / start_var, 1e-8)
        
        # Loserå­˜æ´»
        unchosen_survival = pm.math.maximum(1 - safe_normal_cdf(z1_unchosen), 1e-8)
        
        # è¯åˆä¼¼ç„¶
        joint_likelihood = chosen_likelihood * unchosen_survival
        joint_likelihood = pm.math.maximum(joint_likelihood, 1e-10)
        log_likelihood = pm.math.log(joint_likelihood)
        log_likelihood_safe = pm.math.clip(log_likelihood, -40.0, 8.0)
        
        return pm.math.sum(log_likelihood_safe)
    
    def sample(self, data: Dict, **mcmc_kwargs) -> az.InferenceData:
        """åŸ·è¡ŒMCMCæ¡æ¨£"""
        
        model = self.build_model(data)
        
        mcmc_config = self.config.get_mcmc_config()
        mcmc_config.update(mcmc_kwargs)
        
        with model:
            trace = pm.sample(**mcmc_config)
        
        return trace

class EnhancedLBAAnalyzer:
    """å¢å¼·ç‰ˆLBAåˆ†æå™¨"""
    
    def __init__(self, config: LBAConfig = None):
        self.config = config or LBAConfig()
        self.data_filter = DataFilter(self.config)
        self.models = {}
        
        print(f"âœ… å¢å¼·ç‰ˆLBAåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   è‡ªå‹•éæ¿¾: æ­£ç¢ºç‡â‰¥{self.config.min_accuracy:.0%}")
    
    def prepare_subject_data(self, df: pd.DataFrame, subject_id: int) -> Dict:
        """æº–å‚™å–®å€‹å—è©¦è€…æ•¸æ“š"""
        
        subject_df = df[df['participant'] == subject_id].copy()
        
        # æ•¸æ“šæ˜ å°„
        stimulus_map = {0: [1, 0], 1: [1, 1], 2: [0, 0], 3: [0, 1]}
        choice_map = {0: [1, 0], 1: [1, 1], 2: [0, 0], 3: [0, 1]}
        
        left_stim, right_stim = [], []
        left_choice, right_choice = [], []
        
        for _, row in subject_df.iterrows():
            s, c = int(row['Stimulus']), int(row['Response'])
            left_stim.append(stimulus_map[s][0])
            right_stim.append(stimulus_map[s][1])
            left_choice.append(choice_map[c][0])
            right_choice.append(choice_map[c][1])
        
        return {
            'subject_id': subject_id,
            'n_trials': len(subject_df),
            'rt': subject_df['RT'].values,
            'left_stimuli': np.array(left_stim),
            'right_stimuli': np.array(right_stim),
            'left_choices': np.array(left_choice),
            'right_choices': np.array(right_choice)
        }
    
    def fit_subject(self, data: Dict, model_type: str = 'minimal') -> Dict:
        """æ“¬åˆå–®å€‹å—è©¦è€…"""
        
        print(f"\nğŸ¯ æ“¬åˆå—è©¦è€… {data['subject_id']} - æ¨¡å‹: {model_type}")
        
        # ç²å–æˆ–å‰µå»ºæ¨¡å‹
        model_key = f"{model_type}_{data['subject_id']}"
        if model_key not in self.models:
            self.models[model_key] = SimpleLBAModel(self.config, model_type)
        
        model = self.models[model_key]
        
        start_time = time.time()
        
        try:
            # åŸ·è¡Œæ¡æ¨£
            trace = model.sample(data)
            sampling_time = time.time() - start_time
            
            # æ”¶æ–‚è¨ºæ–·
            convergence = self._diagnose_convergence(trace)
            
            # æå–çµæœ
            results = self._extract_results(trace, data, model_type, convergence, sampling_time)
            
            return results
            
        except Exception as e:
            print(f"   âŒ æ“¬åˆå¤±æ•—: {e}")
            return {
                'success': False,
                'subject_id': data['subject_id'],
                'model_type': model_type,
                'error': str(e)
            }
    
    def batch_analysis(self, df: pd.DataFrame, max_subjects: int = 10, 
                      model_type: str = 'minimal') -> Dict:
        """æ‰¹æ¬¡åˆ†æ - è‡ªå‹•éæ¿¾ä½å“è³ªæ•¸æ“š"""
        
        print(f"\nğŸš€ æ‰¹æ¬¡åˆ†æé–‹å§‹")
        print(f"   æ¨¡å‹é¡å‹: {model_type}")
        print(f"   æœ€å¤§å—è©¦è€…: {max_subjects}")
        
        # ç¬¬ä¸€æ­¥ï¼šéæ¿¾æ•¸æ“š
        valid_subjects, filtered_details = self.data_filter.filter_subjects(df)
        
        if len(valid_subjects) == 0:
            print("âŒ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„å—è©¦è€…")
            return {
                'success': False,
                'error': 'æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„å—è©¦è€…',
                'filtered_details': filtered_details
            }
        
        # ç¬¬äºŒæ­¥ï¼šé¸æ“‡è¦åˆ†æçš„å—è©¦è€…
        selected_subjects = valid_subjects[:max_subjects]
        
        print(f"\nğŸ“Š é–‹å§‹åˆ†æ {len(selected_subjects)} å€‹å—è©¦è€…:")
        print(f"   é¸æ“‡çš„å—è©¦è€…: {selected_subjects}")
        
        # ç¬¬ä¸‰æ­¥ï¼šæ‰¹æ¬¡æ“¬åˆ
        results = []
        successful = 0
        converged = 0
        
        for i, subject_id in enumerate(selected_subjects, 1):
            print(f"\nğŸ“ é€²åº¦ {i}/{len(selected_subjects)}: å—è©¦è€… {subject_id}")
            
            try:
                # æº–å‚™æ•¸æ“š
                data = self.prepare_subject_data(df, subject_id)
                
                # åŸ·è¡Œæ“¬åˆ
                result = self.fit_subject(data, model_type)
                results.append(result)
                
                if result['success']:
                    successful += 1
                    if result['converged']:
                        converged += 1
                        print(f"   âœ… æˆåŠŸæ”¶æ–‚")
                    else:
                        print(f"   âš ï¸ æˆåŠŸä½†æœªå®Œå…¨æ”¶æ–‚")
                
            except Exception as e:
                print(f"   âŒ å—è©¦è€… {subject_id} å¤±æ•—: {e}")
                results.append({
                    'success': False,
                    'subject_id': subject_id,
                    'error': str(e)
                })
        
        # ç”Ÿæˆå ±å‘Š
        return self._generate_batch_report(results, successful, converged, filtered_details)
    
    def _diagnose_convergence(self, trace) -> Dict:
        """æ”¶æ–‚è¨ºæ–·"""
        try:
            summary = az.summary(trace)
            max_rhat = summary['r_hat'].max() if 'r_hat' in summary.columns else np.nan
            min_ess = summary['ess_bulk'].min() if 'ess_bulk' in summary.columns else np.nan
            
            n_divergent = 0
            if hasattr(trace, 'sample_stats') and 'diverging' in trace.sample_stats:
                n_divergent = int(trace.sample_stats.diverging.sum())
            
            converged = (max_rhat < 1.05 and min_ess > 200 and n_divergent == 0)
            
            status = "âœ… æ”¶æ–‚è‰¯å¥½" if converged else "âš ï¸ æ”¶æ–‚è­¦å‘Š"
            print(f"   {status}: RÌ‚={max_rhat:.3f}, ESS={min_ess:.0f}, ç™¼æ•£={n_divergent}")
            
            return {
                'converged': converged,
                'max_rhat': max_rhat,
                'min_ess': min_ess,
                'n_divergent': n_divergent
            }
        except Exception as e:
            return {'converged': False, 'error': str(e)}
    
    def _extract_results(self, trace, data: Dict, model_type: str, 
                        convergence: Dict, sampling_time: float) -> Dict:
        """æå–çµæœ"""
        try:
            summary = az.summary(trace)
            
            # åƒæ•¸ä¼°è¨ˆ
            param_estimates = {}
            for param in ['left_drift_match', 'left_drift_mismatch', 'right_drift_match', 'right_drift_mismatch']:
                if param in summary.index:
                    param_estimates[param] = float(summary.loc[param, 'mean'])
                else:
                    param_estimates[param] = np.nan
            
            # è¡ç”ŸæŒ‡æ¨™
            left_discrimination = param_estimates['left_drift_match'] - param_estimates['left_drift_mismatch']
            right_discrimination = param_estimates['right_drift_match'] - param_estimates['right_drift_mismatch']
            processing_asymmetry = abs(param_estimates['left_drift_match'] - param_estimates['right_drift_match'])
            discrimination_asymmetry = abs(left_discrimination - right_discrimination)
            
            results = {
                'success': True,
                'model_type': model_type,
                'subject_id': data['subject_id'],
                'converged': convergence['converged'],
                'convergence_diagnostics': convergence,
                'sampling_time_minutes': sampling_time / 60,
                'param_estimates': param_estimates,
                'left_discrimination': left_discrimination,
                'right_discrimination': right_discrimination,
                'processing_asymmetry': processing_asymmetry,
                'discrimination_asymmetry': discrimination_asymmetry,
                'symmetry_supported': (processing_asymmetry < 0.3 and discrimination_asymmetry < 0.4)
            }
            
            # æ‰“å°ç°¡è¦çµæœ
            status = "âœ… æ”¶æ–‚" if results['converged'] else "âš ï¸ è­¦å‘Š"
            print(f"   {status}: å·¦è¾¨åˆ¥={left_discrimination:.3f}, å³è¾¨åˆ¥={right_discrimination:.3f}")
            
            return results
            
        except Exception as e:
            print(f"   âŒ çµæœæå–å¤±æ•—: {e}")
            return {'success': False, 'extraction_error': str(e)}
    
    def _generate_batch_report(self, results: List[Dict], successful: int, 
                              converged: int, filtered_details: List[Dict]) -> Dict:
        """ç”Ÿæˆæ‰¹æ¬¡åˆ†æå ±å‘Š"""
        
        print(f"\nğŸ“Š æ‰¹æ¬¡åˆ†æå ±å‘Š")
        print("=" * 50)
        
        total_analyzed = len(results)
        
        print(f"åˆ†æçµ±è¨ˆ:")
        print(f"   åˆ†æå—è©¦è€…: {total_analyzed}")
        print(f"   æˆåŠŸæ“¬åˆ: {successful} ({successful/total_analyzed*100:.1f}%)")
        if successful > 0:
            print(f"   å®Œå…¨æ”¶æ–‚: {converged} ({converged/successful*100:.1f}%)")
        
        print(f"\néæ¿¾çµ±è¨ˆ:")
        print(f"   éæ¿¾å—è©¦è€…: {len(filtered_details)}")
        
        # éæ¿¾åŸå› çµ±è¨ˆ
        filter_reasons = {}
        for detail in filtered_details:
            reason = detail['reason']
            if 'æ­£ç¢ºç‡éä½' in reason:
                filter_reasons['æ­£ç¢ºç‡éä½'] = filter_reasons.get('æ­£ç¢ºç‡éä½', 0) + 1
            elif 'è©¦é©—æ•¸ä¸è¶³' in reason:
                filter_reasons['è©¦é©—æ•¸ä¸è¶³'] = filter_reasons.get('è©¦é©—æ•¸ä¸è¶³', 0) + 1
            elif 'RTç¯„åœç•°å¸¸' in reason:
                filter_reasons['RTç¯„åœç•°å¸¸'] = filter_reasons.get('RTç¯„åœç•°å¸¸', 0) + 1
        
        for reason, count in filter_reasons.items():
            print(f"     {reason}: {count} ä½")
        
        # å°ç¨±æ€§çµ±è¨ˆ
        successful_results = [r for r in results if r.get('success', False)]
        if successful_results:
            symmetry_count = sum(1 for r in successful_results if r.get('symmetry_supported', False))
            print(f"\nå°ç¨±æ€§åˆ†æ:")
            print(f"   æ”¯æŒå°ç¨±æ€§: {symmetry_count}/{len(successful_results)} ({symmetry_count/len(successful_results)*100:.1f}%)")
        
        return {
            'success': True,
            'total_analyzed': total_analyzed,
            'successful': successful,
            'converged': converged,
            'filtered_count': len(filtered_details),
            'filter_reasons': filter_reasons,
            'results': results,
            'filtered_details': filtered_details
        }

def run_enhanced_analysis():
    """é‹è¡Œå¢å¼·ç‰ˆåˆ†æ"""
    
    print("ğŸš€ å¢å¼·ç‰ˆLBAåˆ†æ - è‡ªå‹•éæ¿¾ä½å“è³ªæ•¸æ“š")
    print("=" * 60)
    
    # è©¢å•åƒæ•¸
    csv_file = input("è«‹è¼¸å…¥CSVæª”æ¡ˆè·¯å¾‘ (æˆ–æŒ‰Enterä½¿ç”¨é è¨­ 'GRT_LBA.csv'): ").strip()
    if not csv_file:
        csv_file = 'GRT_LBA.csv'
    
    # è©¢å•éæ¿¾é–€æª»
    min_acc_input = input("æœ€ä½æ­£ç¢ºç‡é–€æª» (æŒ‰Enterä½¿ç”¨50%): ").strip()
    min_accuracy = float(min_acc_input) / 100 if min_acc_input else 0.5
    
    # è©¢å•å—è©¦è€…æ•¸é‡
    max_subjects_input = input("æœ€å¤§åˆ†æå—è©¦è€…æ•¸ (æŒ‰Enterä½¿ç”¨10): ").strip()
    max_subjects = int(max_subjects_input) if max_subjects_input else 10
    
    # è©¢å•æ¨¡å‹é¡å‹
    print("\né¸æ“‡æ¨¡å‹é¡å‹:")
    print("1. minimal - æœ€å°åƒæ•¸é›†")
    print("2. constrained - å¼·ç´„æŸåƒæ•¸åŒ–")
    model_choice = input("è«‹é¸æ“‡ (1-2): ").strip()
    model_type = 'minimal' if model_choice == '1' else 'constrained'
    
    try:
        # è¼‰å…¥æ•¸æ“š
        df = pd.read_csv(csv_file)
        df = df.dropna(subset=['Response', 'RT', 'Stimulus', 'participant'])
        df = df[(df['RT'] >= 0.2) & (df['RT'] <= 2.5)]
        
        print(f"\nâœ… æ•¸æ“šè¼‰å…¥æˆåŠŸ:")
        print(f"   ç¸½è©¦é©—: {len(df)}")
        print(f"   ç¸½å—è©¦è€…: {df['participant'].nunique()}")
        
        # å‰µå»ºåˆ†æå™¨
        config = LBAConfig(
            min_accuracy=min_accuracy,
            draws=500,
            tune=800,
            target_accept=0.93
        )
        analyzer = EnhancedLBAAnalyzer(config)
        
        # åŸ·è¡Œæ‰¹æ¬¡åˆ†æ
        batch_result = analyzer.batch_analysis(df, max_subjects, model_type)
        
        if batch_result['success']:
            print(f"\nğŸ‰ åˆ†æå®Œæˆ!")
            print(f"   éæ¿¾é–€æª»: æ­£ç¢ºç‡â‰¥{min_accuracy:.0%}")
            print(f"   ä½¿ç”¨æ¨¡å‹: {model_type}")
            print(f"   åˆ†æçµæœ: æˆåŠŸ {batch_result['successful']}/{batch_result['total_analyzed']}")
        else:
            print(f"âŒ åˆ†æå¤±æ•—: {batch_result.get('error', 'Unknown error')}")
        
        return batch_result
        
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {csv_file}")
        print("ğŸ’¡ è«‹ç¢ºä¿æª”æ¡ˆè·¯å¾‘æ­£ç¢º")
        return None
    except Exception as e:
        print(f"âŒ åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_single_subject_analysis():
    """é‹è¡Œå–®ä¸€å—è©¦è€…åˆ†æ"""
    
    print("ğŸ¯ å–®ä¸€å—è©¦è€…åˆ†æ")
    print("=" * 40)
    
    # è©¢å•åƒæ•¸
    csv_file = input("è«‹è¼¸å…¥CSVæª”æ¡ˆè·¯å¾‘ (æˆ–æŒ‰Enterä½¿ç”¨é è¨­): ").strip() or 'GRT_LBA.csv'
    subject_input = input("è«‹è¼¸å…¥å—è©¦è€…ID (æˆ–æŒ‰Enterè‡ªå‹•é¸æ“‡): ").strip()
    subject_id = int(subject_input) if subject_input else None
    
    # è©¢å•æ¨¡å‹é¡å‹
    print("é¸æ“‡æ¨¡å‹é¡å‹:")
    print("1. minimal - æœ€å°åƒæ•¸é›†")
    print("2. constrained - å¼·ç´„æŸåƒæ•¸åŒ–")
    model_choice = input("è«‹é¸æ“‡ (1-2): ").strip()
    model_type = 'minimal' if model_choice == '1' else 'constrained'
    
    try:
        # è¼‰å…¥æ•¸æ“š
        df = pd.read_csv(csv_file)
        df = df.dropna(subset=['Response', 'RT', 'Stimulus', 'participant'])
        df = df[(df['RT'] >= 0.2) & (df['RT'] <= 2.5)]
        
        # å‰µå»ºåˆ†æå™¨
        config = LBAConfig()
        analyzer = EnhancedLBAAnalyzer(config)
        
        # å¦‚æœæ²’æœ‰æŒ‡å®šå—è©¦è€…ï¼Œå…ˆéæ¿¾æ•¸æ“šæ‰¾åˆé©çš„
        if subject_id is None:
            valid_subjects, _ = analyzer.data_filter.filter_subjects(df)
            if not valid_subjects:
                print("âŒ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„å—è©¦è€…")
                return None
            subject_id = valid_subjects[0]
            print(f"âœ… è‡ªå‹•é¸æ“‡å—è©¦è€…: {subject_id}")
        
        # æº–å‚™æ•¸æ“š
        data = analyzer.prepare_subject_data(df, subject_id)
        
        # æª¢æŸ¥è©²å—è©¦è€…æ˜¯å¦ç¬¦åˆæ¢ä»¶
        subject_df = df[df['participant'] == subject_id]
        accuracy = analyzer.data_filter._calculate_accuracy(subject_df)
        
        if accuracy < config.min_accuracy:
            print(f"âŒ å—è©¦è€… {subject_id} æ­£ç¢ºç‡éä½: {accuracy:.1%} < {config.min_accuracy:.0%}")
            return None
        
        print(f"âœ… å—è©¦è€… {subject_id} ç¬¦åˆæ¢ä»¶: æ­£ç¢ºç‡ {accuracy:.1%}")
        
        # åŸ·è¡Œåˆ†æ
        result = analyzer.fit_subject(data, model_type)
        
        if result['success']:
            print(f"\nğŸ‰ å–®ä¸€å—è©¦è€…åˆ†æå®Œæˆ!")
            print(f"   æ¨¡å‹é¡å‹: {result['model_type']}")
            print(f"   æ”¶æ–‚ç‹€æ…‹: {'âœ… æ”¶æ–‚' if result['converged'] else 'âš ï¸ è­¦å‘Š'}")
            print(f"   æ¡æ¨£æ™‚é–“: {result['sampling_time_minutes']:.1f} åˆ†é˜")
        else:
            print(f"âŒ åˆ†æå¤±æ•—: {result.get('error', 'Unknown error')}")
        
        return result
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±æ•—: {e}")
        return None

def show_filter_demo():
    """æ¼”ç¤ºéæ¿¾åŠŸèƒ½"""
    
    print("ğŸ“Š æ•¸æ“šéæ¿¾åŠŸèƒ½æ¼”ç¤º")
    print("=" * 40)
    
    csv_file = input("è«‹è¼¸å…¥CSVæª”æ¡ˆè·¯å¾‘ (æˆ–æŒ‰Enterä½¿ç”¨é è¨­): ").strip() or 'GRT_LBA.csv'
    
    try:
        # è¼‰å…¥æ•¸æ“š
        df = pd.read_csv(csv_file)
        df = df.dropna(subset=['Response', 'RT', 'Stimulus', 'participant'])
        df = df[(df['RT'] >= 0.2) & (df['RT'] <= 2.5)]
        
        print(f"âœ… æ•¸æ“šè¼‰å…¥: {len(df)} trials, {df['participant'].nunique()} å—è©¦è€…")
        
        # å‰µå»ºéæ¿¾å™¨
        config = LBAConfig(min_accuracy=0.5)  # 50%æ­£ç¢ºç‡é–€æª»
        data_filter = DataFilter(config)
        
        # åŸ·è¡Œéæ¿¾
        valid_subjects, filtered_details = data_filter.filter_subjects(df)
        
        print(f"\nğŸ“ˆ éæ¿¾çµæœæ‘˜è¦:")
        print(f"   ç¬¦åˆæ¢ä»¶: {len(valid_subjects)} ä½å—è©¦è€…")
        print(f"   è¢«éæ¿¾: {len(filtered_details)} ä½å—è©¦è€…")
        
        # é¡¯ç¤ºè¢«éæ¿¾çš„å—è©¦è€…è©³æƒ…
        if filtered_details:
            print(f"\nâŒ è¢«éæ¿¾çš„å—è©¦è€…è©³æƒ…:")
            for detail in filtered_details[:10]:  # æœ€å¤šé¡¯ç¤º10å€‹
                print(f"   å—è©¦è€… {detail['subject_id']}: {detail['reason']}")
            
            if len(filtered_details) > 10:
                print(f"   ... é‚„æœ‰ {len(filtered_details) - 10} ä½")
        
        # é¡¯ç¤ºç¬¦åˆæ¢ä»¶çš„å—è©¦è€…
        if valid_subjects:
            print(f"\nâœ… ç¬¦åˆæ¢ä»¶çš„å—è©¦è€… (å‰10ä½): {valid_subjects[:10]}")
            if len(valid_subjects) > 10:
                print(f"   ... é‚„æœ‰ {len(valid_subjects) - 10} ä½")
        
        return {'valid_subjects': valid_subjects, 'filtered_details': filtered_details}
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±æ•—: {e}")
        return None

if __name__ == "__main__":
    print("ğŸ¯ å¢å¼·ç‰ˆLBAåˆ†æé¸é …:")
    print("1. æ‰¹æ¬¡åˆ†æ (è‡ªå‹•éæ¿¾ + æ··åˆæ¨¡å‹)")
    print("2. å–®ä¸€å—è©¦è€…åˆ†æ")
    print("3. æ•¸æ“šéæ¿¾åŠŸèƒ½æ¼”ç¤º")
    
    try:
        choice = input("\nè«‹é¸æ“‡ (1-3): ").strip()
        
        if choice == '1':
            result = run_enhanced_analysis()
            
        elif choice == '2':
            result = run_single_subject_analysis()
            
        elif choice == '3':
            result = show_filter_demo()
            
        else:
            print("ç„¡æ•ˆé¸æ“‡")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ åˆ†æè¢«ä¸­æ–·")
    except Exception as e:
        print(f"\nğŸ’¥ éŒ¯èª¤: {e}")

# ============================================================================
# ä½¿ç”¨èªªæ˜
# ============================================================================

"""
ğŸ¯ å¢å¼·ç‰ˆLBAåˆ†æå™¨åŠŸèƒ½èªªæ˜ï¼š

1. **è‡ªå‹•éæ¿¾ä½å“è³ªæ•¸æ“š** âœ…
   - æ­£ç¢ºç‡ < 50% è‡ªå‹•éæ¿¾
   - è©¦é©—æ•¸ < 50 è‡ªå‹•éæ¿¾
   - RTç¯„åœç•°å¸¸è‡ªå‹•éæ¿¾
   - è©³ç´°è¨˜éŒ„éæ¿¾åŸå› 

2. **å…©ç¨®æ¨¡å‹å¯é¸** âœ…
   - minimal: æœ€å°åƒæ•¸é›† (4å€‹æ¼‚ç§»ç‡åƒæ•¸)
   - constrained: å¼·ç´„æŸåƒæ•¸åŒ– (å°æ•¸è®Šæ› + å°ç¨±æ€§ç´„æŸ)

3. **æ‰¹æ¬¡åˆ†æåŠŸèƒ½** âœ…
   - è‡ªå‹•éæ¿¾ä¸¦åˆ†æå¤šå€‹å—è©¦è€…
   - è©³ç´°çš„åˆ†æå ±å‘Š
   - æ”¶æ–‚çµ±è¨ˆå’Œå°ç¨±æ€§åˆ†æ

4. **å–®ä¸€å—è©¦è€…åˆ†æ** âœ…
   - é‡å°ç‰¹å®šå—è©¦è€…çš„è©³ç´°åˆ†æ
   - è‡ªå‹•æª¢æŸ¥æ•¸æ“šå“è³ª

ä½¿ç”¨æµç¨‹ï¼š
1. é¸æ“‡é¸é …1é€²è¡Œæ‰¹æ¬¡åˆ†æ
2. è¼¸å…¥CSVæª”æ¡ˆè·¯å¾‘
3. è¨­å®šæ­£ç¢ºç‡é–€æª» (å»ºè­°50%)
4. é¸æ“‡æ¨¡å‹é¡å‹ (å»ºè­°å…ˆè©¦minimal)
5. æŸ¥çœ‹åˆ†æçµæœ

é æœŸæ”¹å–„ï¼š
- è‡ªå‹•æ’é™¤ä½å“è³ªæ•¸æ“šï¼Œæé«˜æ•´é«”åˆ†æå“è³ª
- å…©ç¨®æ¨¡å‹ç­–ç•¥æ‡‰å°ä¸åŒæ”¶æ–‚æƒ…æ³  
- è©³ç´°å ±å‘Šå¹«åŠ©ç†è§£æ•¸æ“šç‰¹æ€§
"""
