# -*- coding: utf-8 -*-
"""
LBA æ¨¡å‹æ¯”è¼ƒåˆ†æè…³æœ¬ (Coactive vs. Parallel AND)
*** ç‰ˆæœ¬ v9: çµ‚æ¥µä¿®æ­£ç‰ˆï¼Œé‡å¯« random å‡½æ•¸ä»¥è§£æ±ºæœ€çµ‚çš„ PPC å½¢ç‹€éŒ¯èª¤ ***
"""
import pymc as pm
import pytensor.tensor as pt
import numpy as np
import pandas as pd
import arviz as az
import time
import matplotlib.pyplot as plt

# ===================================================================
# Phase 1: LBA çš„æ ¸å¿ƒæ•¸å­¸å¼•æ“ (logp) èˆ‡éš¨æ©Ÿæ¨£æœ¬ç”Ÿæˆå™¨ (random)
# ===================================================================

# --- logp å‡½æ•¸ (ç”¨æ–¼ NUTS æ¡æ¨£) ---
# é€™å€‹å‡½æ•¸å·²ç¶“è¢«è­‰æ˜æ˜¯æ­£ç¢ºçš„ï¼Œä¿æŒä¸è®Š
def logp_lba(value, v_correct, v_incorrect, b, A, t0):
    rt = value[:, 0]; response = value[:, 1]; t = pt.maximum(rt - t0, 0.001)
    def normal_pdf(x, mu=0.0, sigma=1.0): return (1.0 / (sigma * pt.sqrt(2.0 * np.pi))) * pt.exp(-0.5 * ((x - mu) / sigma)**2)
    def normal_cdf(x, mu=0.0, sigma=1.0): return 0.5 * (1 + pt.erf((x - mu) / (sigma * pt.sqrt(2.0))))
    v_chosen = pt.switch(pt.eq(response, 1), v_correct, v_incorrect); v_unchosen = pt.switch(pt.eq(response, 1), v_incorrect, v_correct)
    v_chosen = pt.maximum(v_chosen, 1e-6); v_unchosen = pt.maximum(v_unchosen, 1e-6)
    term1_chosen = (b - A) / v_chosen; term2_chosen = b / v_chosen; term1_unchosen = (b - A) / v_unchosen
    g_chosen = (1/A) * (-v_chosen * normal_cdf(term1_chosen, mu=t, sigma=1) + v_chosen * normal_cdf(term2_chosen, mu=t, sigma=1) + normal_pdf(term1_chosen, mu=t, sigma=1) - normal_pdf(term2_chosen, mu=t, sigma=1))
    S_unchosen = 1 - normal_cdf(term1_unchosen, mu=t, sigma=1)
    joint_likelihood = g_chosen * S_unchosen
    safe_joint_likelihood = pt.maximum(joint_likelihood, 1e-10)
    return pt.sum(pt.log(safe_joint_likelihood))

# --- random å‡½æ•¸ (ç”¨æ–¼äº‹å¾Œé æ¸¬æª¢æŸ¥ PPC) ---
# é€™æ˜¯å…¨æ–°çš„ã€æ›´å¼·å›ºçš„ç‰ˆæœ¬
def lba_random(v_correct, v_incorrect, b, A, t0, rng=None, size=None):
    # 'size' ç”± pm.sample_posterior_predictive å‚³å…¥ï¼Œé€šå¸¸æ˜¯ (n_trials, 2)
    n_trials = size[0]

    # å¼·åˆ¶å°‡å›ºå®šåƒæ•¸è½‰ç‚ºç´”é‡ï¼Œé¿å…å»£æ’­å•é¡Œ
    b_ = np.asarray(b).item()
    A_ = np.asarray(A).item()
    t0_ = np.asarray(t0).item()
    v_incorrect_ = np.asarray(v_incorrect).item()
    
    # v_correct æ˜¯å”¯ä¸€çš„å‘é‡ï¼Œå…¶é•·åº¦æ‡‰ç­‰æ–¼ n_trials
    v_correct_ = np.asarray(v_correct)

    # ç”¨å®‰å…¨çš„æ–¹å¼å»ºç«‹ (n_trials, 2) çš„æ¼‚ç§»ç‡é™£åˆ—
    v = np.empty((n_trials, 2))
    v[:, 0] = v_correct_
    v[:, 1] = v_incorrect_

    # æ¥ä¸‹ä¾†çš„æ¨¡æ“¬é‚è¼¯èˆ‡ä¹‹å‰é¡ä¼¼ï¼Œä½†ä½¿ç”¨ç´”é‡åŒ–çš„åƒæ•¸
    start_points = rng.uniform(low=0, high=A_, size=(n_trials, 2))
    drifts = rng.normal(loc=v, scale=1)
    drifts[drifts < 0] = 1e-10
    
    threshold = np.maximum(b_, A_ + 1e-4)
    time_diff = threshold - start_points
    time_diff[time_diff < 0] = 0
    
    time_to_boundary = time_diff / drifts
    
    winner = 1 - np.argmin(time_to_boundary, axis=1)
    rt = (np.min(time_to_boundary, axis=1) + t0_).flatten()
    
    return np.stack([rt, winner], axis=1)

# ===================================================================
# Phase 2 & 3: æ•¸æ“šæº–å‚™èˆ‡ä¸»åŸ·è¡Œæµç¨‹
# (é€™éƒ¨åˆ†å®Œå…¨ä¸è®Šï¼Œä½†ç¾åœ¨æ‡‰è©²å¯ä»¥é †åˆ©è·‘å®Œ)
# ===================================================================
def prepare_data_for_model(df, subject_id):
    subject_df = df[df['participant'] == subject_id].copy()
    if len(subject_df) == 0: raise ValueError(f"æ‰¾ä¸åˆ°å—è©¦è€… {subject_id} çš„è³‡æ–™")
    stimulus_mapping = {0: {'left': 1, 'right': 0}, 1: {'left': 1, 'right': 1}, 2: {'left': 0, 'right': 0}, 3: {'left': 0, 'right': 1}}
    choice_mapping = {0: {'left': 1, 'right': 0}, 1: {'left': 1, 'right': 1}, 2: {'left': 0, 'right': 0}, 3: {'left': 0, 'right': 1}}
    left_stim_is_diag = subject_df['Stimulus'].map(lambda s: stimulus_mapping[s]['left']).values
    right_stim_is_diag = subject_df['Stimulus'].map(lambda s: stimulus_mapping[s]['right']).values
    left_choice_is_diag = subject_df['Response'].map(lambda r: choice_mapping[r]['left']).values
    right_choice_is_diag = subject_df['Response'].map(lambda r: choice_mapping[r]['right']).values
    left_match = (left_stim_is_diag == 1); right_match = (right_stim_is_diag == 1)
    is_correct = (left_stim_is_diag == left_choice_is_diag) & (right_stim_is_diag == right_choice_is_diag)
    return {"rt": subject_df['RT'].values, "response_correct": is_correct.astype(int), "left_match": left_match.astype(int), "right_match": right_match.astype(int)}

# ===================================================================
# Phase 3: ä¸»åŸ·è¡Œæµç¨‹ (ä¿®æ­£ç‰ˆ)
# ===================================================================
if __name__ == '__main__':
    # --- æ•¸æ“šè¼‰å…¥èˆ‡æº–å‚™ (ä¸è®Š) ---
    try:
        df = pd.read_csv('GRT_LBA.csv'); df = df.dropna(subset=['Response', 'RT', 'Stimulus', 'participant']); df = df[(df['RT'] >= 0.1) & (df['RT'] <= 3.0)]
        print("âœ… æ•¸æ“šè¼‰å…¥æˆåŠŸã€‚")
    except FileNotFoundError:
        print("âŒ éŒ¯èª¤: GRT_LBA.csv æª”æ¡ˆæœªæ‰¾åˆ°ã€‚"); exit()
    # --- æ­¥é©Ÿ 1: åœ¨æœ€æºé ­çš„ DataFrame ä¸Šï¼Œé€²è¡Œæ•¸æ“šç¯©é¸å’Œæ¸…ç† ---
    print("--- æ­¥é©Ÿ 1: æ ¹æ“šåæ‡‰æ™‚é–“é–¾å€¼ï¼Œç¯©é¸åŸå§‹æ•¸æ“š ---")
    rt_threshold = 0.150  # è¨­å®š 150 æ¯«ç§’ç‚ºé–¾å€¼

    # !!! é‡è¦ï¼šè«‹å°‡ 'RT' æ›æˆæ‚¨æ•¸æ“šä¸­åæ‡‰æ™‚é–“æ¬„ä½çš„çœŸå¯¦åç¨± !!!
    # å¯èƒ½æ˜¯ 'rt', 'RT', 'reaction_time' ç­‰ç­‰ï¼Œè«‹å‹™å¿…ç¢ºèª
    rt_column_name = 'RT' 
    
    original_rows = len(df)
    df_cleaned = df[df[rt_column_name] >= rt_threshold].copy()
    print(f"åŸå§‹æ•¸æ“šå…± {original_rows} è¡Œã€‚")
    print(f"ç¯©é¸å¾Œ (RT >= {rt_threshold}s)ï¼Œå‰©ä¸‹ {len(df_cleaned)} è¡Œã€‚")
    print(f"å…±ç§»é™¤äº† {original_rows - len(df_cleaned)} å€‹éå¿«çš„è©¦æ¬¡ã€‚")


    # --- æ­¥é©Ÿ 2: ä½¿ç”¨ã€Œæ¸…ç†å¾Œã€çš„æ•¸æ“šä¾†æº–å‚™æ¨¡å‹è¼¸å…¥ ---
    print("\n--- æ­¥é©Ÿ 2: ä½¿ç”¨æ¸…ç†å¾Œçš„æ•¸æ“šä¾†æº–å‚™æ¨¡å‹è¼¸å…¥ ---")
    SUBJECT_ID_TO_RUN = 47
    print(f"æº–å‚™è¦è·‘çš„å—è©¦è€…: {SUBJECT_ID_TO_RUN}")
    
    # å°‡ df_cleaned å‚³å…¥å‡½æ•¸ï¼Œè€Œä¸æ˜¯åŸå§‹çš„ df
    prepared_data = prepare_data_for_model(df_cleaned, SUBJECT_ID_TO_RUN)
    observed_value = np.column_stack([
    np.asarray(prepared_data['rt'], np.float32),
    np.asarray(prepared_data['response_correct'], np.float32)
])


    print("âœ… æ•¸æ“šæº–å‚™å®Œæˆã€‚")
    B_CONSTANT = 1.0; A_CONSTANT = 0.3; T0_CONSTANT = 0.2
    MCMC_CONFIG = {'draws': 2000, 'tune': 1500, 'chains': 4, 'cores': 1}
    
    print(f"\n--- æ¨¡å‹è¨­å®š ---\nå›ºå®šåƒæ•¸: b={B_CONSTANT}, A={A_CONSTANT}, t0={T0_CONSTANT}\nMCMC é…ç½®: {MCMC_CONFIG}")
    
    # --- Coactive Model ---
    with pm.Model() as model_coactive:
        # ... (æ¨¡å‹å®šç¾©ä¸è®Š) ...
        v_left_match = pm.HalfNormal('v_left_match', sigma=1.0); v_left_mismatch = pm.HalfNormal('v_left_mismatch', sigma=0.5)
        v_right_match = pm.HalfNormal('v_right_match', sigma=1.0); v_right_mismatch = pm.HalfNormal('v_right_mismatch', sigma=0.5)
        v_left = v_left_match * prepared_data['left_match'] + v_left_mismatch * (1 - prepared_data['left_match'])
        v_right = v_right_match * prepared_data['right_match'] + v_right_mismatch * (1 - prepared_data['right_match'])
        v_final_correct = pm.Deterministic('v_final_correct', v_left + v_right); v_final_incorrect = 0.1 
        likelihood_coactive = pm.CustomDist('likelihood', v_final_correct, v_final_incorrect, B_CONSTANT, A_CONSTANT, T0_CONSTANT, logp=logp_lba, random=lba_random, observed=observed_value)
    print("\n--- æ­£åœ¨å»ºç«‹ Coactive æ¨¡å‹ ---"); print("ğŸ”¬ é–‹å§‹æ“¬åˆ Coactive æ¨¡å‹...")
    idata_coactive = pm.sample(model=model_coactive, **MCMC_CONFIG)
    
    # --- é—œéµä¿®æ­£ 1ï¼šè¨ˆç®—ä¸¦å„²å­˜ log_likelihood ---
    with model_coactive:
        pm.compute_log_likelihood(idata_coactive)
        idata_coactive.extend(pm.sample_posterior_predictive(idata_coactive))
    print("âœ… Coactive æ¨¡å‹æ“¬åˆå®Œæˆ")

    # --- Parallel AND Model ---
# --- Parallel AND æ¨¡å‹ (ä½¿ç”¨ LogSumExp å¹³æ»‘è¿‘ä¼¼) ---
    with pm.Model() as model_parallel:
        # ... (v_left_match, v_right_match ç­‰å…ˆé©—çš„å®šç¾©ä¿æŒä¸è®Š) ...
        v_left_match = pm.HalfNormal('v_left_match', sigma=1.0); v_left_mismatch = pm.HalfNormal('v_left_mismatch', sigma=0.5)
        v_right_match = pm.HalfNormal('v_right_match', sigma=1.0); v_right_mismatch = pm.HalfNormal('v_right_mismatch', sigma=0.5)
        v_left = v_left_match * prepared_data['left_match'] + v_left_mismatch * (1 - prepared_data['left_match'])
        v_right = v_right_match * prepared_data['right_match'] + v_right_mismatch * (1 - prepared_data['right_match'])
        
        # --- é—œéµä¿®æ­£ï¼šä½¿ç”¨ LogSumExp ä¾†å¹³æ»‘åœ°è¿‘ä¼¼ max() ---
        # å¼•å…¥ä¸€å€‹æ§åˆ¶å¹³æ»‘åº¦çš„åƒæ•¸ k
        k = pm.HalfNormal('k_smoothness', sigma=1.0) 
        
        # LogSumExp å…¬å¼
        v_final_correct = pm.Deterministic(
            'v_final_correct', 
            pm.math.log(pm.math.exp(k * v_left) + pm.math.exp(k * v_right)) / k
        )
        
        v_final_incorrect = 0.1
        likelihood_parallel = pm.CustomDist('likelihood', v_final_correct, v_final_incorrect, B_CONSTANT, A_CONSTANT, T0_CONSTANT, logp=logp_lba, random=lba_random, observed=observed_value)    
        idata_parallel = pm.sample(model=model_parallel, **MCMC_CONFIG)

    # --- é—œéµä¿®æ­£ 2ï¼šè¨ˆç®—ä¸¦å„²å­˜ log_likelihood ---
    
    with model_parallel:
        pm.compute_log_likelihood(idata_parallel)
        idata_parallel.extend(pm.sample_posterior_predictive(idata_parallel))
    print("âœ… Parallel AND æ¨¡å‹æ“¬åˆå®Œæˆ")
    
    # --- Phase 4: è¦–è¦ºåŒ–è¨ºæ–·èˆ‡æ¨¡å‹æ¯”è¼ƒ ---
    print("\n\n--- è¦–è¦ºåŒ–è¨ºæ–·èˆ‡æœ€çµ‚æ¯”è¼ƒ ---")
    az.plot_trace(idata_coactive, var_names=['v_left_match', 'v_left_mismatch', 'v_right_match', 'v_right_mismatch']); plt.suptitle("Trace Plot for Coactive Model", y=1.02); plt.tight_layout(); plt.show()
    az.plot_ppc(idata_coactive, kind='cumulative', num_pp_samples=100); plt.suptitle("PPC (Cumulative) for Coactive Model"); plt.show()
    az.plot_trace(idata_parallel, var_names=['v_left_match', 'v_left_mismatch', 'v_right_match', 'v_right_mismatch']); plt.suptitle("Trace Plot for Parallel AND Model", y=1.02); plt.tight_layout(); plt.show()
    az.plot_ppc(idata_parallel, kind='cumulative', num_pp_samples=100); plt.suptitle("PPC (Cumulative) for Parallel AND Model"); plt.show()
    model_comparison = {"Coactive": idata_coactive, "Parallel": idata_parallel}
    loo_compare = az.compare(model_comparison, ic="loo")
    print(loo_compare)
    az.plot_compare(loo_compare); plt.title("Model Comparison (LOO)"); plt.show()
    
    # --- Modified Plotting and Comparison Code ---
    
    # Plotting for the Coactive Model
    az.plot_trace(idata_coactive, var_names=['v_left_match', 'v_left_mismatch', 'v_right_match', 'v_right_mismatch'])
    # Using an f-string to add the subject ID to the title
    plt.suptitle(f"Trace Plot for Coactive Model (Subject: {SUBJECT_ID_TO_RUN})", y=1.02)
    plt.tight_layout()
    plt.show()
    
    az.plot_ppc(idata_coactive, kind='cumulative', num_pp_samples=100)
    plt.suptitle(f"PPC (Cumulative) for Coactive Model (Subject: {SUBJECT_ID_TO_RUN})")
    plt.show()
    
    # Plotting for the Parallel AND Model
    az.plot_trace(idata_parallel, var_names=['v_left_match', 'v_left_mismatch', 'v_right_match', 'v_right_mismatch'])
    # Using an f-string here as well
    plt.suptitle(f"Trace Plot for Parallel AND Model (Subject: {SUBJECT_ID_TO_RUN})", y=1.02)
    plt.tight_layout()
    plt.show()
    
    az.plot_ppc(idata_parallel, kind='cumulative', num_pp_samples=100)
    plt.suptitle(f"PPC (Cumulative) for Parallel AND Model (Subject: {SUBJECT_ID_TO_RUN})")
    plt.show()
    
    # Model comparison remains the same, but the plots above will be specific
    model_comparison = {"Coactive": idata_coactive, "Parallel": idata_parallel}
    loo_compare = az.compare(model_comparison, ic="loo")
    print(f"\n--- Model Comparison for Subject: {SUBJECT_ID_TO_RUN} ---")
    print(loo_compare)
    # --- æœ€çµ‚å°æ±ºï¼šæ¨¡å‹æ¯”è¼ƒ (åŠ å…¥è­¦å‘Šæª¢æŸ¥) ---
    print("\n\n--- æœ€çµ‚å°æ±ºï¼šæ¨¡å‹æ¯”è¼ƒçµæœ (ä½¿ç”¨ LOO) ---")
    
    # æª¢æŸ¥ Parallel æ¨¡å‹çš„æ”¶æ–‚æƒ…æ³
    rhat_parallel = az.rhat(idata_parallel)
    has_convergence_issues = (rhat_parallel.to_array() > 1.01).any().item()

    if has_convergence_issues:
        print("âš ï¸ è­¦å‘Š: Parallel AND æ¨¡å‹å­˜åœ¨åš´é‡çš„æ”¶æ–‚å•é¡Œ (R-hat > 1.01 æˆ–å¤§é‡ç™¼æ•£)ã€‚")
        print("   å…¶ LOO å€¼ä¸å¯é ï¼Œåƒ…ä¾›åƒè€ƒã€‚æ•¸æ“šå¼·çƒˆå‚¾å‘æ–¼ Coactive æ¨¡å‹ã€‚")
        # åªé¡¯ç¤º Coactive æ¨¡å‹çš„æ‘˜è¦
        print("\n--- Coactive æ¨¡å‹æ‘˜è¦ (ç©©å®šä¸”å¯ä¿¡) ---")
        print(az.summary(idata_coactive, var_names=['v_left_match', 'v_left_mismatch', 'v_right_match', 'v_right_mismatch']))
    else:
        # åªæœ‰åœ¨å…©å€‹æ¨¡å‹éƒ½æ”¶æ–‚æ™‚æ‰é€²è¡Œæ­£å¼æ¯”è¼ƒ
        model_comparison = {"Coactive": idata_coactive, "Parallel": idata_parallel}
        loo_compare = az.compare(model_comparison, ic="loo")
        print(loo_compare)
        az.plot_compare(loo_compare); plt.title("Model Comparison (LOO)"); plt.show()
