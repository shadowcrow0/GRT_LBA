# -*- coding: utf-8 -*-
"""
main_analysis.py - ä¸»åˆ†ææµç¨‹
Sequential Processing LBA - Main Analysis Pipeline

åŠŸèƒ½ï¼š
- æ•´åˆå®Œæ•´çš„åˆ†ææµç¨‹
- æ”¯æ´ä¸åŒçš„åˆ†æé¸é …
- æä¾›æ¨¡å‹æ¯”è¼ƒåŠŸèƒ½
- ç”Ÿæˆåˆ†æå ±å‘Š
"""

import numpy as np
import pandas as pd
import time
from datetime import datetime
from typing import Dict, List, Optional
from data_utils import DataProcessor
from model_fitting import SequentialModelFitter

def main_sequential_analysis(csv_file='GRT_LBA.csv', max_subjects=None, 
                           first_side='left', time_split_ratio=0.6,
                           mcmc_config=None, save_results=True):
    """
    ä¸»è¦åºåˆ—åˆ†ææµç¨‹
    
    Args:
        csv_file: è³‡æ–™æª”æ¡ˆè·¯å¾‘
        max_subjects: æœ€å¤§å—è©¦è€…æ•¸é™åˆ¶
        first_side: é¦–å…ˆè™•ç†çš„é€šé“ ('left' æˆ– 'right')
        time_split_ratio: æ™‚é–“åˆ†å‰²æ¯”ä¾‹
        mcmc_config: MCMCé…ç½®å­—å…¸
        save_results: æ˜¯å¦å„²å­˜çµæœ
        
    Returns:
        dict: å®Œæ•´åˆ†æçµæœ
    """
    
    print("ğŸš€ åºåˆ—è™•ç†LBAåˆ†æ")
    print("=" * 60)
    print(f"ğŸ“‚ è³‡æ–™æª”æ¡ˆ: {csv_file}")
    print(f"ğŸ§  è™•ç†é †åº: {first_side} å…ˆè™•ç†")
    print(f"â° æ™‚é–“åˆ†å‰²: {time_split_ratio:.1%} / {1-time_split_ratio:.1%}")
    if max_subjects:
        print(f"ğŸ‘¥ å—è©¦è€…é™åˆ¶: {max_subjects}")
    print("=" * 60)
    
    analysis_start_time = time.time()
    
    try:
        # ========================================
        # 1. è¼‰å…¥å’Œé è™•ç†è³‡æ–™
        # ========================================
        
        print("\nğŸ“‚ éšæ®µ1: è³‡æ–™è¼‰å…¥å’Œé è™•ç†")
        print("-" * 40)
        
        processor = DataProcessor()
        data_df = processor.load_and_clean_data(csv_file)
        
        # è³‡æ–™å“è³ªæª¢æŸ¥
        quality_report = processor.validate_data_quality(data_df)
        
        # å—è©¦è€…é¸æ“‡
        participants = sorted(data_df['participant'].unique())
        if max_subjects:
            participants = participants[:max_subjects]
        
        print(f"âœ… è³‡æ–™é è™•ç†å®Œæˆ")
        print(f"   ç¸½è©¦é©—æ•¸: {len(data_df)}")
        print(f"   åˆ†æå—è©¦è€…: {len(participants)}")
        
        # ========================================
        # 2. æº–å‚™å—è©¦è€…è³‡æ–™
        # ========================================
        
        print(f"\nğŸ”„ éšæ®µ2: æº–å‚™å—è©¦è€…è³‡æ–™")
        print("-" * 40)
        
        subjects_data = []
        excluded_subjects = []
        
        for subject_id in participants:
            try:
                subject_data = processor.extract_subject_data(data_df, subject_id)
                
                # æª¢æŸ¥è³‡æ–™å……è¶³æ€§
                if subject_data['n_trials'] >= 50:
                    subjects_data.append(subject_data)
                else:
                    excluded_subjects.append({
                        'subject_id': subject_id,
                        'reason': f"è³‡æ–™ä¸è¶³ ({subject_data['n_trials']} < 50 trials)"
                    })
            except Exception as e:
                excluded_subjects.append({
                    'subject_id': subject_id,
                    'reason': f"è³‡æ–™æå–å¤±æ•—: {e}"
                })
        
        print(f"âœ… å—è©¦è€…è³‡æ–™æº–å‚™å®Œæˆ")
        print(f"   æœ‰æ•ˆå—è©¦è€…: {len(subjects_data)}")
        if excluded_subjects:
            print(f"   æ’é™¤å—è©¦è€…: {len(excluded_subjects)}")
            for excluded in excluded_subjects[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                print(f"     å—è©¦è€… {excluded['subject_id']}: {excluded['reason']}")
            if len(excluded_subjects) > 3:
                print(f"     ... é‚„æœ‰ {len(excluded_subjects)-3} å€‹")
        
        if len(subjects_data) == 0:
            raise ValueError("æ²’æœ‰æœ‰æ•ˆçš„å—è©¦è€…è³‡æ–™å¯ä»¥åˆ†æ")
        
        # ========================================
        # 3. æ¨¡å‹æ“¬åˆ
        # ========================================
        
        print(f"\nğŸ¯ éšæ®µ3: åºåˆ—LBAæ¨¡å‹æ“¬åˆ")
        print("-" * 40)
        
        # å‰µå»ºæ“¬åˆå™¨
        fitter = SequentialModelFitter(
            first_side=first_side, 
            time_split_ratio=time_split_ratio,
            mcmc_config=mcmc_config
        )
        
        # æ‰¹æ¬¡æ“¬åˆ
        fitting_results = fitter.fit_multiple_subjects(
            subjects_data, 
            max_subjects=None,  # å·²ç¶“åœ¨å‰é¢é™åˆ¶äº†
            continue_on_failure=True,
            verbose=True
        )
        
        # ========================================
        # 4. çµæœåˆ†æå’Œæ‘˜è¦
        # ========================================
        
        print(f"\nğŸ“Š éšæ®µ4: çµæœåˆ†æ")
        print("-" * 40)
        
        analysis_summary = analyze_fitting_results(fitting_results)
        
        # ========================================
        # 5. å„²å­˜çµæœ
        # ========================================
        
        if save_results:
            print(f"\nğŸ’¾ éšæ®µ5: å„²å­˜çµæœ")
            print("-" * 40)
            
            # ç”Ÿæˆæª”å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_prefix = f"sequential_lba_{first_side}_first_{timestamp}"
            
            saved_files = fitter.save_results(fitting_results, output_prefix)
            
            # å„²å­˜åˆ†ææ‘˜è¦
            summary_filename = f"{output_prefix}_summary.txt"
            save_analysis_summary(analysis_summary, summary_filename)
            saved_files['summary'] = summary_filename
        else:
            saved_files = {}
        
        # ========================================
        # 6. æœ€çµ‚æ‘˜è¦
        # ========================================
        
        total_time = time.time() - analysis_start_time
        
        final_summary = {
            'analysis_complete': True,
            'total_time_minutes': total_time / 60,
            'data_info': {
                'csv_file': csv_file,
                'total_trials': len(data_df),
                'total_subjects': len(participants),
                'analyzed_subjects': len(subjects_data),
                'excluded_subjects': len(excluded_subjects)
            },
            'model_config': {
                'first_side': first_side,
                'time_split_ratio': time_split_ratio,
                'mcmc_config': mcmc_config
            },
            'fitting_results': fitting_results,
            'analysis_summary': analysis_summary,
            'quality_report': quality_report,
            'saved_files': saved_files
        }
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ åºåˆ—LBAåˆ†æå®Œæˆ!")
        print(f"â±ï¸ ç¸½æ™‚é–“: {total_time/60:.1f} åˆ†é˜")
        print(f"âœ… æˆåŠŸç‡: {analysis_summary['success_rate']:.1%}")
        print(f"ğŸ”„ æ”¶æ–‚ç‡: {analysis_summary['convergence_rate']:.1%}")
        print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆ: {len(saved_files)} å€‹")
        print(f"{'='*60}")
        
        return final_summary
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        
        return {
            'analysis_complete': False,
            'error': str(e),
            'total_time_minutes': (time.time() - analysis_start_time) / 60
        }

def analyze_fitting_results(results):
    """åˆ†ææ“¬åˆçµæœï¼Œç”Ÿæˆæ‘˜è¦çµ±è¨ˆ"""
    
    print("ğŸ” åˆ†ææ“¬åˆçµæœ...")
    
    total_subjects = len(results)
    successful_results = [r for r in results if r.get('success', False)]
    converged_results = [r for r in successful_results if r.get('converged', False)]
    
    success_rate = len(successful_results) / total_subjects if total_subjects > 0 else 0
    convergence_rate = len(converged_results) / len(successful_results) if successful_results else 0
    
    # åŸºæœ¬çµ±è¨ˆ
    summary = {
        'total_subjects': total_subjects,
        'successful_subjects': len(successful_results),
        'converged_subjects': len(converged_results),
        'success_rate': success_rate,
        'convergence_rate': convergence_rate
    }
    
    if successful_results:
        # æ™‚é–“çµ±è¨ˆ
        sampling_times = [r['sampling_time_minutes'] for r in successful_results]
        summary.update({
            'mean_sampling_time': np.mean(sampling_times),
            'std_sampling_time': np.std(sampling_times),
            'total_sampling_time': np.sum(sampling_times)
        })
        
        # æ”¶æ–‚çµ±è¨ˆ
        if converged_results:
            rhat_values = [r['convergence_diagnostics']['rhat_max'] for r in converged_results 
                          if 'convergence_diagnostics' in r]
            ess_values = [r['convergence_diagnostics']['ess_bulk_min'] for r in converged_results 
                         if 'convergence_diagnostics' in r]
            
            if rhat_values:
                summary.update({
                    'mean_rhat': np.mean(rhat_values),
                    'max_rhat': np.max(rhat_values),
                    'min_ess': np.min(ess_values) if ess_values else np.nan,
                    'mean_ess': np.mean(ess_values) if ess_values else np.nan
                })
        
        # åƒæ•¸çµ±è¨ˆ
        if converged_results:
            # æ”¶é›†æ‰€æœ‰åƒæ•¸
            all_params = {}
            for result in converged_results:
                for param_name, param_value in result['posterior_means'].items():
                    if not np.isnan(param_value):
                        if param_name not in all_params:
                            all_params[param_name] = []
                        all_params[param_name].append(param_value)
            
            # è¨ˆç®—åƒæ•¸çµ±è¨ˆ
            param_stats = {}
            for param_name, values in all_params.items():
                if len(values) > 0:
                    param_stats[param_name] = {
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'min': np.min(values),
                        'max': np.max(values),
                        'n_subjects': len(values)
                    }
            
            summary['parameter_statistics'] = param_stats
        
        # å¤±æ•—åŸå› çµ±è¨ˆ
        failed_results = [r for r in results if not r.get('success', False)]
        if failed_results:
            failure_reasons = {}
            for result in failed_results:
                reason = result.get('error', 'Unknown error')
                # ç°¡åŒ–éŒ¯èª¤è¨Šæ¯
                if 'Insufficient data' in reason:
                    reason = 'Insufficient data'
                elif 'validation failed' in reason:
                    reason = 'Model validation failed'
                elif 'MCMC' in reason or 'sampling' in reason:
                    reason = 'MCMC sampling failed'
                else:
                    reason = 'Other error'
                
                failure_reasons[reason] = failure_reasons.get(reason, 0) + 1
            
            summary['failure_reasons'] = failure_reasons
    
    print(f"âœ… çµæœåˆ†æå®Œæˆ")
    print(f"   æˆåŠŸç‡: {success_rate:.1%}")
    print(f"   æ”¶æ–‚ç‡: {convergence_rate:.1%}")
    if successful_results:
        print(f"   å¹³å‡æ¡æ¨£æ™‚é–“: {summary['mean_sampling_time']:.1f} åˆ†é˜")
    
    return summary

def save_analysis_summary(summary, filename):
    """å„²å­˜åˆ†ææ‘˜è¦åˆ°æ–‡å­—æª”"""
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("åºåˆ—è™•ç†LBAåˆ†ææ‘˜è¦å ±å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            # åŸºæœ¬çµ±è¨ˆ
            f.write("åŸºæœ¬çµ±è¨ˆ:\n")
            f.write("-" * 20 + "\n")
            f.write(f"ç¸½å—è©¦è€…æ•¸: {summary['total_subjects']}\n")
            f.write(f"æˆåŠŸæ“¬åˆ: {summary['successful_subjects']}\n")
            f.write(f"æ”¶æ–‚æ“¬åˆ: {summary['converged_subjects']}\n")
            f.write(f"æˆåŠŸç‡: {summary['success_rate']:.1%}\n")
            f.write(f"æ”¶æ–‚ç‡: {summary['convergence_rate']:.1%}\n\n")
            
            # æ™‚é–“çµ±è¨ˆ
            if 'mean_sampling_time' in summary:
                f.write("æ™‚é–“çµ±è¨ˆ:\n")
                f.write("-" * 20 + "\n")
                f.write(f"å¹³å‡æ¡æ¨£æ™‚é–“: {summary['mean_sampling_time']:.1f} åˆ†é˜\n")
                f.write(f"æ¨™æº–å·®: {summary['std_sampling_time']:.1f} åˆ†é˜\n")
                f.write(f"ç¸½æ¡æ¨£æ™‚é–“: {summary['total_sampling_time']:.1f} åˆ†é˜\n\n")
            
            # æ”¶æ–‚çµ±è¨ˆ
            if 'mean_rhat' in summary:
                f.write("æ”¶æ–‚çµ±è¨ˆ:\n")
                f.write("-" * 20 + "\n")
                f.write(f"å¹³å‡ RÌ‚: {summary['mean_rhat']:.3f}\n")
                f.write(f"æœ€å¤§ RÌ‚: {summary['max_rhat']:.3f}\n")
                f.write(f"æœ€å° ESS: {summary['min_ess']:.0f}\n")
                f.write(f"å¹³å‡ ESS: {summary['mean_ess']:.0f}\n\n")
            
            # åƒæ•¸çµ±è¨ˆ
            if 'parameter_statistics' in summary:
                f.write("åƒæ•¸çµ±è¨ˆæ‘˜è¦:\n")
                f.write("-" * 20 + "\n")
                for param_name, stats in summary['parameter_statistics'].items():
                    f.write(f"{param_name}:\n")
                    f.write(f"  å¹³å‡: {stats['mean']:.3f}\n")
                    f.write(f"  æ¨™æº–å·®: {stats['std']:.3f}\n")
                    f.write(f"  ç¯„åœ: [{stats['min']:.3f}, {stats['max']:.3f}]\n")
                    f.write(f"  å—è©¦è€…æ•¸: {stats['n_subjects']}\n\n")
            
            # å¤±æ•—åŸå› 
            if 'failure_reasons' in summary:
                f.write("å¤±æ•—åŸå› :\n")
                f.write("-" * 20 + "\n")
                for reason, count in summary['failure_reasons'].items():
                    f.write(f"{reason}: {count}\n")
        
        print(f"âœ… æ‘˜è¦å ±å‘Šå·²å„²å­˜: {filename}")
        
    except Exception as e:
        print(f"âš ï¸ æ‘˜è¦å ±å‘Šå„²å­˜å¤±æ•—: {e}")

def compare_processing_orders(csv_file='GRT_LBA.csv', max_subjects=3, 
                            time_split_ratio=0.6, mcmc_config=None):
    """
    æ¯”è¼ƒä¸åŒè™•ç†é †åºçš„åˆ†æçµæœ
    
    Args:
        csv_file: è³‡æ–™æª”æ¡ˆè·¯å¾‘
        max_subjects: æœ€å¤§å—è©¦è€…æ•¸é™åˆ¶
        time_split_ratio: æ™‚é–“åˆ†å‰²æ¯”ä¾‹
        mcmc_config: MCMCé…ç½®
        
    Returns:
        dict: æ¯”è¼ƒçµæœ
    """
    
    print("ğŸ† è™•ç†é †åºæ¯”è¼ƒåˆ†æ")
    print("=" * 60)
    
    comparison_results = {}
    total_start_time = time.time()
    
    # å·¦å…ˆè™•ç†
    print("\nğŸ“ åˆ†æ1: å·¦é‚Šå…ˆè™•ç†")
    print("-" * 30)
    left_first_result = main_sequential_analysis(
        csv_file=csv_file,
        max_subjects=max_subjects,
        first_side='left',
        time_split_ratio=time_split_ratio,
        mcmc_config=mcmc_config,
        save_results=True
    )
    comparison_results['left_first'] = left_first_result
    
    # å³å…ˆè™•ç†
    print("\nğŸ“ åˆ†æ2: å³é‚Šå…ˆè™•ç†")
    print("-" * 30)
    right_first_result = main_sequential_analysis(
        csv_file=csv_file,
        max_subjects=max_subjects,
        first_side='right',
        time_split_ratio=time_split_ratio,
        mcmc_config=mcmc_config,
        save_results=True
    )
    comparison_results['right_first'] = right_first_result
    
    # æ¯”è¼ƒåˆ†æ
    total_time = time.time() - total_start_time
    
    print(f"\n{'='*60}")
    print("ğŸ¯ è™•ç†é †åºæ¯”è¼ƒçµæœ")
    print("-" * 60)
    
    for order_name, result in comparison_results.items():
        if result['analysis_complete']:
            summary = result['analysis_summary']
            print(f"{order_name:15s}: æˆåŠŸç‡={summary['success_rate']:5.1%}, "
                  f"æ”¶æ–‚ç‡={summary['convergence_rate']:5.1%}, "
                  f"æ™‚é–“={result['total_time_minutes']:5.1f}åˆ†")
        else:
            print(f"{order_name:15s}: åˆ†æå¤±æ•— - {result.get('error', 'Unknown error')}")
    
    print(f"\nç¸½æ¯”è¼ƒæ™‚é–“: {total_time/60:.1f} åˆ†é˜")
    print("="*60)
    
    comparison_results['comparison_summary'] = {
        'total_comparison_time': total_time / 60,
        'both_successful': (left_first_result['analysis_complete'] and 
                           right_first_result['analysis_complete'])
    }
    
    return comparison_results

def quick_test_sequential(csv_file='GRT_LBA.csv'):
    """å¿«é€Ÿæ¸¬è©¦åºåˆ—æ¨¡å‹"""
    
    print("ğŸ§ª å¿«é€Ÿæ¸¬è©¦: åºåˆ—è™•ç†LBA")
    print("=" * 40)
    
    # ä½¿ç”¨ç°¡åŒ–çš„MCMCè¨­å®šé€²è¡Œå¿«é€Ÿæ¸¬è©¦
    quick_mcmc_config = {
        'draws': 100,
        'tune': 100,
        'chains': 1,
        'cores': 1,
        'target_accept': 0.80,
        'max_treedepth': 6,
        'progressbar': True,
        'return_inferencedata': True
    }
    
    try:
        result = main_sequential_analysis(
            csv_file=csv_file,
            max_subjects=1,
            first_side='left',
            time_split_ratio=0.6,
            mcmc_config=quick_mcmc_config,
            save_results=False
        )
        
        if result['analysis_complete'] and result['analysis_summary']['successful_subjects'] > 0:
            print("âœ… åºåˆ—æ¨¡å‹å¿«é€Ÿæ¸¬è©¦æˆåŠŸ!")
            print("ğŸ¯ æ¨¡çµ„æ¶æ§‹é‹ä½œæ­£å¸¸")
            print("ğŸ”¬ æº–å‚™é€²è¡Œå®Œæ•´åˆ†æ")
            return True
        else:
            print("âŒ å¿«é€Ÿæ¸¬è©¦å¤±æ•—")
            return False
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return False

def run_full_analysis_pipeline(csv_file='GRT_LBA.csv', analysis_type='medium'):
    """
    åŸ·è¡Œå®Œæ•´åˆ†æç®¡ç·š
    
    Args:
        csv_file: è³‡æ–™æª”æ¡ˆè·¯å¾‘
        analysis_type: åˆ†æé¡å‹ ('quick', 'small', 'medium', 'large', 'full', 'compare')
    """
    
    # æ ¹æ“šåˆ†æé¡å‹è¨­å®šåƒæ•¸
    analysis_configs = {
        'quick': {
            'max_subjects': 1,
            'mcmc_config': {'draws': 100, 'tune': 100, 'chains': 1}
        },
        'small': {
            'max_subjects': 3,
            'mcmc_config': {'draws': 200, 'tune': 200, 'chains': 2}
        },
        'medium': {
            'max_subjects': 5,
            'mcmc_config': {'draws': 400, 'tune': 400, 'chains': 2}
        },
        'large': {
            'max_subjects': 10,
            'mcmc_config': {'draws': 500, 'tune': 500, 'chains': 2}
        },
        'full': {
            'max_subjects': None,
            'mcmc_config': {'draws': 600, 'tune': 600, 'chains': 3}
        }
    }
    
    if analysis_type == 'compare':
        # è™•ç†é †åºæ¯”è¼ƒ
        return compare_processing_orders(
            csv_file=csv_file,
            max_subjects=3,
            mcmc_config={'draws': 200, 'tune': 200, 'chains': 2}
        )
    else:
        # å–®ä¸€åˆ†æ
        config = analysis_configs.get(analysis_type, analysis_configs['medium'])
        return main_sequential_analysis(
            csv_file=csv_file,
            first_side='left',
            time_split_ratio=0.6,
            **config
        )

# ============================================================================
# åŸ·è¡Œä»‹é¢
# ============================================================================

if __name__ == "__main__":
    print("ğŸ¯ åºåˆ—è™•ç†LBAåˆ†æé¸é …:")
    print("=" * 40)
    print("1. å¿«é€Ÿæ¸¬è©¦ (1å—è©¦è€…, ç°¡åŒ–MCMC)")
    print("2. å°å‹åˆ†æ (3å—è©¦è€…)")
    print("3. ä¸­å‹åˆ†æ (5å—è©¦è€…)")
    print("4. å¤§å‹åˆ†æ (10å—è©¦è€…)")
    print("5. å®Œæ•´åˆ†æ (æ‰€æœ‰å—è©¦è€…)")
    print("6. è™•ç†é †åºæ¯”è¼ƒ (å·¦vså³å…ˆè™•ç†)")
    
    try:
        choice = input("\nè«‹é¸æ“‡ (1-6): ").strip()
        
        analysis_map = {
            '1': 'quick',
            '2': 'small', 
            '3': 'medium',
            '4': 'large',
            '5': 'full',
            '6': 'compare'
        }
        
        analysis_type = analysis_map.get(choice, 'quick')
        
        print(f"\nğŸš€ åŸ·è¡Œ{analysis_type}åˆ†æ...")
        
        if analysis_type == 'quick':
            # å¿«é€Ÿæ¸¬è©¦ä½¿ç”¨ç‰¹æ®Šå‡½æ•¸
            success = quick_test_sequential()
        else:
            # å…¶ä»–åˆ†æä½¿ç”¨å®Œæ•´ç®¡ç·š
            result = run_full_analysis_pipeline(analysis_type=analysis_type)
            success = result.get('analysis_complete', False) if isinstance(result, dict) else False
        
        if success:
            print("\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆ!")
            print("âœ… åºåˆ—LBAæ¨¡çµ„æ¶æ§‹é‹ä½œæ­£å¸¸")
        else:
            print("\nâŒ åˆ†æå¤±æ•—")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ åˆ†æè¢«ä½¿ç”¨è€…ä¸­æ–·")
    except Exception as e:
        print(f"\nğŸ’¥ æœªé æœŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

# ============================================================================
# ä½¿ç”¨ç¯„ä¾‹å’Œèªªæ˜
# ============================================================================

"""
åºåˆ—è™•ç†LBAåˆ†æä½¿ç”¨ç¯„ä¾‹:

# 1. åŸºæœ¬ä½¿ç”¨
from main_analysis import main_sequential_analysis
result = main_sequential_analysis('GRT_LBA.csv', max_subjects=5, first_side='left')

# 2. å¿«é€Ÿæ¸¬è©¦
from main_analysis import quick_test_sequential
success = quick_test_sequential('GRT_LBA.csv')

# 3. è™•ç†é †åºæ¯”è¼ƒ
from main_analysis import compare_processing_orders
comparison = compare_processing_orders('GRT_LBA.csv', max_subjects=3)

# 4. è‡ªè¨‚MCMCè¨­å®š
custom_mcmc = {
    'draws': 800,
    'tune': 800,
    'chains': 3,
    'target_accept': 0.90
}
result = main_sequential_analysis('GRT_LBA.csv', mcmc_config=custom_mcmc)

# 5. å®Œæ•´åˆ†æç®¡ç·š
from main_analysis import run_full_analysis_pipeline
result = run_full_analysis_pipeline('GRT_LBA.csv', 'large')

æª”æ¡ˆçµæ§‹:
- data_utils.py: è³‡æ–™é è™•ç†
- single_side_lba.py: å–®é‚ŠLBAè™•ç†å™¨
- four_choice_lba.py: å››é¸ä¸€LBAç«¶çˆ­å™¨
- sequential_model.py: åºåˆ—è™•ç†ä¸»æ¨¡å‹
- model_fitting.py: æ¨¡å‹æ“¬åˆå™¨
- main_analysis.py: ä¸»åˆ†ææµç¨‹ (æ­¤æª”æ¡ˆ)

è¼¸å‡ºæª”æ¡ˆ:
- *_main_*.csv: ä¸»è¦çµæœ
- *_params_*.csv: åƒæ•¸ä¼°è¨ˆ
- *_convergence_*.csv: æ”¶æ–‚è¨ºæ–·
- *_summary.txt: åˆ†ææ‘˜è¦å ±å‘Š
"""
