# -*- coding: utf-8 -*-
"""
LBA Models Module - Based on HDDM and original papers (FIXED & COMPATIBLE)
lba_models.py
"""

import pymc as pm
import pytensor.tensor as pt
import numpy as np
import arviz as az
from lba_fixes import lba_logp_correct, safe_lba_random # <-- æ·»åŠ é€™è¡Œï¼Œä½¿ç”¨æ–°å‡½æ•¸
# ===================================================================
# Standardized & Robust Model Definitions
# Using tighter, more informed priors to improve sampling stability.
# ===================================================================

def create_coactive_lba_model(observed_data, input_data):
    """
    Robust Coactive LBA Model - drift rates are summed (v_left + v_right).
    """
    with pm.Model() as model:
        # Priors with reasonable constraints to improve stability
        v_match = pm.TruncatedNormal('v_match', mu=1.5, sigma=0.5, lower=0.5, upper=5.0)
        v_mismatch = pm.TruncatedNormal('v_mismatch', mu=1.0, sigma=0.5, lower=0.1, upper=3.0)
        
        # Standardized LBA parameters
        start_var = pm.TruncatedNormal('start_var', mu=0.5, sigma=0.2, lower=0.1, upper=1.5)
        boundary_offset = pm.TruncatedNormal('boundary_offset', mu=1.0, sigma=0.3, lower=0.3, upper=2.0)
        b_safe = pm.Deterministic('b_safe', start_var + boundary_offset)
        
        # --- ä¿®æ­£å¾Œçš„ç¨‹å¼ç¢¼ (lba_models.py) ---
        # 1. å¾å‚³å…¥çš„æ•¸æ“šä¸­å‹•æ…‹è¨ˆç®—æœ€å°åæ‡‰æ™‚é–“ (å–®ä½ï¼šç§’)
        min_rt_seconds = np.min(observed_data[:, 0]) / 1000.0
        
        # 2. ç‚ºäº†æ•¸å€¼ç©©å®šæ€§ï¼Œå°‡ä¸Šç•Œè¨­ç‚ºç•¥å°æ–¼æœ€å°RTï¼Œä¸¦åŠ ä¸Šä¸€å€‹åˆç†çš„çµ•å°ä¸Šé™
        #    ä½¿ç”¨ pt.minimum ç¢ºä¿ä¸Šç•Œä¸æœƒæ„å¤–åœ°è®Šå¾—éå¤§
        upper_bound = pt.minimum(min_rt_seconds * 0.99, 0.5)
        
        # 3. ä½¿ç”¨é€™å€‹å‹•æ…‹è¨ˆç®—å‡ºçš„ä¸Šç•Œä¾†å®šç¾©å…ˆé©—
        #    é€™æ˜¯é˜²æ­¢ "Bad initial energy" éŒ¯èª¤çš„é—œéµ
        non_decision = pm.TruncatedNormal('non_decision',
                                        mu=0.15,
                                        sigma=0.05,
                                        lower=0.05,
                                        upper=upper_bound) # <-- ä½¿ç”¨å‹•æ…‹ä¸Šç•Œ
        # Calculate drift rates based on input data
        v_left = v_match * input_data['left_match'] + v_mismatch * (1 - input_data['left_match'])
        v_right = v_match * input_data['right_match'] + v_mismatch * (1 - input_data['right_match'])
        
        # Coactive integration rule
        v_final_correct = pm.Deterministic('v_final_correct', v_left + v_right)
        
        # --- æ–°çš„ä¿®æ­£ç¨‹å¼ç¢¼ ---
        # Define the mean drift rate for the incorrect accumulator as a free parameter
        v_incorrect_base = pm.TruncatedNormal('v_incorrect_base', mu=0.5, sigma=0.3, lower=0.1, upper=2.0)
        
        # Create a deterministic variable that has the same shape as v_final_correct.
        # This ensures its shape is (n_trials,), making it compatible for stacking.
        v_final_incorrect = pm.Deterministic('v_final_incorrect', pt.full_like(v_final_correct, v_incorrect_base))
        
        # Now both tensors have the shape (n_trials,), so stacking along axis=1 will work.
        v_rates = pt.stack([v_final_correct, v_final_incorrect], axis=1)
                
        likelihood = pm.CustomDist('likelihood',
                                  v_rates,              # <--- ä½¿ç”¨å †ç–Šå¾Œçš„æ¼‚ç§»ç‡
                                  start_var, b_safe, non_decision,
                                  logp=lba_logp_correct, # <--- æŒ‡å‘æ–°çš„ logp å‡½æ•¸
                                  random=safe_lba_random,
                                  observed=observed_data)
    return model

def create_parallel_and_lba_model(observed_data, input_data):
    """
    Robust Parallel AND LBA Model - takes the maximum drift rate.
    """
    with pm.Model() as model:
        v_match = pm.TruncatedNormal('v_match', mu=1.5, sigma=0.5, lower=0.5, upper=5.0)
        v_mismatch = pm.TruncatedNormal('v_mismatch', mu=1.0, sigma=0.5, lower=0.1, upper=3.0)
        start_var = pm.TruncatedNormal('start_var', mu=0.5, sigma=0.2, lower=0.1, upper=1.5)
        boundary_offset = pm.TruncatedNormal('boundary_offset', mu=1.0, sigma=0.3, lower=0.3, upper=2.0)
        b_safe = pm.Deterministic('b_safe', start_var + boundary_offset)
        # å‹•æ…‹è¨ˆç®—æœ€å°åæ‡‰æ™‚é–“ (ä»¥ç§’ç‚ºå–®ä½)
        # --- ä¿®æ­£å¾Œçš„ç¨‹å¼ç¢¼ (lba_models.py) ---
        # 1. å¾å‚³å…¥çš„æ•¸æ“šä¸­å‹•æ…‹è¨ˆç®—æœ€å°åæ‡‰æ™‚é–“ (å–®ä½ï¼šç§’)
        min_rt_seconds = np.min(observed_data[:, 0]) / 1000.0
        
        # 2. ç‚ºäº†æ•¸å€¼ç©©å®šæ€§ï¼Œå°‡ä¸Šç•Œè¨­ç‚ºç•¥å°æ–¼æœ€å°RTï¼Œä¸¦åŠ ä¸Šä¸€å€‹åˆç†çš„çµ•å°ä¸Šé™
        #    ä½¿ç”¨ pt.minimum ç¢ºä¿ä¸Šç•Œä¸æœƒæ„å¤–åœ°è®Šå¾—éå¤§
        upper_bound = pt.minimum(min_rt_seconds * 0.99, 0.5)
        
        # 3. ä½¿ç”¨é€™å€‹å‹•æ…‹è¨ˆç®—å‡ºçš„ä¸Šç•Œä¾†å®šç¾©å…ˆé©—
        #    é€™æ˜¯é˜²æ­¢ "Bad initial energy" éŒ¯èª¤çš„é—œéµ
        non_decision = pm.TruncatedNormal('non_decision',
                                        mu=0.15,
                                        sigma=0.05,
                                        lower=0.05,
                                        upper=upper_bound) # <-- ä½¿ç”¨å‹•æ…‹ä¸Šç•Œ
        v_left = v_match * input_data['left_match'] + v_mismatch * (1 - input_data['left_match'])
        v_right = v_match * input_data['right_match'] + v_mismatch * (1 - input_data['right_match'])
        v_final_correct = pm.Deterministic('v_final_correct', pt.maximum(v_left, v_right))
        # --- æ–°çš„ä¿®æ­£ç¨‹å¼ç¢¼ ---
        # Define the mean drift rate for the incorrect accumulator as a free parameter
        v_incorrect_base = pm.TruncatedNormal('v_incorrect_base', mu=0.5, sigma=0.3, lower=0.1, upper=2.0)
        
        # Create a deterministic variable that has the same shape as v_final_correct.
        # This ensures its shape is (n_trials,), making it compatible for stacking.
        v_final_incorrect = pm.Deterministic('v_final_incorrect', pt.full_like(v_final_correct, v_incorrect_base))
        
        # Now both tensors have the shape (n_trials,), so stacking along axis=1 will work.
        v_rates = pt.stack([v_final_correct, v_final_incorrect], axis=1)
                
        
        likelihood = pm.CustomDist('likelihood',
                                  v_rates,              # <--- ä½¿ç”¨å †ç–Šå¾Œçš„æ¼‚ç§»ç‡
                                  start_var, b_safe, non_decision,
                                  logp=lba_logp_correct, # <--- æŒ‡å‘æ–°çš„ logp å‡½æ•¸
                                  random=safe_lba_random,
                                  observed=observed_data)
    return model

def create_parallel_or_lba_model(observed_data, input_data):
    """
    Robust Parallel OR LBA Model - takes the minimum drift rate.
    """
    with pm.Model() as model:
        v_match = pm.TruncatedNormal('v_match', mu=1.5, sigma=0.5, lower=0.5, upper=5.0)
        v_mismatch = pm.TruncatedNormal('v_mismatch', mu=1.0, sigma=0.5, lower=0.1, upper=3.0)
        start_var = pm.TruncatedNormal('start_var', mu=0.5, sigma=0.2, lower=0.1, upper=1.5)
        boundary_offset = pm.TruncatedNormal('boundary_offset', mu=1.0, sigma=0.3, lower=0.3, upper=2.0)
        b_safe = pm.Deterministic('b_safe', start_var + boundary_offset)
        # --- ä¿®æ­£å¾Œçš„ç¨‹å¼ç¢¼ (lba_models.py) ---
        # 1. å¾å‚³å…¥çš„æ•¸æ“šä¸­å‹•æ…‹è¨ˆç®—æœ€å°åæ‡‰æ™‚é–“ (å–®ä½ï¼šç§’)
        min_rt_seconds = np.min(observed_data[:, 0]) / 1000.0
        
        # 2. ç‚ºäº†æ•¸å€¼ç©©å®šæ€§ï¼Œå°‡ä¸Šç•Œè¨­ç‚ºç•¥å°æ–¼æœ€å°RTï¼Œä¸¦åŠ ä¸Šä¸€å€‹åˆç†çš„çµ•å°ä¸Šé™
        #    ä½¿ç”¨ pt.minimum ç¢ºä¿ä¸Šç•Œä¸æœƒæ„å¤–åœ°è®Šå¾—éå¤§
        upper_bound = pt.minimum(min_rt_seconds * 0.99, 0.5)
        
        # 3. ä½¿ç”¨é€™å€‹å‹•æ…‹è¨ˆç®—å‡ºçš„ä¸Šç•Œä¾†å®šç¾©å…ˆé©—
        #    é€™æ˜¯é˜²æ­¢ "Bad initial energy" éŒ¯èª¤çš„é—œéµ
        non_decision = pm.TruncatedNormal('non_decision',
                                        mu=0.15,
                                        sigma=0.05,
                                        lower=0.05,
                                        upper=upper_bound) # <-- ä½¿ç”¨å‹•æ…‹ä¸Šç•Œ        v_left = v_match * input_data['left_match'] + v_mismatch * (1 - input_data['left_match'])
        v_left = v_match * input_data['left_match'] + v_mismatch * (1 - input_data['left_match'])
        v_right = v_match * input_data['right_match'] + v_mismatch * (1 - input_data['right_match'])
        v_final_correct = pm.Deterministic('v_final_correct', pt.minimum(v_left, v_right))
        # --- æ–°çš„ä¿®æ­£ç¨‹å¼ç¢¼ ---
        # Define the mean drift rate for the incorrect accumulator as a free parameter
        v_incorrect_base = pm.TruncatedNormal('v_incorrect_base', mu=0.5, sigma=0.3, lower=0.1, upper=2.0)
        
        # Create a deterministic variable that has the same shape as v_final_correct.
        # This ensures its shape is (n_trials,), making it compatible for stacking.
        v_final_incorrect = pm.Deterministic('v_final_incorrect', pt.full_like(v_final_correct, v_incorrect_base))
        
        # Now both tensors have the shape (n_trials,), so stacking along axis=1 will work.
        v_rates = pt.stack([v_final_correct, v_final_incorrect], axis=1)
                
        likelihood = pm.CustomDist('likelihood',
                                  v_rates,              # <--- ä½¿ç”¨å †ç–Šå¾Œçš„æ¼‚ç§»ç‡
                                  start_var, b_safe, non_decision,
                                  logp=lba_logp_correct, # <--- æŒ‡å‘æ–°çš„ logp å‡½æ•¸
                                  random=safe_lba_random,
                                  observed=observed_data)
    return model

def create_weighted_average_lba_model(observed_data, input_data):
    """
    Robust Weighted Average LBA Model.
    """
    with pm.Model() as model:
        v_match = pm.TruncatedNormal('v_match', mu=1.5, sigma=0.5, lower=0.5, upper=5.0)
        v_mismatch = pm.TruncatedNormal('v_mismatch', mu=1.0, sigma=0.5, lower=0.1, upper=3.0)
        w_left = pm.Beta('w_left', alpha=1.0, beta=1.0)
        w_right = pm.Deterministic('w_right', 1.0 - w_left)
        start_var = pm.TruncatedNormal('start_var', mu=0.5, sigma=0.2, lower=0.1, upper=1.5)
        boundary_offset = pm.TruncatedNormal('boundary_offset', mu=1.0, sigma=0.3, lower=0.3, upper=2.0)
        b_safe = pm.Deterministic('b_safe', start_var + boundary_offset)
        # å‹•æ…‹è¨ˆç®—æœ€å°åæ‡‰æ™‚é–“ (ä»¥ç§’ç‚ºå–®ä½)
        # --- ä¿®æ­£å¾Œçš„ç¨‹å¼ç¢¼ (lba_models.py) ---
        # 1. å¾å‚³å…¥çš„æ•¸æ“šä¸­å‹•æ…‹è¨ˆç®—æœ€å°åæ‡‰æ™‚é–“ (å–®ä½ï¼šç§’)
        min_rt_seconds = np.min(observed_data[:, 0]) / 1000.0
        
        # 2. ç‚ºäº†æ•¸å€¼ç©©å®šæ€§ï¼Œå°‡ä¸Šç•Œè¨­ç‚ºç•¥å°æ–¼æœ€å°RTï¼Œä¸¦åŠ ä¸Šä¸€å€‹åˆç†çš„çµ•å°ä¸Šé™
        #    ä½¿ç”¨ pt.minimum ç¢ºä¿ä¸Šç•Œä¸æœƒæ„å¤–åœ°è®Šå¾—éå¤§
        upper_bound = pt.minimum(min_rt_seconds * 0.99, 0.5)
        
        # 3. ä½¿ç”¨é€™å€‹å‹•æ…‹è¨ˆç®—å‡ºçš„ä¸Šç•Œä¾†å®šç¾©å…ˆé©—
        #    é€™æ˜¯é˜²æ­¢ "Bad initial energy" éŒ¯èª¤çš„é—œéµ
        non_decision = pm.TruncatedNormal('non_decision',
                                        mu=0.15,
                                        sigma=0.05,
                                        lower=0.05,
                                        upper=upper_bound) # <-- ä½¿ç”¨å‹•æ…‹ä¸Šç•Œ
        v_left = v_match * input_data['left_match'] + v_mismatch * (1 - input_data['left_match'])
        v_right = v_match * input_data['right_match'] + v_mismatch * (1 - input_data['right_match'])
        v_final_correct = pm.Deterministic('v_final_correct', w_left * v_left + w_right * v_right)
        # --- æ–°çš„ä¿®æ­£ç¨‹å¼ç¢¼ ---
        # Define the mean drift rate for the incorrect accumulator as a free parameter
        v_incorrect_base = pm.TruncatedNormal('v_incorrect_base', mu=0.5, sigma=0.3, lower=0.1, upper=2.0)
        
        # Create a deterministic variable that has the same shape as v_final_correct.
        # This ensures its shape is (n_trials,), making it compatible for stacking.
        v_final_incorrect = pm.Deterministic('v_final_incorrect', pt.full_like(v_final_correct, v_incorrect_base))
        
        # Now both tensors have the shape (n_trials,), so stacking along axis=1 will work.
        v_rates = pt.stack([v_final_correct, v_final_incorrect], axis=1)
                
        
        likelihood = pm.CustomDist('likelihood',
                                  v_rates,              # <--- ä½¿ç”¨å †ç–Šå¾Œçš„æ¼‚ç§»ç‡
                                  start_var, b_safe, non_decision,
                                  logp=lba_logp_correct, # <--- æŒ‡å‘æ–°çš„ logp å‡½æ•¸
                                  random=safe_lba_random,
                                  observed=observed_data)
        return model

MODEL_REGISTRY = {
    'Coactive_Addition': {
        'function': create_coactive_lba_model,
        'description': 'Coactive LBA: drift rates are summed (v_left + v_right)',
        'integration_type': 'Addition'
    },
    'Parallel_AND_Maximum': {
        'function': create_parallel_and_lba_model, 
        'description': 'Parallel AND LBA: maximum drift rate (max(v_left, v_right))',
        'integration_type': 'Maximum'
    },
    'Parallel_OR_Minimum': {
        'function': create_parallel_or_lba_model,
        'description': 'Parallel OR LBA: minimum drift rate (min(v_left, v_right))', 
        'integration_type': 'Minimum'
    },
    'Weighted_Average': {
        'function': create_weighted_average_lba_model,
        'description': 'Weighted Average LBA: weighted combination of drift rates',
        'integration_type': 'Weighted_Average'
    }
}

def get_available_models():
    return list(MODEL_REGISTRY.keys())

def create_model_by_name(model_name, observed_data, input_data):
    if model_name not in MODEL_REGISTRY:
        raise ValueError(f"Unknown model: {model_name}. Available: {get_available_models()}")
    
    model_func = MODEL_REGISTRY[model_name]['function']
    print(f"Creating model: {model_name}")
    return model_func(observed_data, input_data)

# åœ¨ lba_models.py ä¸­ä¿®å¾© CustomDist ä»¥ç¢ºä¿ log_likelihood æ­£ç¢ºç”Ÿæˆ

import pymc as pm
import pytensor.tensor as pt
import numpy as np
from lba_fixes import lba_logp_correct, safe_lba_random

def create_coactive_lba_model_fixed(observed_data, input_data):
    """
    ä¿®å¾©ç‰ˆ Coactive LBA Model - ç¢ºä¿ log_likelihood æ­£ç¢ºç”Ÿæˆ
    """
    with pm.Model() as model:
        # åƒæ•¸å®šç¾©ä¿æŒä¸è®Š
        v_match = pm.TruncatedNormal('v_match', mu=1.5, sigma=0.5, lower=0.5, upper=5.0)
        v_mismatch = pm.TruncatedNormal('v_mismatch', mu=1.0, sigma=0.5, lower=0.1, upper=3.0)
        start_var = pm.TruncatedNormal('start_var', mu=0.5, sigma=0.2, lower=0.1, upper=1.5)
        boundary_offset = pm.TruncatedNormal('boundary_offset', mu=1.0, sigma=0.3, lower=0.3, upper=2.0)
        b_safe = pm.Deterministic('b_safe', start_var + boundary_offset)
        
        # å‹•æ…‹è¨ˆç®— non_decision ä¸Šç•Œ
        min_rt_seconds = np.min(observed_data[:, 0]) / 1000.0
        upper_bound = pt.minimum(min_rt_seconds * 0.99, 0.5)
        non_decision = pm.TruncatedNormal('non_decision', mu=0.15, sigma=0.05, 
                                        lower=0.05, upper=upper_bound)
        
        # è¨ˆç®—æ¼‚ç§»ç‡
        v_left = v_match * input_data['left_match'] + v_mismatch * (1 - input_data['left_match'])
        v_right = v_match * input_data['right_match'] + v_mismatch * (1 - input_data['right_match'])
        v_final_correct = pm.Deterministic('v_final_correct', v_left + v_right)
        
        v_incorrect_base = pm.TruncatedNormal('v_incorrect_base', mu=0.5, sigma=0.3, lower=0.1, upper=2.0)
        v_final_incorrect = pm.Deterministic('v_final_incorrect', pt.full_like(v_final_correct, v_incorrect_base))
        
        # å †ç–Šæ¼‚ç§»ç‡
        v_rates = pt.stack([v_final_correct, v_final_incorrect], axis=1)
        
        # *** é—œéµä¿®å¾©ï¼šç¢ºä¿ log_likelihood æ­£ç¢ºç”Ÿæˆ ***
        # æ–¹æ³• 1: ä½¿ç”¨ pm.Potential ç›´æ¥æŒ‡å®š log_likelihood
        def lba_logp_wrapper(value, v_rates, start_var, b_safe, non_decision):
            """åŒ…è£å‡½æ•¸ç¢ºä¿æ­£ç¢ºçš„ log_likelihood è¨ˆç®—"""
            logp = lba_logp_correct(value, v_rates, start_var, b_safe, non_decision)
            # ç¢ºä¿è¿”å›çš„æ˜¯æ¨™é‡ï¼ˆæ‰€æœ‰è©¦é©—çš„ç¸½ log_likelihoodï¼‰
            return pt.sum(logp)
        
        # ä½¿ç”¨ pm.Potential è€Œä¸æ˜¯ CustomDist
        pm.Potential('likelihood_potential', 
                    lba_logp_wrapper(observed_data, v_rates, start_var, b_safe, non_decision))
        
        # åŒæ™‚ä½¿ç”¨ CustomDist ä¾†è™•ç†è§€æ¸¬æ•¸æ“šï¼Œä½†å°ˆæ³¨æ–¼ random åŠŸèƒ½
        likelihood = pm.CustomDist('likelihood',
                                  v_rates, start_var, b_safe, non_decision,
                                  logp=lba_logp_correct,
                                  random=safe_lba_random,
                                  observed=observed_data)
        
        # *** æ–¹æ³• 2: æ‰‹å‹•è¨ˆç®— log_likelihood ***
        # è¨ˆç®—æ¯å€‹è§€æ¸¬çš„ log_likelihood
        individual_logp = lba_logp_correct(observed_data, v_rates, start_var, b_safe, non_decision)
        
        # å°‡å…¶å­˜å„²ç‚ºç¢ºå®šæ€§è®Šé‡ï¼Œé€™æ¨£ ArviZ å¯ä»¥è¨ªå•å®ƒ
        pm.Deterministic('log_likelihood_manual', individual_logp)
    
    return model

def create_parallel_and_lba_model_fixed(observed_data, input_data):
    """
    ä¿®å¾©ç‰ˆ Parallel AND LBA Model
    """
    with pm.Model() as model:
        v_match = pm.TruncatedNormal('v_match', mu=1.5, sigma=0.5, lower=0.5, upper=5.0)
        v_mismatch = pm.TruncatedNormal('v_mismatch', mu=1.0, sigma=0.5, lower=0.1, upper=3.0)
        start_var = pm.TruncatedNormal('start_var', mu=0.5, sigma=0.2, lower=0.1, upper=1.5)
        boundary_offset = pm.TruncatedNormal('boundary_offset', mu=1.0, sigma=0.3, lower=0.3, upper=2.0)
        b_safe = pm.Deterministic('b_safe', start_var + boundary_offset)
        
        min_rt_seconds = np.min(observed_data[:, 0]) / 1000.0
        upper_bound = pt.minimum(min_rt_seconds * 0.99, 0.5)
        non_decision = pm.TruncatedNormal('non_decision', mu=0.15, sigma=0.05,
                                        lower=0.05, upper=upper_bound)
        
        v_left = v_match * input_data['left_match'] + v_mismatch * (1 - input_data['left_match'])
        v_right = v_match * input_data['right_match'] + v_mismatch * (1 - input_data['right_match'])
        v_final_correct = pm.Deterministic('v_final_correct', pt.maximum(v_left, v_right))
        
        v_incorrect_base = pm.TruncatedNormal('v_incorrect_base', mu=0.5, sigma=0.3, lower=0.1, upper=2.0)
        v_final_incorrect = pm.Deterministic('v_final_incorrect', pt.full_like(v_final_correct, v_incorrect_base))
        
        v_rates = pt.stack([v_final_correct, v_final_incorrect], axis=1)
        
        # åŒæ¨£çš„ä¿®å¾©
        def lba_logp_wrapper(value, v_rates, start_var, b_safe, non_decision):
            logp = lba_logp_correct(value, v_rates, start_var, b_safe, non_decision)
            return pt.sum(logp)
        
        pm.Potential('likelihood_potential',
                    lba_logp_wrapper(observed_data, v_rates, start_var, b_safe, non_decision))
        
        likelihood = pm.CustomDist('likelihood',
                                  v_rates, start_var, b_safe, non_decision,
                                  logp=lba_logp_correct,
                                  random=safe_lba_random,
                                  observed=observed_data)
        
        individual_logp = lba_logp_correct(observed_data, v_rates, start_var, b_safe, non_decision)
        pm.Deterministic('log_likelihood_manual', individual_logp)
    
    return model

# æ›´æ–°æ¨¡å‹è¨»å†Šè¡¨
MODEL_REGISTRY_FIXED = {
    'Coactive_Addition': {
        'function': create_coactive_lba_model_fixed,
        'description': 'Coactive LBA: drift rates are summed (v_left + v_right) - LOG_LIKELIHOOD FIXED',
        'integration_type': 'Addition'
    },
    'Parallel_AND_Maximum': {
        'function': create_parallel_and_lba_model_fixed,
        'description': 'Parallel AND LBA: maximum drift rate (max(v_left, v_right)) - LOG_LIKELIHOOD FIXED',
        'integration_type': 'Maximum'
    }
}

def create_model_by_name_fixed(model_name, observed_data, input_data):
    """
    ä¿®å¾©ç‰ˆæ¨¡å‹å‰µå»ºå‡½æ•¸
    """
    if model_name not in MODEL_REGISTRY_FIXED:
        # å›é€€åˆ°åŸå§‹è¨»å†Šè¡¨
        if model_name in MODEL_REGISTRY:
            print(f"âš ï¸ ä½¿ç”¨åŸå§‹ç‰ˆæœ¬çš„ {model_name}ï¼ˆå¯èƒ½æœ‰ log_likelihood å•é¡Œï¼‰")
            return MODEL_REGISTRY[model_name]['function'](observed_data, input_data)
        else:
            raise ValueError(f"Unknown model: {model_name}. Available: {list(MODEL_REGISTRY_FIXED.keys())}")
    
    model_func = MODEL_REGISTRY_FIXED[model_name]['function']
    print(f"âœ… å‰µå»ºä¿®å¾©ç‰ˆæ¨¡å‹: {model_name}")
    return model_func(observed_data, input_data)

# å¢å¼·ç‰ˆæ¡æ¨£å‡½æ•¸ï¼Œç¢ºä¿ log_likelihood è¢«æ­£ç¢ºè¨ˆç®—
def sample_with_log_likelihood_fix(model, **kwargs):
    """
    æ¡æ¨£å¾Œä¿®å¾© log_likelihood å•é¡Œ
    """
    print("ğŸ”§ ä½¿ç”¨ log_likelihood ä¿®å¾©ç‰ˆæ¡æ¨£...")
    
    # å…ˆé€²è¡Œæ­£å¸¸æ¡æ¨£
    from LBA_tool import sample_with_convergence_check
    trace, diagnostics = sample_with_convergence_check(model, **kwargs)
    
    if trace is None:
        return None, None
    
    # æª¢æŸ¥ log_likelihood æ˜¯å¦å­˜åœ¨
    if not hasattr(trace, 'log_likelihood'):
        print("âš ï¸ trace ç¼ºå°‘ log_likelihoodï¼Œå˜—è©¦æ‰‹å‹•è¨ˆç®—...")
        
        try:
            # ä½¿ç”¨æ‰‹å‹•è¨ˆç®—çš„ log_likelihood
            if 'log_likelihood_manual' in trace.posterior:
                print("âœ… æ‰¾åˆ°æ‰‹å‹•è¨ˆç®—çš„ log_likelihood")
                
                # å‰µå»ºä¸€å€‹åŒ…å« log_likelihood çš„æ–° trace
                # é€™æ˜¯ä¸€å€‹ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›å¯¦ç¾å¯èƒ½éœ€è¦æ›´è¤‡é›œçš„è™•ç†
                import xarray as xr
                
                # å°‡æ‰‹å‹• log_likelihood é‡å‘½å
                ll_data = trace.posterior['log_likelihood_manual']
                
                # å‰µå»ºæ–°çš„ log_likelihood æ•¸æ“šé›†
                log_likelihood = xr.Dataset({
                    'likelihood': ll_data
                })
                
                # å°‡å…¶æ·»åŠ åˆ° trace
                trace = trace.assign(log_likelihood=log_likelihood)
                print("âœ… log_likelihood ä¿®å¾©æˆåŠŸ")
            else:
                print("âŒ ç„¡æ³•æ‰¾åˆ°æ‰‹å‹•è¨ˆç®—çš„ log_likelihood")
        except Exception as e:
            print(f"âŒ log_likelihood ä¿®å¾©å¤±æ•—: {e}")
    else:
        print("âœ… trace å·²åŒ…å« log_likelihood")
    
    return trace, diagnostics

# æ¸¬è©¦å‡½æ•¸
def test_log_likelihood_fix():
    """
    æ¸¬è©¦ log_likelihood ä¿®å¾©æ˜¯å¦æœ‰æ•ˆ
    """
    print("ğŸ§ª æ¸¬è©¦ log_likelihood ä¿®å¾©...")
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    n_trials = 50
    test_data = np.zeros((n_trials, 2))
    test_data[:, 0] = np.random.uniform(300, 1500, n_trials)
    test_data[:, 1] = np.random.binomial(1, 0.8, n_trials)
    
    test_input = {
        'left_match': np.random.binomial(1, 0.5, n_trials).astype(float),
        'right_match': np.random.binomial(1, 0.5, n_trials).astype(float)
    }
    
    try:
        # æ¸¬è©¦ä¿®å¾©ç‰ˆæ¨¡å‹
        model = create_model_by_name_fixed('Coactive_Addition', test_data, test_input)
        print("âœ… ä¿®å¾©ç‰ˆæ¨¡å‹å‰µå»ºæˆåŠŸ")
        
        # æ¸¬è©¦æ¨¡å‹ç·¨è­¯
        with model:
            test_point = model.initial_point()
            logp = model.compile_logp()
            logp_val = logp(test_point)
            print(f"âœ… æ¨¡å‹ç·¨è­¯æˆåŠŸï¼Œlogp: {logp_val:.2f}")
        
        # å¿«é€Ÿæ¡æ¨£æ¸¬è©¦
        trace, diagnostics = sample_with_log_likelihood_fix(
            model, max_attempts=1, draws=100, tune=200
        )
        
        if trace is not None:
            # æª¢æŸ¥ log_likelihood
            if hasattr(trace, 'log_likelihood'):
                print("âœ… log_likelihood å­˜åœ¨")
                
                # æ¸¬è©¦ WAIC è¨ˆç®—
                try:
                    waic_result = az.waic(trace)
                    print(f"âœ… WAIC è¨ˆç®—æˆåŠŸ: {waic_result.elpd_waic:.2f}")
                    return True
                except Exception as e:
                    print(f"âŒ WAIC è¨ˆç®—å¤±æ•—: {e}")
                    return False
            else:
                print("âŒ log_likelihood ä»ç„¶ç¼ºå¤±")
                return False
        else:
            print("âŒ æ¡æ¨£å¤±æ•—")
            return False
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        return False
def manual_model_comparison(models):
    """æ‰‹å‹•æ¨¡å‹æ¯”è¼ƒç•¶ ArviZ å¤±æ•—æ™‚"""
    
    results = {}
    
    for name, trace in models.items():
        try:
            # å˜—è©¦æ‰‹å‹• WAIC è¨ˆç®—
            if hasattr(trace, 'log_likelihood'):
                ll = trace.log_likelihood.likelihood.values
                
                # æ¸…ç†ç•°å¸¸å€¼
                ll_clean = ll[np.isfinite(ll)]
                
                if len(ll_clean) > 0:
                    # ç°¡åŒ–çš„ WAIC è¿‘ä¼¼
                    lppd = np.sum(np.log(np.mean(np.exp(ll_clean), axis=0)))
                    p_waic = np.sum(np.var(ll_clean, axis=0))
                    waic = -2 * (lppd - p_waic)
                    
                    results[name] = {
                        'waic': waic,
                        'lppd': lppd,
                        'p_waic': p_waic,
                        'valid': True
                    }
                else:
                    results[name] = {'valid': False, 'reason': 'No valid log_likelihood'}
            else:
                results[name] = {'valid': False, 'reason': 'No log_likelihood'}
                
        except Exception as e:
            results[name] = {'valid': False, 'reason': str(e)}
    
    # æ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼ˆæœ€ä½ WAICï¼‰
    valid_results = {k: v for k, v in results.items() if v.get('valid', False)}
    
    if valid_results:
        winner = min(valid_results, key=lambda k: valid_results[k]['waic'])
        return {
            'winner': winner,
            'method': 'Manual_WAIC',
            'results': results,
            'success': True
        }
    else:
        return {
            'winner': list(models.keys())[0],
            'method': 'Default',
            'results': results,
            'success': False
        }
    
if __name__ == "__main__":
    print("ğŸ”§ LBA log_likelihood ä¿®å¾©å·¥å…·")
    
    # é‹è¡Œæ¸¬è©¦
    success = test_log_likelihood_fix()
    if success:
        print("\nâœ… log_likelihood ä¿®å¾©æ¸¬è©¦é€šéï¼")
        print("ç¾åœ¨å¯ä»¥åœ¨ lba_models.py ä¸­ä½¿ç”¨ create_model_by_name_fixed")
    else:
        print("\nâŒ log_likelihood ä¿®å¾©æ¸¬è©¦å¤±æ•—")