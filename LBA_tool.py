# -*- coding: utf-8 -*-
"""
Created on Sun Jun 29 03:29:44 2025

@author: spt904
"""

"""
LBA Analysis - Utility Functions Module
è¼”åŠ©å‡½æ•¸å’Œå·¥å…·ï¼ŒåŒ…å«æ¡æ¨£ã€è¨ºæ–·ã€æ•¸æ“šè™•ç†ç­‰
"""

import numpy as np
import pandas as pd
import pymc as pm
import arviz as az
import os
import warnings
from datetime import datetime

# FIXED: Remove the conflicting import - we'll use the function defined below
# from lba_fixes import robust_sample_with_convergence_check as sample_with_convergence_check

# --- LBA_tool.py ä¸­ä¿®æ­£å¾Œçš„ç¨‹å¼ç¢¼ ---

# LBA_tool.py çš„å®Œæ•´ä¿®æ­£å‡½æ•¸

def sample_with_convergence_check(model, max_attempts=2, draws=200, tune=300, chains=2, target_accept=0.90):
    """
    ä¿®å¾©å¾Œçš„æ¨¡å‹æ¡æ¨£å‡½æ•¸ï¼ŒåŒ…å«æ”¶æ–‚æª¢æŸ¥ (å·²ä¿®æ­£ SyntaxError)
    """
    
    print(f"  é–‹å§‹æ¡æ¨£ (draws={draws}, tune={tune}, chains={chains})...")
    
    for attempt in range(max_attempts):
        try: # <--- try å€å¡Šé–‹å§‹
            print(f"    å˜—è©¦ {attempt + 1}/{max_attempts}")
            
            with model:
                # ç”±æ–¼ find_MAP ä¸ç©©å®šï¼Œæˆ‘å€‘ç›´æ¥è·³éå®ƒ
                print("    âš ï¸ å·²åœç”¨ find_MAPï¼Œä½¿ç”¨é è¨­çš„ 'jitter+adapt_diag' åˆå§‹åŒ–ã€‚")
                
                # åŸ·è¡Œæ¡æ¨£
                trace = pm.sample(
                    draws=draws,
                    tune=tune,
                    chains=chains,
                    target_accept=target_accept,
                    return_inferencedata=True,
                    progressbar=True,
                    random_seed=42 + attempt,
                    init='jitter+adapt_diag',
                    cores=1
                )
                
                # æ”¶æ–‚è¨ºæ–·
                diagnostics = check_convergence_diagnostics(trace)
                
                print(f"    æœ€å¤§ R-hat: {diagnostics['max_rhat']:.4f}")
                print(f"    æœ€å° ESS bulk: {diagnostics['min_ess']:.0f}")
                
                # æ”¾å¯¬çš„æ”¶æ–‚æ¨™æº–
                convergence_ok = (
                    diagnostics['max_rhat'] < 1.2 and
                    diagnostics['min_ess'] > 50
                )
                
                if convergence_ok:
                    print(f"    âœ“ æ”¶æ–‚æˆåŠŸ (å˜—è©¦ {attempt + 1})")
                    return trace, diagnostics
                else:
                    print(f"    âš ï¸ æ”¶æ–‚æ¨™æº–æœªé”æˆï¼Œä½†ç¹¼çºŒ...")
                    if attempt == max_attempts - 1:
                        print(f"    âš ï¸ æœ€å¾Œå˜—è©¦ï¼Œè¿”å›ç•¶å‰çµæœ")
                        return trace, diagnostics

        # *** é€™æ˜¯ä¿®æ­£çš„éƒ¨åˆ†ï¼šåŠ ä¸Šå°æ‡‰ try çš„ except å€å¡Š ***
        except Exception as e:
            print(f"    âŒ æ¡æ¨£æˆ–è¨ºæ–·å¤±æ•—: {e}")
            if attempt < max_attempts - 1:
                print(f"    èª¿æ•´åƒæ•¸å¾Œé‡è©¦...")
                # ç°¡å–®åœ°èª¿æ•´åƒæ•¸ä»¥å¢åŠ æˆåŠŸçš„æ©Ÿæœƒ
                draws = max(100, int(draws * 0.9))
                tune = max(150, int(tune * 1.1))
                target_accept = min(0.98, target_accept + 0.02)
            
    print(f"    âŒ {max_attempts} æ¬¡å˜—è©¦å¾Œä»æœªæˆåŠŸ")
    return None, None
def check_convergence_diagnostics(trace):
    """
    æª¢æŸ¥ MCMC æ”¶æ–‚è¨ºæ–·
    """
    
    try:
        summary = az.summary(trace)
        
        # åŸºæœ¬è¨ºæ–·æŒ‡æ¨™
        diagnostics = {
            'max_rhat': summary['r_hat'].max() if 'r_hat' in summary.columns else np.nan,
            'min_ess': summary['ess_bulk'].min() if 'ess_bulk' in summary.columns else np.nan,
            'min_ess_tail': summary['ess_tail'].min() if 'ess_tail' in summary.columns else np.nan,
            'mean_accept_stat': np.nan,
            'n_divergent': 0,
            'n_parameters': len(summary)
        }
        
        # å˜—è©¦ç²å–æ¡æ¨£çµ±è¨ˆ
        try:
            if hasattr(trace, 'sample_stats'):
                if hasattr(trace.sample_stats, 'diverging'):
                    diagnostics['n_divergent'] = int(trace.sample_stats.diverging.sum())
                
                if hasattr(trace.sample_stats, 'accept'):
                    diagnostics['mean_accept_stat'] = float(trace.sample_stats.accept.mean())
        except Exception:
            pass  # å¦‚æœç„¡æ³•ç²å–æ¡æ¨£çµ±è¨ˆï¼Œä½¿ç”¨é»˜èªå€¼
        
        return diagnostics
        
    except Exception as e:
        print(f"Warning: ç„¡æ³•è¨ˆç®—è¨ºæ–·æŒ‡æ¨™: {e}")
        return {
            'max_rhat': np.nan,
            'min_ess': np.nan,
            'min_ess_tail': np.nan,
            'n_divergent': np.nan,
            'mean_accept_stat': np.nan,
            'n_parameters': 0
        }

def calculate_sigma_matrices_from_traces(models, participant_id, save_dir=None):
    """
    å¾ traces è¨ˆç®— sigma matrices
    """
    
    print(f"  Computing sigma matrices...")
    
    sigma_results = {}
    
    for model_name, trace in models.items():
        try:
            # IMPROVED: Use safe parameter extraction from our fixes
            try:
                from LBA_tool_fixes import safe_parameter_extraction
                v_correct_samples = safe_parameter_extraction(trace, 'v_final_correct')
                v_incorrect_samples = safe_parameter_extraction(trace, 'v_final_incorrect')
            except ImportError:
                # Fallback to original method if fixes not available
                v_correct_samples = trace.posterior['v_final_correct'].values.flatten()
                v_incorrect_samples = trace.posterior['v_final_incorrect'].values.flatten()
            
            # å‰µå»ºæ¨£æœ¬çŸ©é™£
            samples_matrix = np.column_stack([v_correct_samples, v_incorrect_samples])
            
            # è¨ˆç®—å”æ–¹å·®çŸ©é™£
            cov_matrix = np.cov(samples_matrix.T)
            
            # è¨ˆç®—ç›¸é—œä¿‚æ•¸çŸ©é™£
            corr_matrix = np.corrcoef(samples_matrix.T)
            
            # æå–é—œéµçµ±è¨ˆé‡
            correlation = corr_matrix[0, 1]
            variance_A = cov_matrix[0, 0]  # v_correct è®Šç•°æ•¸
            variance_B = cov_matrix[1, 1]  # v_incorrect è®Šç•°æ•¸
            covariance = cov_matrix[0, 1]
            
            # è¨ˆç®—æ¢ä»¶æ•¸
            condition_number = np.linalg.cond(cov_matrix)
            
            # è¨ˆç®—è¡Œåˆ—å¼
            determinant = np.linalg.det(cov_matrix)
            
            sigma_data = {
                'model': model_name,
                'participant': participant_id,
                'correlation': correlation,
                'variance_A': variance_A,
                'variance_B': variance_B,
                'covariance': covariance,
                'condition_number': condition_number,
                'determinant': determinant,
                'cov_matrix': cov_matrix,
                'corr_matrix': corr_matrix
            }
            
            sigma_results[model_name] = sigma_data
            
            print(f"    {model_name}: correlation = {correlation:.3f}, condition = {condition_number:.1f}")
            
        except Exception as e:
            print(f"    âŒ {model_name}: Error computing sigma matrix: {e}")
    
    # ä¿å­˜çµæœï¼ˆå¦‚æœæŒ‡å®šäº†ç›®éŒ„ï¼‰
    if save_dir and sigma_results:
        sigma_file = os.path.join(save_dir, f'participant_{participant_id}_sigma_matrices.npz')
        save_data = {}
        
        for model_name, data in sigma_results.items():
            for key, value in data.items():
                if isinstance(value, np.ndarray):
                    save_data[f'{model_name}_{key}'] = value
                else:
                    save_data[f'{model_name}_{key}'] = value
        
        np.savez(sigma_file, **save_data)
        print(f"    âœ“ Sigma matrices saved")
    
    return sigma_results

def improved_model_comparison(models, method='auto'):
    """
    å®Œå…¨é‡å¯«çš„æ¨¡å‹æ¯”è¼ƒå‡½æ•¸ - è§£æ±º WAIC/LOO å¤±æ•—å•é¡Œ
    æ›¿æ› LBA_tool.py ä¸­çš„åŸå§‹å‡½æ•¸
    """
    
    print("ğŸ”¬ é–‹å§‹å¢å¼·ç‰ˆæ¨¡å‹æ¯”è¼ƒ...")
    
    if len(models) < 2:
        print("âŒ éœ€è¦è‡³å°‘ 2 å€‹æ¨¡å‹é€²è¡Œæ¯”è¼ƒ")
        return None
    
    # æ­¥é©Ÿ 1: è¨ºæ–·æ¯å€‹æ¨¡å‹
    print("\nğŸ“‹ è¨ºæ–·æ‰€æœ‰æ¨¡å‹...")
    model_diagnostics = {}
    valid_models = {}
    
    for model_name, trace in models.items():
        print(f"  æª¢æŸ¥ {model_name}...")
        
        # æª¢æŸ¥åŸºæœ¬è¦æ±‚
        has_log_likelihood = hasattr(trace, 'log_likelihood')
        has_posterior = hasattr(trace, 'posterior')
        
        if has_log_likelihood:
            try:
                ll_values = trace.log_likelihood.likelihood.values
                n_nan = np.isnan(ll_values).sum()
                n_inf = np.isinf(ll_values).sum()
                n_total = ll_values.size
                
                if n_nan == 0 and n_inf == 0:
                    valid_models[model_name] = trace
                    print(f"    âœ“ {model_name}: log_likelihood æ­£å¸¸")
                else:
                    print(f"    âŒ {model_name}: log_likelihood æœ‰ {n_nan} NaN, {n_inf} inf")
            except:
                print(f"    âŒ {model_name}: log_likelihood ç„¡æ³•è¨ªå•")
        else:
            print(f"    âŒ {model_name}: ç¼ºå°‘ log_likelihood")
        
        model_diagnostics[model_name] = {
            'has_log_likelihood': has_log_likelihood,
            'has_posterior': has_posterior
        }
    
    # æ­¥é©Ÿ 2: å¦‚æœæœ‰æœ‰æ•ˆæ¨¡å‹ï¼Œå˜—è©¦æ¨™æº–æ–¹æ³•
    if len(valid_models) >= 2:
        print(f"\nğŸ“Š æ‰¾åˆ° {len(valid_models)} å€‹æœ‰æ•ˆæ¨¡å‹ï¼Œå˜—è©¦æ¨™æº–æ–¹æ³•...")
        
        # å˜—è©¦ WAIC
        try:
            print("  å˜—è©¦ WAIC...")
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                comparison_result = az.compare(valid_models, ic='waic')
            
            winner = comparison_result.index[0]
            
            if len(comparison_result) >= 2:
                elpd_diff = comparison_result.iloc[1]['elpd_diff']
                dse = comparison_result.iloc[1]['dse'] if 'dse' in comparison_result.columns else 1
            else:
                elpd_diff = 0
                dse = 1
            
            effect_size = abs(elpd_diff / dse) if dse > 0 else 0
            significance = 'Significant' if effect_size > 2 else ('Weak' if effect_size > 1 else 'Non-significant')
            
            print(f"    âœ… WAIC æˆåŠŸï¼ç²å‹è€…: {winner}")
            
            return {
                'winner': winner,
                'method': 'WAIC',
                'elpd_diff': elpd_diff,
                'dse': dse,
                'effect_size': effect_size,
                'significance': significance,
                'comparison_table': comparison_result,
                'success': True
            }
            
        except Exception as e:
            print(f"    âŒ WAIC å¤±æ•—: {str(e)[:100]}...")
        
        # å˜—è©¦ LOO
        try:
            print("  å˜—è©¦ LOO...")
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                comparison_result = az.compare(valid_models, ic='loo')
            
            winner = comparison_result.index[0]
            
            if len(comparison_result) >= 2:
                elpd_diff = comparison_result.iloc[1]['elpd_diff']
                dse = comparison_result.iloc[1]['dse'] if 'dse' in comparison_result.columns else 1
            else:
                elpd_diff = 0
                dse = 1
            
            effect_size = abs(elpd_diff / dse) if dse > 0 else 0
            significance = 'Significant' if effect_size > 2 else ('Weak' if effect_size > 1 else 'Non-significant')
            
            print(f"    âœ… LOO æˆåŠŸï¼ç²å‹è€…: {winner}")
            
            return {
                'winner': winner,
                'method': 'LOO', 
                'elpd_diff': elpd_diff,
                'dse': dse,
                'effect_size': effect_size,
                'significance': significance,
                'comparison_table': comparison_result,
                'success': True
            }
            
        except Exception as e:
            print(f"    âŒ LOO å¤±æ•—: {str(e)[:100]}...")
    
    # æ­¥é©Ÿ 3: ä½¿ç”¨æ›¿ä»£æ¯”è¼ƒæ–¹æ³•
    print("\nğŸ”„ ä½¿ç”¨æ›¿ä»£æ¯”è¼ƒæ–¹æ³•...")
    
    model_scores = {}
    
    for model_name, trace in models.items():
        try:
            score = 0
            
            # æ–¹æ³• 1: å¦‚æœæœ‰ log_likelihoodï¼Œä½¿ç”¨å¹³å‡å€¼
            if hasattr(trace, 'log_likelihood'):
                try:
                    ll_values = trace.log_likelihood.likelihood.values
                    # æ¸…ç†ç•°å¸¸å€¼
                    ll_clean = ll_values[np.isfinite(ll_values)]
                    if len(ll_clean) > 0:
                        score = np.mean(ll_clean)
                        print(f"    {model_name}: å¹³å‡ log-likelihood = {score:.2f}")
                        model_scores[model_name] = score
                        continue
                except:
                    pass
            
            # æ–¹æ³• 2: åŸºæ–¼æ”¶æ–‚æ€§è©•åˆ†
            try:
                summary = az.summary(trace)
                max_rhat = summary['r_hat'].max() if 'r_hat' in summary.columns else 1.0
                min_ess = summary['ess_bulk'].min() if 'ess_bulk' in summary.columns else 1000
                
                # æ”¶æ–‚è©•åˆ†ï¼šæ‡²ç½°é«˜ R-hatï¼Œçå‹µé«˜ ESS
                score = min_ess / max(max_rhat - 1.0, 0.01)
                print(f"    {model_name}: æ”¶æ–‚è©•åˆ† = {score:.2f}")
                model_scores[model_name] = score
                continue
            except:
                pass
            
            # æ–¹æ³• 3: åŸºæœ¬è©•åˆ†ï¼ˆæ¨£æœ¬æ•¸ï¼‰
            try:
                n_samples = len(trace.posterior.coords['draw']) * len(trace.posterior.coords['chain'])
                score = n_samples
                print(f"    {model_name}: æ¨£æœ¬æ•¸è©•åˆ† = {score}")
                model_scores[model_name] = score
            except:
                model_scores[model_name] = 0
                print(f"    {model_name}: ç„¡æ³•è©•åˆ†")
        
        except Exception as e:
            print(f"    âŒ {model_name} è©•åˆ†å¤±æ•—: {e}")
            model_scores[model_name] = 0
    
    # ç¢ºå®šç²å‹è€…
    if model_scores:
        winner = max(model_scores, key=model_scores.get)
        winner_score = model_scores[winner]
        
        # è¨ˆç®—æ•ˆæ‡‰é‡
        sorted_scores = sorted(model_scores.values(), reverse=True)
        if len(sorted_scores) >= 2 and sorted_scores[1] > 0:
            effect_size = (sorted_scores[0] - sorted_scores[1]) / abs(sorted_scores[1])
            effect_size = min(effect_size, 5.0)  # é™åˆ¶æœ€å¤§å€¼
        else:
            effect_size = 0
        
        significance = 'Significant' if effect_size > 0.5 else ('Weak' if effect_size > 0.2 else 'Non-significant')
        
        print(f"    ğŸ† æ›¿ä»£æ–¹æ³•ç²å‹è€…: {winner} (è©•åˆ†: {winner_score:.2f})")
        print(f"    æ•ˆæ‡‰é‡: {effect_size:.3f} ({significance})")
        
        return {
            'winner': winner,
            'method': 'Alternative',
            'elpd_diff': np.nan,
            'dse': np.nan,
            'effect_size': effect_size,
            'significance': significance,
            'comparison_table': None,
            'model_scores': model_scores,
            'success': True
        }
    else:
        print("    âŒ æ‰€æœ‰è©•åˆ†æ–¹æ³•éƒ½å¤±æ•—")
        # æœ€å¾Œæ‰‹æ®µï¼šé¸æ“‡ç¬¬ä¸€å€‹æ¨¡å‹
        winner = list(models.keys())[0]
        print(f"    ğŸ³ï¸ é»˜èªé¸æ“‡: {winner}")
        
        return {
            'winner': winner,
            'method': 'Default',
            'elpd_diff': np.nan,
            'dse': np.nan,
            'effect_size': np.nan,
            'significance': 'Unknown',
            'comparison_table': None,
            'success': False
        }
def safe_trace_summary(trace, model_name):
    """
    å®‰å…¨çš„ trace ç¸½çµå‡½æ•¸
    """
    try:
        summary = az.summary(trace)
        
        result = {
            'model_name': model_name,
            'n_parameters': len(summary),
            'max_rhat': summary['r_hat'].max() if 'r_hat' in summary.columns else np.nan,
            'min_ess_bulk': summary['ess_bulk'].min() if 'ess_bulk' in summary.columns else np.nan,
            'min_ess_tail': summary['ess_tail'].min() if 'ess_tail' in summary.columns else np.nan,
            'summary_table': summary
        }
        
        return result
        
    except Exception as e:
        print(f"    âš ï¸ {model_name} ç¸½çµè¨ˆç®—å¤±æ•—: {e}")
        return {
            'model_name': model_name,
            'n_parameters': 0,
            'max_rhat': np.nan,
            'min_ess_bulk': np.nan,
            'min_ess_tail': np.nan,
            'summary_table': None
        }

def extract_parameter_estimates(trace, model_name):
    """
    æå–åƒæ•¸ä¼°è¨ˆå€¼
    """
    try:
        estimates = {}
        
        # å¸¸è¦‹çš„ LBA åƒæ•¸
        param_names = ['v_match', 'v_mismatch', 'start_var', 'boundary_offset', 
                      'non_decision', 'v_final_correct', 'v_final_incorrect']
        
        for param in param_names:
            if param in trace.posterior:
                # IMPROVED: Try to use safe extraction if available
                try:
                    from LBA_tool_fixes import safe_parameter_extraction
                    samples = safe_parameter_extraction(trace, param)
                except ImportError:
                    samples = trace.posterior[param].values.flatten()
                
                estimates[param] = {
                    'mean': np.mean(samples),
                    'std': np.std(samples),
                    'median': np.median(samples),
                    'q025': np.percentile(samples, 2.5),
                    'q975': np.percentile(samples, 97.5)
                }
        
        return estimates
        
    except Exception as e:
        print(f"    âš ï¸ {model_name} åƒæ•¸æå–å¤±æ•—: {e}")
        return {}

def create_model_summary_report(models, participant_id, save_dir):
    """
    å‰µå»ºæ¨¡å‹ç¸½çµå ±å‘Š
    """
    try:
        report_lines = []
        report_lines.append(f"æ¨¡å‹ç¸½çµå ±å‘Š - åƒèˆ‡è€… {participant_id}")
        report_lines.append("=" * 50)
        report_lines.append("")
        
        for model_name, trace in models.items():
            report_lines.append(f"æ¨¡å‹: {model_name}")
            report_lines.append("-" * 30)
            
            # åŸºæœ¬çµ±è¨ˆ
            summary = safe_trace_summary(trace, model_name)
            report_lines.append(f"åƒæ•¸æ•¸é‡: {summary['n_parameters']}")
            report_lines.append(f"æœ€å¤§ R-hat: {summary['max_rhat']:.4f}")
            report_lines.append(f"æœ€å° ESS bulk: {summary['min_ess_bulk']:.0f}")
            report_lines.append(f"æœ€å° ESS tail: {summary['min_ess_tail']:.0f}")
            
            # åƒæ•¸ä¼°è¨ˆ
            estimates = extract_parameter_estimates(trace, model_name)
            if estimates:
                report_lines.append("\nä¸»è¦åƒæ•¸ä¼°è¨ˆ:")
                for param, stats in estimates.items():
                    if not np.isnan(stats['mean']):
                        report_lines.append(f"  {param}: {stats['mean']:.3f} Â± {stats['std']:.3f}")
            
            report_lines.append("")
        
        # ä¿å­˜å ±å‘Š
        report_file = os.path.join(save_dir, f'participant_{participant_id}_model_summary.txt')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"    âœ“ æ¨¡å‹ç¸½çµå ±å‘Šå·²ä¿å­˜: {report_file}")
        return report_file
        
    except Exception as e:
        print(f"    âŒ å‰µå»ºæ¨¡å‹ç¸½çµå ±å‘Šå¤±æ•—: {e}")
        return None
def improved_model_comparison(models, method='auto'):
    """
    å®Œå…¨é‡å¯«çš„æ¨¡å‹æ¯”è¼ƒå‡½æ•¸ - è§£æ±º WAIC/LOO å¤±æ•—å•é¡Œ
    æ›¿æ› LBA_tool.py ä¸­çš„åŸå§‹å‡½æ•¸
    """
    
    print("ğŸ”¬ é–‹å§‹å¢å¼·ç‰ˆæ¨¡å‹æ¯”è¼ƒ...")
    
    if len(models) < 2:
        print("âŒ éœ€è¦è‡³å°‘ 2 å€‹æ¨¡å‹é€²è¡Œæ¯”è¼ƒ")
        return None
    
    # æ­¥é©Ÿ 1: è¨ºæ–·æ¯å€‹æ¨¡å‹
    print("\nğŸ“‹ è¨ºæ–·æ‰€æœ‰æ¨¡å‹...")
    model_diagnostics = {}
    valid_models = {}
    
    for model_name, trace in models.items():
        print(f"  æª¢æŸ¥ {model_name}...")
        
        # æª¢æŸ¥åŸºæœ¬è¦æ±‚
        has_log_likelihood = hasattr(trace, 'log_likelihood')
        has_posterior = hasattr(trace, 'posterior')
        
        if has_log_likelihood:
            try:
                ll_values = trace.log_likelihood.likelihood.values
                n_nan = np.isnan(ll_values).sum()
                n_inf = np.isinf(ll_values).sum()
                n_total = ll_values.size
                
                if n_nan == 0 and n_inf == 0:
                    valid_models[model_name] = trace
                    print(f"    âœ“ {model_name}: log_likelihood æ­£å¸¸")
                else:
                    print(f"    âŒ {model_name}: log_likelihood æœ‰ {n_nan} NaN, {n_inf} inf")
            except:
                print(f"    âŒ {model_name}: log_likelihood ç„¡æ³•è¨ªå•")
        else:
            print(f"    âŒ {model_name}: ç¼ºå°‘ log_likelihood")
        
        model_diagnostics[model_name] = {
            'has_log_likelihood': has_log_likelihood,
            'has_posterior': has_posterior
        }
    
    # æ­¥é©Ÿ 2: å¦‚æœæœ‰æœ‰æ•ˆæ¨¡å‹ï¼Œå˜—è©¦æ¨™æº–æ–¹æ³•
    if len(valid_models) >= 2:
        print(f"\nğŸ“Š æ‰¾åˆ° {len(valid_models)} å€‹æœ‰æ•ˆæ¨¡å‹ï¼Œå˜—è©¦æ¨™æº–æ–¹æ³•...")
        
        # å˜—è©¦ WAIC
        try:
            print("  å˜—è©¦ WAIC...")
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                comparison_result = az.compare(valid_models, ic='waic')
            
            winner = comparison_result.index[0]
            
            if len(comparison_result) >= 2:
                elpd_diff = comparison_result.iloc[1]['elpd_diff']
                dse = comparison_result.iloc[1]['dse'] if 'dse' in comparison_result.columns else 1
            else:
                elpd_diff = 0
                dse = 1
            
            effect_size = abs(elpd_diff / dse) if dse > 0 else 0
            significance = 'Significant' if effect_size > 2 else ('Weak' if effect_size > 1 else 'Non-significant')
            
            print(f"    âœ… WAIC æˆåŠŸï¼ç²å‹è€…: {winner}")
            
            return {
                'winner': winner,
                'method': 'WAIC',
                'elpd_diff': elpd_diff,
                'dse': dse,
                'effect_size': effect_size,
                'significance': significance,
                'comparison_table': comparison_result,
                'success': True
            }
            
        except Exception as e:
            print(f"    âŒ WAIC å¤±æ•—: {str(e)[:100]}...")
        
        # å˜—è©¦ LOO
        try:
            print("  å˜—è©¦ LOO...")
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                comparison_result = az.compare(valid_models, ic='loo')
            
            winner = comparison_result.index[0]
            
            if len(comparison_result) >= 2:
                elpd_diff = comparison_result.iloc[1]['elpd_diff']
                dse = comparison_result.iloc[1]['dse'] if 'dse' in comparison_result.columns else 1
            else:
                elpd_diff = 0
                dse = 1
            
            effect_size = abs(elpd_diff / dse) if dse > 0 else 0
            significance = 'Significant' if effect_size > 2 else ('Weak' if effect_size > 1 else 'Non-significant')
            
            print(f"    âœ… LOO æˆåŠŸï¼ç²å‹è€…: {winner}")
            
            return {
                'winner': winner,
                'method': 'LOO', 
                'elpd_diff': elpd_diff,
                'dse': dse,
                'effect_size': effect_size,
                'significance': significance,
                'comparison_table': comparison_result,
                'success': True
            }
            
        except Exception as e:
            print(f"    âŒ LOO å¤±æ•—: {str(e)[:100]}...")
    
    # æ­¥é©Ÿ 3: ä½¿ç”¨æ›¿ä»£æ¯”è¼ƒæ–¹æ³•
    print("\nğŸ”„ ä½¿ç”¨æ›¿ä»£æ¯”è¼ƒæ–¹æ³•...")
    
    model_scores = {}
    
    for model_name, trace in models.items():
        try:
            score = 0
            
            # æ–¹æ³• 1: å¦‚æœæœ‰ log_likelihoodï¼Œä½¿ç”¨å¹³å‡å€¼
            if hasattr(trace, 'log_likelihood'):
                try:
                    ll_values = trace.log_likelihood.likelihood.values
                    # æ¸…ç†ç•°å¸¸å€¼
                    ll_clean = ll_values[np.isfinite(ll_values)]
                    if len(ll_clean) > 0:
                        score = np.mean(ll_clean)
                        print(f"    {model_name}: å¹³å‡ log-likelihood = {score:.2f}")
                        model_scores[model_name] = score
                        continue
                except:
                    pass
            
            # æ–¹æ³• 2: åŸºæ–¼æ”¶æ–‚æ€§è©•åˆ†
            try:
                summary = az.summary(trace)
                max_rhat = summary['r_hat'].max() if 'r_hat' in summary.columns else 1.0
                min_ess = summary['ess_bulk'].min() if 'ess_bulk' in summary.columns else 1000
                
                # æ”¶æ–‚è©•åˆ†ï¼šæ‡²ç½°é«˜ R-hatï¼Œçå‹µé«˜ ESS
                score = min_ess / max(max_rhat - 1.0, 0.01)
                print(f"    {model_name}: æ”¶æ–‚è©•åˆ† = {score:.2f}")
                model_scores[model_name] = score
                continue
            except:
                pass
            
            # æ–¹æ³• 3: åŸºæœ¬è©•åˆ†ï¼ˆæ¨£æœ¬æ•¸ï¼‰
            try:
                n_samples = len(trace.posterior.coords['draw']) * len(trace.posterior.coords['chain'])
                score = n_samples
                print(f"    {model_name}: æ¨£æœ¬æ•¸è©•åˆ† = {score}")
                model_scores[model_name] = score
            except:
                model_scores[model_name] = 0
                print(f"    {model_name}: ç„¡æ³•è©•åˆ†")
        
        except Exception as e:
            print(f"    âŒ {model_name} è©•åˆ†å¤±æ•—: {e}")
            model_scores[model_name] = 0
    
    # ç¢ºå®šç²å‹è€…
    if model_scores:
        winner = max(model_scores, key=model_scores.get)
        winner_score = model_scores[winner]
        
        # è¨ˆç®—æ•ˆæ‡‰é‡
        sorted_scores = sorted(model_scores.values(), reverse=True)
        if len(sorted_scores) >= 2 and sorted_scores[1] > 0:
            effect_size = (sorted_scores[0] - sorted_scores[1]) / abs(sorted_scores[1])
            effect_size = min(effect_size, 5.0)  # é™åˆ¶æœ€å¤§å€¼
        else:
            effect_size = 0
        
        significance = 'Significant' if effect_size > 0.5 else ('Weak' if effect_size > 0.2 else 'Non-significant')
        
        print(f"    ğŸ† æ›¿ä»£æ–¹æ³•ç²å‹è€…: {winner} (è©•åˆ†: {winner_score:.2f})")
        print(f"    æ•ˆæ‡‰é‡: {effect_size:.3f} ({significance})")
        
        return {
            'winner': winner,
            'method': 'Alternative',
            'elpd_diff': np.nan,
            'dse': np.nan,
            'effect_size': effect_size,
            'significance': significance,
            'comparison_table': None,
            'model_scores': model_scores,
            'success': True
        }
    else:
        print("    âŒ æ‰€æœ‰è©•åˆ†æ–¹æ³•éƒ½å¤±æ•—")
        # æœ€å¾Œæ‰‹æ®µï¼šé¸æ“‡ç¬¬ä¸€å€‹æ¨¡å‹
        winner = list(models.keys())[0]
        print(f"    ğŸ³ï¸ é»˜èªé¸æ“‡: {winner}")
        
        return {
            'winner': winner,
            'method': 'Default',
            'elpd_diff': np.nan,
            'dse': np.nan,
            'effect_size': np.nan,
            'significance': 'Unknown',
            'comparison_table': None,
            'success': False
        }
def quick_data_check(data_file):
    """
    å¿«é€Ÿæ•¸æ“šæª¢æŸ¥
    """
    try:
        print("åŸ·è¡Œå¿«é€Ÿæ•¸æ“šæª¢æŸ¥...")
        
        data = np.load(data_file, allow_pickle=True)
        observed_value = data['observed_value']
        participant_idx = data['participant_idx']
        model_input_data = data['model_input_data'].item()
        
        # åŸºæœ¬çµ±è¨ˆ
        n_trials = len(observed_value)
        n_participants = len(np.unique(participant_idx))
        rt_mean = observed_value[:, 0].mean()
        rt_std = observed_value[:, 0].std()
        accuracy = observed_value[:, 1].mean()
        
        print(f"âœ“ ç¸½è©¦é©—æ•¸: {n_trials}")
        print(f"âœ“ åƒèˆ‡è€…æ•¸: {n_participants}")
        print(f"âœ“ å¹³å‡ RT: {rt_mean:.2f}")
        print(f"âœ“ RT æ¨™æº–å·®: {rt_std:.2f}")
        print(f"âœ“ å¹³å‡æº–ç¢ºç‡: {accuracy:.3f}")
        
        # æª¢æŸ¥ RT å–®ä½
        if rt_mean < 10:
            print("âš ï¸ RT ä¼¼ä¹æ˜¯ç§’å–®ä½ï¼Œå»ºè­°è½‰æ›ç‚ºæ¯«ç§’")
            return False, "RT unit issue"
        
        # æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§
        if np.any(np.isnan(observed_value)):
            print("âš ï¸ æ•¸æ“šåŒ…å« NaN å€¼")
            return False, "NaN values"
        
        if np.any(observed_value[:, 0] <= 0):
            print("âš ï¸ ç™¼ç¾éæ­£æ•¸ RT å€¼")
            return False, "Invalid RT"
        
        print("âœ… æ•¸æ“šæª¢æŸ¥é€šé")
        return True, "OK"
        
    except Exception as e:
        print(f"âŒ æ•¸æ“šæª¢æŸ¥å¤±æ•—: {e}")
        return False, str(e)
def quick_comparison_test(models):
    """å¿«é€Ÿæ¸¬è©¦æ¨¡å‹æ¯”è¼ƒä¿®å¾©"""
    print("ğŸ§ª æ¸¬è©¦æ¨¡å‹æ¯”è¼ƒä¿®å¾©...")
    
    if not models or len(models) < 2:
        print("âŒ éœ€è¦è‡³å°‘ 2 å€‹æ¨¡å‹é€²è¡Œæ¸¬è©¦")
        return False
    
    try:
        result = improved_model_comparison(models)
        
        if result and result.get('success', False):
            print(f"âœ… æ¯”è¼ƒæˆåŠŸï¼ç²å‹è€…: {result['winner']}")
            print(f"   æ–¹æ³•: {result['method']}")
            return True
        else:
            print("âŒ æ¯”è¼ƒå¤±æ•—")
            return False
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        return False
# å¦‚æœç›´æ¥é‹è¡Œæ­¤æ¨¡çµ„
if __name__ == '__main__':
    print("LBA Analysis - Utility Functions Module (ä¿®å¾©ç‰ˆ)")
    print("æ­¤æ¨¡çµ„åŒ…å«æ¡æ¨£ã€è¨ºæ–·å’Œæ•¸æ“šè™•ç†åŠŸèƒ½")
    
    # æ¸¬è©¦æ•¸æ“šæª¢æŸ¥åŠŸèƒ½
    print("\næ¸¬è©¦æ•¸æ“šæª¢æŸ¥...")
    try:
        success, message = quick_data_check('model_data.npz')
        if success:
            print("âœ… å·¥å…·æ¨¡çµ„æ¸¬è©¦é€šé")
        else:
            print(f"âš ï¸ æ•¸æ“šå•é¡Œ: {message}")
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        print("è«‹ç¢ºä¿ model_data.npz æ–‡ä»¶å­˜åœ¨")