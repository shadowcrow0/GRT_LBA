# -*- coding: utf-8 -*-
"""
Linear Ballistic Accumulator (LBA) Model Visualization
LBA model visualization for four-choice line orientation judgment task
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import Rectangle
import seaborn as sns

# Configure font and style settings
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid", {'grid.alpha': 0.3})

def create_lba_figure():
    """
    Creates LBA model figure showing competition process between four accumulators
    
    Purpose: Visualize the classic LBA architecture and dual-channel implementation
    Implementation: 
    - Left plot: Classic LBA with 4 competing accumulators
    - Right plot: Dual-channel architecture specific to this experiment
    
    Variables:
    - A: Start point variability (uniform distribution range)
    - b: Decision threshold (boundary for decision making)
    - t0: Non-decision time (encoding + motor response time)
    - drift_rates: Evidence accumulation rates for each choice
    - colors: Color scheme for different choices
    """
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    
    # ============================================================================
    # Left plot: Classic LBA architecture
    # ============================================================================
    
    # LBA parameter settings
    A = 0.35      # Start point variability
    b = 0.75      # Decision threshold
    t0 = 0.3      # Non-decision time
    max_time = 2.0  # Maximum simulation time
    
    # Drift rates for four accumulators (based on experimental data)
    drift_rates = {
        'Choice 0 (\\|)': 2.1,   # Adjusted based on average usage rate
        'Choice 1 (\\/)': 2.4,   # Most frequently used, higher drift rate
        'Choice 2 (||)': 1.2,    # Least used, lower drift rate
        'Choice 3 (|/)': 2.6     # Second most used, highest drift rate
    }
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']  # Color palette for choices
    choice_labels = list(drift_rates.keys())
    
    # Draw accumulator trajectories
    time_points = np.linspace(t0, max_time, 1000)  # Time vector for simulation
    
    for i, (choice, drift) in enumerate(drift_rates.items()):
        # Start point (with random variability)
        start_point = np.random.uniform(0, A)
        
        # Accumulation trajectory (linear + noise)
        noise = np.random.normal(0, 0.1, len(time_points))  # Gaussian noise
        accumulation = start_point + drift * (time_points - t0) + np.cumsum(noise) * 0.05
        
        # Plot trajectory
        ax1.plot(time_points, accumulation, color=colors[i], linewidth=3, 
                label=choice, alpha=0.8)
        
        # Mark start point region
        if i == 0:
            ax1.axhspan(0, A, alpha=0.3, color='gray', 
                       label=f'Start Point Variability (A = {A})')
    
    # Add decision threshold line
    ax1.axhline(y=b, color='red', linestyle='--', linewidth=2, 
               label=f'Decision Threshold (b = {b})')
    
    # Add non-decision time line
    ax1.axvline(x=t0, color='orange', linestyle=':', linewidth=2,
               label=f'Non-decision Time (tâ‚€ = {t0}s)')
    
    # Set plot properties
    ax1.set_xlabel('Accumulation Time (seconds)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Evidence Accumulation', fontsize=12, fontweight='bold')
    ax1.set_title('LBA Model: Four-Choice Line Orientation Judgment\n(Parameters Based on Experimental Data)', 
                 fontsize=14, fontweight='bold')
    ax1.legend(loc='upper left', fontsize=10)
    ax1.grid(True, alpha=0.3)
    ax1.set_xlim(0, max_time)
    ax1.set_ylim(-0.1, 1.2)
    
    # Add winner annotation
    winner_time = 1.1
    winner_choice = 3  # Choice 3 wins
    ax1.annotate('Winner!', xy=(winner_time, b), xytext=(winner_time+0.2, b+0.2),
                arrowprops=dict(arrowstyle='->', color=colors[winner_choice], lw=2),
                fontsize=12, fontweight='bold', color=colors[winner_choice])
    
    # ============================================================================
    # Right plot: Dual-channel LBA architecture (your model)
    # ============================================================================
    
    # Draw dual-channel architecture
    ax2.set_xlim(0, 10)
    ax2.set_ylim(0, 10)
    
    # Left channel
    left_rect = Rectangle((1, 6), 3, 2.5, linewidth=2, edgecolor='blue', 
                         facecolor='lightblue', alpha=0.7)
    ax2.add_patch(left_rect)
    ax2.text(2.5, 7.2, 'Left Channel', ha='center', va='center', 
            fontsize=11, fontweight='bold')
    ax2.text(2.5, 6.5, 'left_bias\nleft_drift\nnoise_left', ha='center', va='center', 
            fontsize=9)
    
    # Right channel
    right_rect = Rectangle((6, 6), 3, 2.5, linewidth=2, edgecolor='green', 
                          facecolor='lightgreen', alpha=0.7)
    ax2.add_patch(right_rect)
    ax2.text(7.5, 7.2, 'Right Channel', ha='center', va='center', 
            fontsize=11, fontweight='bold')
    ax2.text(7.5, 6.5, 'right_bias\nright_drift\nnoise_right', ha='center', va='center', 
            fontsize=9)
    
    # Stimulus input
    stimulus_rect = Rectangle((4, 8.5), 2, 1, linewidth=2, edgecolor='purple', 
                             facecolor='lavender', alpha=0.7)
    ax2.add_patch(stimulus_rect)
    ax2.text(5, 9, 'Visual Stimulus\n(\\|, \\/, ||, |/)', ha='center', va='center', 
            fontsize=10, fontweight='bold')
    
    # Decision integration
    decision_rect = Rectangle((3.5, 3), 3, 1.5, linewidth=2, edgecolor='red', 
                             facecolor='mistyrose', alpha=0.7)
    ax2.add_patch(decision_rect)
    ax2.text(5, 3.75, 'Decision Integration', ha='center', va='center', 
            fontsize=11, fontweight='bold')
    
    # Four choice outputs
    choice_boxes = [
        (1.5, 0.5, 'Choice 0\n(\\|)'),
        (3.5, 0.5, 'Choice 1\n(\\/)'),
        (5.5, 0.5, 'Choice 2\n(||)'),
        (7.5, 0.5, 'Choice 3\n(|/)')
    ]
    
    for i, (x, y, label) in enumerate(choice_boxes):
        choice_rect = Rectangle((x-0.4, y), 0.8, 1, linewidth=2, 
                               edgecolor=colors[i], facecolor=colors[i], alpha=0.6)
        ax2.add_patch(choice_rect)
        ax2.text(x, y+0.5, label, ha='center', va='center', 
                fontsize=9, fontweight='bold')
    
    # Add connecting arrows
    # Stimulus to channels
    ax2.annotate('', xy=(2.5, 8.5), xytext=(4.5, 8.5),
                arrowprops=dict(arrowstyle='->', lw=2, color='blue'))
    ax2.annotate('', xy=(7.5, 8.5), xytext=(5.5, 8.5),
                arrowprops=dict(arrowstyle='->', lw=2, color='green'))
    
    # Channels to decision
    ax2.annotate('', xy=(4, 4.5), xytext=(2.5, 6),
                arrowprops=dict(arrowstyle='->', lw=2, color='blue'))
    ax2.annotate('', xy=(6, 4.5), xytext=(7.5, 6),
                arrowprops=dict(arrowstyle='->', lw=2, color='green'))
    
    # Decision to choices
    for i, (x, y, label) in enumerate(choice_boxes):
        ax2.annotate('', xy=(x, 1.5), xytext=(5, 3),
                    arrowprops=dict(arrowstyle='->', lw=1.5, color=colors[i]))
    
    # Add key finding annotation
    ax2.text(5, 2, 'Key Finding: r = -0.633 (Negative correlation between left-right bias)', 
            ha='center', va='center', fontsize=10, fontweight='bold',
            bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
    
    ax2.set_title('Dual-Channel LBA Model Architecture\n(Based on Your Experimental Design)', 
                 fontsize=14, fontweight='bold')
    ax2.axis('off')
    
    plt.tight_layout()
    return fig

def create_parameter_comparison():
    """
    Creates parameter comparison plot showing LBA parameter distributions across 18 subjects
    
    Purpose: Visualize individual differences in model parameters
    Implementation: Bar plots with mean lines and extreme value annotations
    
    Variables:
    - n_subjects: Number of participants (18)
    - subjects_data: Dictionary containing simulated parameter values
    - param_info: List of tuples with parameter names, descriptions, and color schemes
    """
    
    # Simulate parameters based on experimental results
    np.random.seed(42)
    n_subjects = 18
    
    # Simulate parameter distributions (based on actual results)
    subjects_data = {
        'left_bias': np.random.normal(-0.036, 0.185, n_subjects),     # Left channel bias
        'right_bias': np.random.normal(0.066, 0.201, n_subjects),    # Right channel bias
        'left_drift': np.random.gamma(2, 1, n_subjects),             # Left channel drift rate
        'right_drift': np.random.gamma(2, 1, n_subjects),            # Right channel drift rate
        'noise_left': np.random.gamma(1, 0.5, n_subjects),           # Left channel noise
        'noise_right': np.random.gamma(1, 0.5, n_subjects),          # Right channel noise
        'accuracy': np.random.normal(0.626, 0.204, n_subjects)       # Overall accuracy
    }
    
    # Ensure accuracy is within reasonable range
    subjects_data['accuracy'] = np.clip(subjects_data['accuracy'], 0.2, 0.9)
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('LBA Model Parameter Distributions - 18 Subjects', fontsize=16, fontweight='bold')
    
    # Parameter names and descriptions
    param_info = [
        ('left_bias', 'Left Channel Bias', 'Blues'),
        ('right_bias', 'Right Channel Bias', 'Greens'),
        ('left_drift', 'Left Channel Drift Rate', 'Reds'),
        ('right_drift', 'Right Channel Drift Rate', 'Purples'),
        ('noise_left', 'Left Channel Noise', 'Oranges'),
        ('noise_right', 'Right Channel Noise', 'Greys')
    ]
    
    for i, (param, title, cmap) in enumerate(param_info):
        row, col = i // 3, i % 3
        ax = axes[row, col]
        
        # Draw distribution plot
        data = subjects_data[param]
        colors = plt.cm.get_cmap(cmap)(np.linspace(0.3, 0.8, len(data)))
        
        bars = ax.bar(range(1, n_subjects+1), data, color=colors, alpha=0.7, edgecolor='black')
        
        # Add mean line
        mean_val = np.mean(data)
        ax.axhline(y=mean_val, color='red', linestyle='--', linewidth=2, 
                  label=f'Mean: {mean_val:.3f}')
        
        # Mark extreme values
        max_idx = np.argmax(data)
        min_idx = np.argmin(data)
        ax.annotate(f'Max: {data[max_idx]:.3f}', 
                   xy=(max_idx+1, data[max_idx]), 
                   xytext=(max_idx+1, data[max_idx]+0.1*np.std(data)),
                   arrowprops=dict(arrowstyle='->', color='red'),
                   fontsize=9, ha='center')
        
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.set_xlabel('Subject ID', fontsize=10)
        ax.set_ylabel('Parameter Value', fontsize=10)
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def create_correlation_heatmap():
    """
    Creates correlation heatmap showing Sigma matrix results
    
    Purpose: Visualize cross-channel correlations that violate independence assumptions
    Implementation: Heatmap with correlation values, significance markers, and color coding
    
    Variables:
    - correlation_matrix: 5x5 matrix of parameter correlations
    - labels: Parameter names for axis labels
    - significance_markers: Locations and symbols for statistical significance
    """
    
    # Based on Sigma matrix results
    correlation_matrix = np.array([
        [1.000, -0.633, 0.372, -0.612, -0.514],  # left_bias
        [-0.633, 1.000, -0.598, 0.969, -0.615],  # right_bias  
        [0.372, -0.598, 1.000, -0.588, 0.672],   # left_sensitivity
        [-0.612, 0.969, -0.588, 1.000, -0.615],  # right_sensitivity
        [-0.514, -0.615, 0.672, -0.615, 1.000]   # consistency
    ])
    
    labels = ['Left Bias', 'Right Bias', 'Left Sensitivity', 'Right Sensitivity', 'Consistency']
    
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # Create heatmap
    im = ax.imshow(correlation_matrix, cmap='RdBu_r', vmin=-1, vmax=1)
    
    # Add value annotations
    for i in range(len(labels)):
        for j in range(len(labels)):
            text = ax.text(j, i, f'{correlation_matrix[i, j]:.3f}',
                          ha="center", va="center", 
                          color="black" if abs(correlation_matrix[i, j]) < 0.5 else "white",
                          fontweight='bold', fontsize=11)
    
    # Set labels
    ax.set_xticks(np.arange(len(labels)))
    ax.set_yticks(np.arange(len(labels)))
    ax.set_xticklabels(labels, fontsize=12)
    ax.set_yticklabels(labels, fontsize=12)
    
    # Rotate x-axis labels
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    
    # Add colorbar
    cbar = ax.figure.colorbar(im, ax=ax)
    cbar.ax.set_ylabel('Correlation Coefficient', rotation=-90, va="bottom", fontsize=12)
    
    ax.set_title("Cross-Channel Correlation Matrix (Sigma Matrix)\nEvidence for Violation of Independence Assumption", 
                fontsize=14, fontweight='bold', pad=20)
    
    # Add significance markers
    significance_markers = [
        (0, 1, '**'), (1, 0, '**'),  # Bias correlation
        (2, 3, '***'), (3, 2, '***'),  # Sensitivity correlation
        (2, 4, '**'), (4, 2, '**')   # Consistency correlation
    ]
    
    for i, j, marker in significance_markers:
        ax.text(j+0.3, i-0.3, marker, ha="center", va="center", 
               color="white", fontweight='bold', fontsize=14)
    
    # Add legend
    legend_text = "Significance: * p < 0.05, ** p < 0.01, *** p < 0.001"
    ax.text(0.5, -0.15, legend_text, ha="center", va="center", 
           transform=ax.transAxes, fontsize=10, style='italic')
    
    plt.tight_layout()
    return fig

def main():
    """
    Main function: Creates all LBA model visualizations
    
    Purpose: Generate comprehensive visualization suite for LBA model analysis
    Implementation: Creates three main figures and saves them as high-resolution PNG files
    
    Output files:
    - lba_model_architecture.png: Basic LBA principles and dual-channel architecture
    - lba_parameter_distribution.png: Parameter distributions across 18 subjects
    - sigma_matrix_heatmap.png: Cross-channel correlations (evidence against independence)
    """
    
    print("ðŸ“Š Creating LBA model visualizations...")
    
    # Create main LBA figure
    fig1 = create_lba_figure()
    fig1.savefig('lba_model_architecture.png', dpi=300, bbox_inches='tight')
    print("âœ… LBA model architecture saved: lba_model_architecture.png")
    
    # Create parameter comparison plot
    fig2 = create_parameter_comparison()
    fig2.savefig('lba_parameter_distribution.png', dpi=300, bbox_inches='tight')
    print("âœ… Parameter distribution plot saved: lba_parameter_distribution.png")
    
    # Create correlation heatmap
    fig3 = create_correlation_heatmap()
    fig3.savefig('sigma_matrix_heatmap.png', dpi=300, bbox_inches='tight')
    print("âœ… Sigma matrix heatmap saved: sigma_matrix_heatmap.png")
    
    plt.show()
    
    print("\nðŸŽ¯ Figure descriptions:")
    print("1. lba_model_architecture.png - Shows LBA model principles and dual-channel architecture")
    print("2. lba_parameter_distribution.png - Shows parameter distributions across 18 subjects")
    print("3. sigma_matrix_heatmap.png - Shows cross-channel correlations (evidence against independence)")

if __name__ == "__main__":
    main()

# ============================================================================
# VARIABLE EXPLANATIONS
# ============================================================================

"""
KEY VARIABLES AND THEIR MEANINGS:

LBA Model Parameters:
- A: Start point variability (0.35)
  * Controls random variation in initial evidence accumulation
  * Higher A = more variability in starting points

- b: Decision threshold (0.75)
  * Boundary that must be reached to make a decision
  * Higher b = more evidence needed, slower but more accurate decisions

- t0: Non-decision time (0.3s)
  * Time for stimulus encoding and motor response
  * Does not include evidence accumulation time

- drift_rates: Evidence accumulation rates for each choice
  * Higher drift = faster accumulation toward that choice
  * Based on experimental choice frequencies

Channel Parameters:
- left_bias/right_bias: Starting point advantages for each channel
  * Positive bias = head start for that channel
  * Key finding: r = -0.633 (strong negative correlation)

- left_drift/right_drift: Accumulation rates for each channel
  * Speed of evidence accumulation
  * Reflects perceptual sensitivity

- noise_left/noise_right: Random variability in each channel
  * Models neural noise and uncertainty
  * Higher noise = more variable responses

Correlation Matrix Variables:
- correlation_matrix: 5x5 matrix showing parameter relationships
  * Values range from -1 (perfect negative) to +1 (perfect positive)
  * Key violation: channels are not independent as assumed

- significance_markers: Statistical significance indicators
  * * p < 0.05, ** p < 0.01, *** p < 0.001
  * Shows which correlations are statistically reliable

Visualization Variables:
- colors: Color palette for different choices/channels
  * Ensures consistent color coding across all plots
  * Improves interpretability and visual appeal

- time_points: Time vector for trajectory simulation
  * np.linspace creates evenly spaced time points
  * Used for smooth accumulation curves

- subjects_data: Dictionary of simulated parameter values
  * Based on actual experimental results
  * Shows individual differences between participants
"""
