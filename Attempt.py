"""
å„ªåŒ–ç‰ˆå”æ–¹å·®çŸ©é™£LBAåˆ†æ
è§£æ±ºæ€§èƒ½å•é¡Œå’ŒPyTensorå…¼å®¹æ€§å•é¡Œ
"""

import numpy as np
import pandas as pd
import pymc as pm
import pytensor.tensor as pt
import arviz as az
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# è¨­ç½®æ›´é«˜æ•ˆçš„æ¡æ¨£åƒæ•¸
import pytensor
pytensor.config.floatX = 'float32'  # ä½¿ç”¨float32æé«˜é€Ÿåº¦

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šè³‡æ–™è¼‰å…¥å‡½æ•¸ï¼ˆå„ªåŒ–ç‰ˆï¼‰
# ============================================================================

def load_real_subject_data(csv_file_path='GRT_LBA.csv', max_trials_per_subject=300):
    """
    è¼‰å…¥çœŸå¯¦å—è©¦è€…è³‡æ–™ï¼ˆé™åˆ¶è©¦é©—æ•¸é‡ä»¥æé«˜é€Ÿåº¦ï¼‰
    """
    
    print("è¼‰å…¥çœŸå¯¦å—è©¦è€…è³‡æ–™...")
    
    try:
        raw_data = pd.read_csv(csv_file_path)
        print(f"æˆåŠŸè®€å– {csv_file_path}")
        print(f"åŸå§‹è³‡æ–™ç¶­åº¦ï¼š{raw_data.shape}")
        
    except FileNotFoundError:
        print(f"æ‰¾ä¸åˆ°æª”æ¡ˆ {csv_file_path}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    except Exception as e:
        print(f"è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    
    # æª¢æŸ¥å¿…è¦çš„æ¬„ä½
    required_columns = ['Chanel1', 'Chanel2', 'Response', 'RT', 'acc']
    missing_columns = [col for col in required_columns if col not in raw_data.columns]
    
    if missing_columns:
        print(f"ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{missing_columns}")
        print(f"å¯ç”¨æ¬„ä½ï¼š{list(raw_data.columns)}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    
    # è³‡æ–™æ¸…ç†å’Œè½‰æ›
    print("è½‰æ›è³‡æ–™æ ¼å¼...")
    
    # ç§»é™¤ç¼ºå¤±å€¼
    clean_data = raw_data.dropna(subset=required_columns)
    print(f"æ¸…ç†å¾Œè³‡æ–™ç¶­åº¦ï¼š{clean_data.shape}")
    
    # ç§»é™¤æ¥µç«¯åæ‡‰æ™‚é–“
    clean_data = clean_data[(clean_data['RT'] > 0.1) & (clean_data['RT'] < 3.0)]
    print(f"ç§»é™¤æ¥µç«¯RTå¾Œç¶­åº¦ï¼š{clean_data.shape}")
    
    # é™åˆ¶æ¯å€‹å—è©¦è€…çš„è©¦é©—æ•¸é‡
    if 'Subject' in clean_data.columns:
        clean_data = clean_data.groupby('Subject').head(max_trials_per_subject)
    elif 'participant' in clean_data.columns:
        clean_data = clean_data.groupby('participant').head(max_trials_per_subject)
    
    # è½‰æ›ç‚ºæ¨¡å‹éœ€è¦çš„æ ¼å¼
    converted_data = []
    
    for idx, row in clean_data.iterrows():
        # ç¢ºä¿Responseåœ¨0-3ç¯„åœå…§
        if not (0 <= row['Response'] <= 3):
            continue
            
        # ç¢ºä¿Chanelå€¼ç‚º0æˆ–1
        if row['Chanel1'] not in [0, 1] or row['Chanel2'] not in [0, 1]:
            continue
        
        converted_row = {
            'subject_id': row.get('Subject', row.get('participant', 1)),
            'trial': len(converted_data) + 1,
            'left_pattern': int(row['Chanel1']),
            'right_pattern': int(row['Chanel2']),
            'response': int(row['Response']),
            'rt': float(row['RT']),
            'accuracy': int(row['acc']),
            'stimulus_type': int(row['Chanel1']) * 2 + int(row['Chanel2']),
            'is_symmetric': 1 if row['Chanel1'] == row['Chanel2'] else 0
        }
        
        converted_data.append(converted_row)
    
    # è½‰æ›ç‚ºDataFrame
    df = pd.DataFrame(converted_data)
    
    if len(df) == 0:
        print("âŒ æ²’æœ‰æœ‰æ•ˆçš„è³‡æ–™è¡Œï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™")
        return generate_test_data()
    
    # è³‡æ–™çµ±è¨ˆ
    print(f"\nâœ… çœŸå¯¦è³‡æ–™è¼‰å…¥å®Œæˆï¼š")
    print(f"  æœ‰æ•ˆè©¦é©—æ•¸ï¼š{len(df)}")
    print(f"  å—è©¦è€…æ•¸ï¼š{df['subject_id'].nunique()}")
    print(f"  æ•´é«”æº–ç¢ºç‡ï¼š{df['accuracy'].mean():.3f}")
    print(f"  å¹³å‡RTï¼š{df['rt'].mean():.3f}ç§’")
    print(f"  å°ç¨±è©¦é©—æ¯”ä¾‹ï¼š{df['is_symmetric'].mean():.3f}")
    
    return df

def generate_test_data(n_trials=200):
    """
    ç”Ÿæˆæ¸¬è©¦ç”¨çš„æ¨¡æ“¬è³‡æ–™
    """
    print("ç”Ÿæˆæ¨¡æ“¬æ¸¬è©¦è³‡æ–™...")
    
    np.random.seed(42)
    data = []
    
    for trial in range(n_trials):
        # éš¨æ©Ÿåˆºæ¿€
        left_pattern = np.random.choice([0, 1])
        right_pattern = np.random.choice([0, 1])
        is_symmetric = int(left_pattern == right_pattern)
        stimulus_type = left_pattern * 2 + right_pattern
        
        # æ¨¡æ“¬åæ‡‰
        # å°ç¨±åˆºæ¿€æœ‰è¼ƒé«˜æº–ç¢ºç‡
        base_accuracy = 0.8 if is_symmetric else 0.7
        accuracy = int(np.random.random() < base_accuracy)
        
        # æ­£ç¢ºåæ‡‰
        if accuracy:
            response = stimulus_type
        else:
            response = np.random.choice([i for i in range(4) if i != stimulus_type])
        
        # åæ‡‰æ™‚é–“
        base_rt = 0.8 if is_symmetric else 0.9
        rt = np.random.gamma(2, base_rt/2)
        rt = np.clip(rt, 0.2, 3.0)
        
        data.append({
            'subject_id': 1,
            'trial': trial + 1,
            'left_pattern': left_pattern,
            'right_pattern': right_pattern,
            'response': response,
            'rt': rt,
            'accuracy': accuracy,
            'stimulus_type': stimulus_type,
            'is_symmetric': is_symmetric
        })
    
    df = pd.DataFrame(data)
    print(f"âœ… æ¨¡æ“¬è³‡æ–™ç”Ÿæˆå®Œæˆï¼š{len(df)} è©¦é©—")
    return df

# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šé«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹
# ============================================================================

def build_fast_covariance_lba_model(data):
    """
    å»ºæ§‹é«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹
    """
    
    print("å»ºæ§‹é«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹...")
    
    # æº–å‚™è³‡æ–™ï¼ˆè½‰ç‚ºnumpyé™£åˆ—æé«˜æ•ˆç‡ï¼‰
    rt_obs = data['rt'].values.astype('float32')
    response_obs = data['response'].values.astype('int32')
    is_symmetric = data['is_symmetric'].values.astype('float32')
    stimulus_type = data['stimulus_type'].values.astype('int32')
    
    n_trials = len(data)
    print(f"è™•ç† {n_trials} å€‹è©¦é©—")
    
    with pm.Model() as model:
        
        # å”æ–¹å·®åƒæ•¸ï¼ˆç°¡åŒ–ï¼‰
        rho_base = pm.Uniform('rho_base', lower=-0.5, upper=0.5)
        rho_symmetry_effect = pm.Normal('rho_symmetry_effect', mu=0, sigma=0.2)
        
        # LBAåƒæ•¸ï¼ˆç°¡åŒ–ï¼‰
        A = pm.HalfNormal('A', sigma=0.2)
        b_base = pm.HalfNormal('b_base', sigma=0.3)
        s = pm.HalfNormal('s', sigma=0.2)
        t0 = pm.HalfNormal('t0', sigma=0.1)
        
        # åŸºç¤drift rate
        drift_base = pm.HalfNormal('drift_base', sigma=0.5)
        drift_correct_boost = pm.HalfNormal('drift_correct_boost', sigma=0.3)
        
        # å°ç¨±æ€§æ•ˆæ‡‰
        symmetry_boost = pm.Normal('symmetry_boost', mu=0, sigma=0.1)
        
        # å‘é‡åŒ–è¨ˆç®—drift rates
        def compute_drift_rates_vectorized():
            # ç‚ºæ¯å€‹ç´¯åŠ å™¨è¨ˆç®—drift rate
            drift_rates = pt.zeros((n_trials, 4))
            
            for acc in range(4):
                # åŸºç¤drift
                base_drift = drift_base
                
                # æ­£ç¢ºç´¯åŠ å™¨å¢å¼·
                correct_boost = pt.where(pt.eq(stimulus_type, acc), drift_correct_boost, 0.0)
                
                # å°ç¨±æ€§æ•ˆæ‡‰
                sym_boost = symmetry_boost * is_symmetric * pt.where(pt.eq(stimulus_type, acc), 1.0, 0.0)
                
                # ç›¸é—œæ€§æ•ˆæ‡‰ï¼ˆç°¡åŒ–ï¼‰
                rho_trial = rho_base + rho_symmetry_effect * is_symmetric
                correlation_effect = pt.abs(rho_trial) * 0.1
                
                # ç¸½drift rate
                final_drift = base_drift + correct_boost + sym_boost + correlation_effect
                final_drift = pt.maximum(final_drift, 0.05)
                
                drift_rates = pt.set_subtensor(drift_rates[:, acc], final_drift)
            
            return drift_rates
        
        drift_rates = compute_drift_rates_vectorized()
        
        # ç°¡åŒ–çš„ä¼¼ç„¶å‡½æ•¸ï¼ˆå‘é‡åŒ–ï¼‰
        def vectorized_lba_logp():
            # èª¿æ•´æ™‚é–“
            t_adj = pt.maximum(rt_obs - t0, 0.01)
            
            # é¸æ“‡çš„ç´¯åŠ å™¨drift rates
            chosen_drifts = drift_rates[pt.arange(n_trials), response_obs]
            
            # é–¾å€¼
            thresholds = b_base
            
            # æ™‚é–“ä¼¼ç„¶ï¼ˆç°¡åŒ–ç‚ºæŒ‡æ•¸åˆ†ä½ˆï¼‰
            lambda_param = chosen_drifts / thresholds
            time_logp = pt.log(lambda_param) - lambda_param * t_adj
            
            # é¸æ“‡ä¼¼ç„¶ï¼ˆsoftmaxï¼‰
            choice_logits = drift_rates * 3.0
            choice_logp = choice_logits[pt.arange(n_trials), response_obs] - pt.logsumexp(choice_logits, axis=1)
            
            return pt.sum(time_logp + choice_logp)
        
        # è§€æ¸¬ä¼¼ç„¶
        pm.Potential('likelihood', vectorized_lba_logp())
        
        # å„²å­˜é‡è¦è®Šæ•¸
        pm.Deterministic('rho_symmetric', rho_base + rho_symmetry_effect)
        pm.Deterministic('rho_asymmetric', rho_base)
        pm.Deterministic('rho_difference', rho_symmetry_effect)
    
    return model

# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¿«é€Ÿçµæœåˆ†æ
# ============================================================================

def analyze_fast_results(trace, data):
    """
    å¿«é€Ÿåˆ†ææ¨¡å‹çµæœ
    """
    
    print("\n" + "="*50)
    print("ğŸ“Š å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹çµæœï¼ˆå¿«é€Ÿç‰ˆï¼‰")
    print("="*50)
    
    # æå–å¾Œé©—æ¨£æœ¬
    posterior = trace.posterior
    
    # å”æ–¹å·®åˆ†æ
    print(f"\nğŸ“ˆ å”æ–¹å·®çŸ©é™£åˆ†æ:")
    
    rho_base = posterior['rho_base'].values.flatten()
    rho_symmetry_effect = posterior['rho_symmetry_effect'].values.flatten()
    rho_symmetric = posterior['rho_symmetric'].values.flatten()
    rho_asymmetric = posterior['rho_asymmetric'].values.flatten()
    
    print(f"åŸºç¤ç›¸é—œä¿‚æ•¸: {np.mean(rho_base):.3f} [{np.percentile(rho_base, 2.5):.3f}, {np.percentile(rho_base, 97.5):.3f}]")
    print(f"å°ç¨±æ€§æ•ˆæ‡‰: {np.mean(rho_symmetry_effect):.3f} [{np.percentile(rho_symmetry_effect, 2.5):.3f}, {np.percentile(rho_symmetry_effect, 97.5):.3f}]")
    print(f"å°ç¨±åˆºæ¿€ç›¸é—œä¿‚æ•¸: {np.mean(rho_symmetric):.3f}")
    print(f"éå°ç¨±åˆºæ¿€ç›¸é—œä¿‚æ•¸: {np.mean(rho_asymmetric):.3f}")
    
    # é¡¯è‘—æ€§æ¸¬è©¦
    rho_diff_significant = not (np.percentile(rho_symmetry_effect, 2.5) < 0 < np.percentile(rho_symmetry_effect, 97.5))
    print(f"å°ç¨±æ€§æ•ˆæ‡‰é¡¯è‘—æ€§: {'æ˜¯' if rho_diff_significant else 'å¦'}")
    
    # ç¨ç«‹æ€§å‡è¨­æª¢é©—
    independence_prob = np.mean(np.abs(rho_base) < 0.1)
    print(f"\nğŸ”¬ GRTç¨ç«‹æ€§å‡è¨­:")
    print(f"åŸºç¤ç¨ç«‹æ€§æ©Ÿç‡: {independence_prob:.3f}")
    
    if independence_prob < 0.05:
        print("ğŸš¨ å¼·çƒˆé•åGRTç¨ç«‹æ€§å‡è¨­")
    elif independence_prob < 0.2:
        print("âš ï¸  ä¸­ç­‰ç¨‹åº¦é•åç¨ç«‹æ€§å‡è¨­")
    else:
        print("âœ… åŸºæœ¬æ”¯æŒç¨ç«‹æ€§å‡è¨­")
    
    # ç†è«–è§£é‡‹
    print(f"\nğŸ’¡ ç†è«–è§£é‡‹:")
    model_rho_diff = np.mean(rho_symmetry_effect)
    
    if abs(model_rho_diff) > 0.05:
        if model_rho_diff > 0:
            print("âœ“ å°ç¨±åˆºæ¿€å¢åŠ sourceé–“ç›¸é—œæ€§")
            print("  â†’ æ”¯æŒé…ç½®æ€§è™•ç†å‡è¨­")
        else:
            print("âœ“ å°ç¨±åˆºæ¿€æ¸›å°‘sourceé–“ç›¸é—œæ€§")
            print("  â†’ æ”¯æŒç¨ç«‹è™•ç†å‡è¨­")
    else:
        print("â€¢ å°ç¨±æ€§å°ç›¸é—œæ€§å½±éŸ¿è¼ƒå°")
    
    return {
        'rho_base': rho_base,
        'rho_symmetry_effect': rho_symmetry_effect,
        'rho_symmetric': rho_symmetric,
        'rho_asymmetric': rho_asymmetric,
        'significant_symmetry_effect': rho_diff_significant,
        'independence_violation': independence_prob < 0.2
    }

# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šå¿«é€Ÿè¦–è¦ºåŒ–
# ============================================================================

def create_fast_visualization(trace, results, data):
    """
    å‰µå»ºå¿«é€Ÿçµæœè¦–è¦ºåŒ–
    """
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # 1. ç›¸é—œä¿‚æ•¸æ¯”è¼ƒ
    axes[0, 0].hist(results['rho_symmetric'], bins=20, alpha=0.6, label='å°ç¨±', color='green', density=True)
    axes[0, 0].hist(results['rho_asymmetric'], bins=20, alpha=0.6, label='éå°ç¨±', color='orange', density=True)
    axes[0, 0].axvline(0, color='red', linestyle='--', alpha=0.7, label='ç¨ç«‹æ€§')
    axes[0, 0].set_xlabel('ç›¸é—œä¿‚æ•¸ Ï')
    axes[0, 0].set_ylabel('å¯†åº¦')
    axes[0, 0].set_title('ç›¸é—œä¿‚æ•¸å¾Œé©—åˆ†ä½ˆ')
    axes[0, 0].legend()
    
    # 2. å°ç¨±æ€§æ•ˆæ‡‰
    axes[0, 1].hist(results['rho_symmetry_effect'], bins=20, alpha=0.7, color='purple', density=True)
    axes[0, 1].axvline(0, color='red', linestyle='--', alpha=0.7)
    axes[0, 1].axvline(np.mean(results['rho_symmetry_effect']), color='black', linestyle='-', label='å¹³å‡')
    axes[0, 1].set_xlabel('ç›¸é—œä¿‚æ•¸å·®ç•°')
    axes[0, 1].set_ylabel('å¯†åº¦')
    axes[0, 1].set_title('å°ç¨±æ€§æ•ˆæ‡‰')
    axes[0, 1].legend()
    
    # 3. è¡Œç‚ºè³‡æ–™å°æ¯”
    symmetric_data = data[data['is_symmetric'] == 1]
    asymmetric_data = data[data['is_symmetric'] == 0]
    
    if len(symmetric_data) > 0 and len(asymmetric_data) > 0:
        categories = ['æº–ç¢ºç‡', 'åæ‡‰æ™‚é–“']
        sym_values = [symmetric_data['accuracy'].mean(), symmetric_data['rt'].mean()]
        asym_values = [asymmetric_data['accuracy'].mean(), asymmetric_data['rt'].mean()]
        
        x = np.arange(len(categories))
        width = 0.35
        
        axes[1, 0].bar(x - width/2, sym_values, width, label='å°ç¨±', color='green', alpha=0.7)
        axes[1, 0].bar(x + width/2, asym_values, width, label='éå°ç¨±', color='orange', alpha=0.7)
        axes[1, 0].set_ylabel('å€¼')
        axes[1, 0].set_title('è¡Œç‚ºè³‡æ–™å°æ¯”')
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels(categories)
        axes[1, 0].legend()
    
    # 4. ç¸½çµ
    axes[1, 1].text(0.1, 0.9, 'æ¨¡å‹ç¸½çµ', fontsize=14, fontweight='bold', transform=axes[1, 1].transAxes)
    
    summary_text = f"""
ç¨ç«‹æ€§é•å: {'æ˜¯' if results['independence_violation'] else 'å¦'}
å°ç¨±æ€§æ•ˆæ‡‰: {'é¡¯è‘—' if results['significant_symmetry_effect'] else 'ä¸é¡¯è‘—'}

åŸºç¤ç›¸é—œ: {np.mean(results['rho_base']):.3f}
å°ç¨±å¢å¼·: {np.mean(results['rho_symmetry_effect']):.3f}

è§£é‡‹: {'é…ç½®æ€§è™•ç†' if np.mean(results['rho_symmetry_effect']) > 0.05 else 'ç¨ç«‹è™•ç†' if np.mean(results['rho_symmetry_effect']) < -0.05 else 'å½±éŸ¿è¼ƒå°'}
"""
    
    axes[1, 1].text(0.1, 0.7, summary_text, fontsize=10, transform=axes[1, 1].transAxes,
                    verticalalignment='top', fontfamily='monospace')
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    plt.savefig('fast_covariance_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("ğŸ“Š åˆ†æçµæœå·²å„²å­˜ç‚º 'fast_covariance_analysis.png'")

# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šå¿«é€ŸåŸ·è¡Œå‡½æ•¸
# ============================================================================

def run_fast_covariance_analysis(csv_file_path='GRT_LBA.csv', subject_id=None, max_trials=200):
    """
    åŸ·è¡Œå¿«é€Ÿå”æ–¹å·®çŸ©é™£åˆ†æ
    """
    
    print("ğŸš€ é–‹å§‹å¿«é€Ÿå”æ–¹å·®çŸ©é™£LBAåˆ†æ")
    print("="*60)
    
    # 1. è¼‰å…¥è³‡æ–™
    data = load_real_subject_data(csv_file_path, max_trials_per_subject=max_trials)
    if data is None:
        return None, None, None
    
    # 2. é¸æ“‡å—è©¦è€…
    if subject_id is None:
        if 'subject_id' in data.columns:
            subject_counts = data['subject_id'].value_counts()
            selected_subject = subject_counts.index[0]
            print(f"\nğŸ¯ è‡ªå‹•é¸æ“‡å—è©¦è€… {selected_subject} (è©¦é©—æ•¸: {subject_counts.iloc[0]})")
        else:
            selected_subject = 1
            print(f"ğŸ¯ ä½¿ç”¨é è¨­å—è©¦è€… {selected_subject}")
    else:
        selected_subject = subject_id
        print(f"ğŸ¯ é¸æ“‡å—è©¦è€… {selected_subject}")
    
    # æå–å—è©¦è€…è³‡æ–™
    subject_data = data[data['subject_id'] == selected_subject].copy()
    
    # 3. é™åˆ¶è©¦é©—æ•¸
    if len(subject_data) > max_trials:
        print(f"âš ï¸  é™åˆ¶è©¦é©—æ•¸ç‚º {max_trials}")
        subject_data = subject_data.sample(n=max_trials, random_state=42).reset_index(drop=True)
    
    print(f"\nğŸ“Š åˆ†æè³‡æ–™æ‘˜è¦ï¼š")
    print(f"  å—è©¦è€…: {selected_subject}")
    print(f"  è©¦é©—æ•¸: {len(subject_data)}")
    print(f"  æº–ç¢ºç‡: {subject_data['accuracy'].mean():.3f}")
    print(f"  å¹³å‡RT: {subject_data['rt'].mean():.3f}s")
    
    # 4. å»ºæ§‹æ¨¡å‹
    print(f"\nğŸ”§ å»ºæ§‹å¿«é€Ÿæ¨¡å‹...")
    model = build_fast_covariance_lba_model(subject_data)
    
    # 5. åŸ·è¡Œå¿«é€Ÿæ¡æ¨£
    print("â³ é–‹å§‹å¿«é€ŸMCMCæ¡æ¨£...")
    print("æ¡æ¨£åƒæ•¸ï¼š200 draws + 100 tune, 2 chains")
    
    try:
        with model:
            trace = pm.sample(
                draws=200,
                tune=100,
                chains=2,
                cores=1,  # å–®æ ¸å¿ƒé¿å…ä¸¦è¡Œå•é¡Œ
                target_accept=0.8,
                return_inferencedata=True,
                random_seed=456,
                progressbar=True,
                compute_convergence_checks=False  # è·³éæ”¶æ–‚æª¢æŸ¥åŠ å¿«é€Ÿåº¦
            )
        
        print("âœ… å¿«é€ŸMCMCæ¡æ¨£å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ MCMCæ¡æ¨£å¤±æ•—: {e}")
        print("å˜—è©¦æ›´ç°¡å–®çš„æ¡æ¨£è¨­å®š...")
        
        try:
            with model:
                trace = pm.sample(
                    draws=100,
                    tune=50,
                    chains=1,
                    cores=1,
                    return_inferencedata=True,
                    random_seed=456,
                    progressbar=True,
                    compute_convergence_checks=False
                )
            print("âœ… ç°¡åŒ–æ¡æ¨£å®Œæˆï¼")
        except Exception as e2:
            print(f"âŒ ç°¡åŒ–æ¡æ¨£ä¹Ÿå¤±æ•—: {e2}")
            return None, None, None
    
    # 6. åˆ†æçµæœ
    print(f"\nğŸ“ˆ åˆ†æçµæœ...")
    results = analyze_fast_results(trace, subject_data)
    
    # 7. å‰µå»ºè¦–è¦ºåŒ–
    print(f"\nğŸ¨ ç”Ÿæˆè¦–è¦ºåŒ–...")
    create_fast_visualization(trace, results, subject_data)
    
    print(f"\nğŸ‰ å¿«é€Ÿå”æ–¹å·®çŸ©é™£LBAåˆ†æå®Œæˆï¼")
    
    return trace, subject_data, model

# ============================================================================
# å¿«é€Ÿæ¸¬è©¦å‡½æ•¸
# ============================================================================

def quick_test():
    """
    å¿«é€Ÿæ¸¬è©¦å‡½æ•¸
    """
    print("ğŸ§ª åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦...")
    return run_fast_covariance_analysis(max_trials=100)

# ============================================================================
# åŸ·è¡Œåˆ†æ
# ============================================================================

if __name__ == "__main__":
    print("ğŸ”¬ å„ªåŒ–ç‰ˆå”æ–¹å·®çŸ©é™£LBAæ¨¡å‹")
    print("è§£æ±ºæ€§èƒ½å•é¡Œå’Œå…¼å®¹æ€§å•é¡Œ")
    print("-" * 60)
    
    # åŸ·è¡Œå¿«é€Ÿåˆ†æ
    trace, data, model = run_fast_covariance_analysis()
    
    if trace is not None:
        print("\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆï¼")
        print("å¦‚æœéœ€è¦æ›´è©³ç´°çš„åˆ†æï¼Œå¯ä»¥å¢åŠ  draws å’Œ tune åƒæ•¸")
    else:
        print("\nâŒ åˆ†æå¤±æ•—")
        print("å˜—è©¦åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦: quick_test()")

# ============================================================================
# ä½¿ç”¨èªªæ˜
# ============================================================================

"""
å¿«é€Ÿä½¿ç”¨æ–¹æ³•ï¼š

1. åŸºæœ¬åŸ·è¡Œï¼š
   trace, data, model = run_fast_covariance_analysis()

2. æŒ‡å®šåƒæ•¸ï¼š
   trace, data, model = run_fast_covariance_analysis(
       csv_file_path='your_file.csv',
       subject_id=1,
       max_trials=150
   )

3. å¿«é€Ÿæ¸¬è©¦ï¼ˆä½¿ç”¨æ¨¡æ“¬è³‡æ–™ï¼‰ï¼š
   trace, data, model = quick_test()

4. å¦‚æœä»ç„¶å¾ˆæ…¢ï¼Œå¯ä»¥é€²ä¸€æ­¥æ¸›å°‘åƒæ•¸ï¼š
   # ä¿®æ”¹ run_fast_covariance_analysis ä¸­çš„æ¡æ¨£åƒæ•¸
   # draws=100, tune=50, chains=1
""""""
å„ªåŒ–ç‰ˆå”æ–¹å·®çŸ©é™£LBAåˆ†æ
è§£æ±ºæ€§èƒ½å•é¡Œå’ŒPyTensorå…¼å®¹æ€§å•é¡Œ
"""

import numpy as np
import pandas as pd
import pymc as pm
import pytensor.tensor as pt
import arviz as az
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# è¨­ç½®æ›´é«˜æ•ˆçš„æ¡æ¨£åƒæ•¸
import pytensor
pytensor.config.floatX = 'float32'  # ä½¿ç”¨float32æé«˜é€Ÿåº¦

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šè³‡æ–™è¼‰å…¥å‡½æ•¸ï¼ˆå„ªåŒ–ç‰ˆï¼‰
# ============================================================================

def load_real_subject_data(csv_file_path='GRT_LBA.csv', max_trials_per_subject=300):
    """
    è¼‰å…¥çœŸå¯¦å—è©¦è€…è³‡æ–™ï¼ˆé™åˆ¶è©¦é©—æ•¸é‡ä»¥æé«˜é€Ÿåº¦ï¼‰
    """
    
    print("è¼‰å…¥çœŸå¯¦å—è©¦è€…è³‡æ–™...")
    
    try:
        raw_data = pd.read_csv(csv_file_path)
        print(f"æˆåŠŸè®€å– {csv_file_path}")
        print(f"åŸå§‹è³‡æ–™ç¶­åº¦ï¼š{raw_data.shape}")
        
    except FileNotFoundError:
        print(f"æ‰¾ä¸åˆ°æª”æ¡ˆ {csv_file_path}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    except Exception as e:
        print(f"è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    
    # æª¢æŸ¥å¿…è¦çš„æ¬„ä½
    required_columns = ['Chanel1', 'Chanel2', 'Response', 'RT', 'acc']
    missing_columns = [col for col in required_columns if col not in raw_data.columns]
    
    if missing_columns:
        print(f"ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{missing_columns}")
        print(f"å¯ç”¨æ¬„ä½ï¼š{list(raw_data.columns)}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    
    # è³‡æ–™æ¸…ç†å’Œè½‰æ›
    print("è½‰æ›è³‡æ–™æ ¼å¼...")
    
    # ç§»é™¤ç¼ºå¤±å€¼
    clean_data = raw_data.dropna(subset=required_columns)
    print(f"æ¸…ç†å¾Œè³‡æ–™ç¶­åº¦ï¼š{clean_data.shape}")
    
    # ç§»é™¤æ¥µç«¯åæ‡‰æ™‚é–“
    clean_data = clean_data[(clean_data['RT'] > 0.1) & (clean_data['RT'] < 3.0)]
    print(f"ç§»é™¤æ¥µç«¯RTå¾Œç¶­åº¦ï¼š{clean_data.shape}")
    
    # é™åˆ¶æ¯å€‹å—è©¦è€…çš„è©¦é©—æ•¸é‡
    if 'Subject' in clean_data.columns:
        clean_data = clean_data.groupby('Subject').head(max_trials_per_subject)
    elif 'participant' in clean_data.columns:
        clean_data = clean_data.groupby('participant').head(max_trials_per_subject)
    
    # è½‰æ›ç‚ºæ¨¡å‹éœ€è¦çš„æ ¼å¼
    converted_data = []
    
    for idx, row in clean_data.iterrows():
        # ç¢ºä¿Responseåœ¨0-3ç¯„åœå…§
        if not (0 <= row['Response'] <= 3):
            continue
            
        # ç¢ºä¿Chanelå€¼ç‚º0æˆ–1
        if row['Chanel1'] not in [0, 1] or row['Chanel2'] not in [0, 1]:
            continue
        
        converted_row = {
            'subject_id': row.get('Subject', row.get('participant', 1)),
            'trial': len(converted_data) + 1,
            'left_pattern': int(row['Chanel1']),
            'right_pattern': int(row['Chanel2']),
            'response': int(row['Response']),
            'rt': float(row['RT']),
            'accuracy': int(row['acc']),
            'stimulus_type': int(row['Chanel1']) * 2 + int(row['Chanel2']),
            'is_symmetric': 1 if row['Chanel1'] == row['Chanel2'] else 0
        }
        
        converted_data.append(converted_row)
    
    # è½‰æ›ç‚ºDataFrame
    df = pd.DataFrame(converted_data)
    
    if len(df) == 0:
        print("âŒ æ²’æœ‰æœ‰æ•ˆçš„è³‡æ–™è¡Œï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™")
        return generate_test_data()
    
    # è³‡æ–™çµ±è¨ˆ
    print(f"\nâœ… çœŸå¯¦è³‡æ–™è¼‰å…¥å®Œæˆï¼š")
    print(f"  æœ‰æ•ˆè©¦é©—æ•¸ï¼š{len(df)}")
    print(f"  å—è©¦è€…æ•¸ï¼š{df['subject_id'].nunique()}")
    print(f"  æ•´é«”æº–ç¢ºç‡ï¼š{df['accuracy'].mean():.3f}")
    print(f"  å¹³å‡RTï¼š{df['rt'].mean():.3f}ç§’")
    print(f"  å°ç¨±è©¦é©—æ¯”ä¾‹ï¼š{df['is_symmetric'].mean():.3f}")
    
    return df

def generate_test_data(n_trials=200):
    """
    ç”Ÿæˆæ¸¬è©¦ç”¨çš„æ¨¡æ“¬è³‡æ–™
    """
    print("ç”Ÿæˆæ¨¡æ“¬æ¸¬è©¦è³‡æ–™...")
    
    np.random.seed(42)
    data = []
    
    for trial in range(n_trials):
        # éš¨æ©Ÿåˆºæ¿€
        left_pattern = np.random.choice([0, 1])
        right_pattern = np.random.choice([0, 1])
        is_symmetric = int(left_pattern == right_pattern)
        stimulus_type = left_pattern * 2 + right_pattern
        
        # æ¨¡æ“¬åæ‡‰
        # å°ç¨±åˆºæ¿€æœ‰è¼ƒé«˜æº–ç¢ºç‡
        base_accuracy = 0.8 if is_symmetric else 0.7
        accuracy = int(np.random.random() < base_accuracy)
        
        # æ­£ç¢ºåæ‡‰
        if accuracy:
            response = stimulus_type
        else:
            response = np.random.choice([i for i in range(4) if i != stimulus_type])
        
        # åæ‡‰æ™‚é–“
        base_rt = 0.8 if is_symmetric else 0.9
        rt = np.random.gamma(2, base_rt/2)
        rt = np.clip(rt, 0.2, 3.0)
        
        data.append({
            'subject_id': 1,
            'trial': trial + 1,
            'left_pattern': left_pattern,
            'right_pattern': right_pattern,
            'response': response,
            'rt': rt,
            'accuracy': accuracy,
            'stimulus_type': stimulus_type,
            'is_symmetric': is_symmetric
        })
    
    df = pd.DataFrame(data)
    print(f"âœ… æ¨¡æ“¬è³‡æ–™ç”Ÿæˆå®Œæˆï¼š{len(df)} è©¦é©—")
    return df

# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šé«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹
# ============================================================================

def build_fast_covariance_lba_model(data):
    """
    å»ºæ§‹é«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹
    """
    
    print("å»ºæ§‹é«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹...")
    
    # æº–å‚™è³‡æ–™ï¼ˆè½‰ç‚ºnumpyé™£åˆ—æé«˜æ•ˆç‡ï¼‰
    rt_obs = data['rt'].values.astype('float32')
    response_obs = data['response'].values.astype('int32')
    is_symmetric = data['is_symmetric'].values.astype('float32')
    stimulus_type = data['stimulus_type'].values.astype('int32')
    
    n_trials = len(data)
    print(f"è™•ç† {n_trials} å€‹è©¦é©—")
    
    with pm.Model() as model:
        
        # å”æ–¹å·®åƒæ•¸ï¼ˆç°¡åŒ–ï¼‰
        rho_base = pm.Uniform('rho_base', lower=-0.5, upper=0.5)
        rho_symmetry_effect = pm.Normal('rho_symmetry_effect', mu=0, sigma=0.2)
        
        # LBAåƒæ•¸ï¼ˆç°¡åŒ–ï¼‰
        A = pm.HalfNormal('A', sigma=0.2)
        b_base = pm.HalfNormal('b_base', sigma=0.3)
        s = pm.HalfNormal('s', sigma=0.2)
        t0 = pm.HalfNormal('t0', sigma=0.1)
        
        # åŸºç¤drift rate
        drift_base = pm.HalfNormal('drift_base', sigma=0.5)
        drift_correct_boost = pm.HalfNormal('drift_correct_boost', sigma=0.3)
        
        # å°ç¨±æ€§æ•ˆæ‡‰
        symmetry_boost = pm.Normal('symmetry_boost', mu=0, sigma=0.1)
        
        # å‘é‡åŒ–è¨ˆç®—drift rates
        def compute_drift_rates_vectorized():
            # ç‚ºæ¯å€‹ç´¯åŠ å™¨è¨ˆç®—drift rate
            drift_rates = pt.zeros((n_trials, 4))
            
            for acc in range(4):
                # åŸºç¤drift
                base_drift = drift_base
                
                # æ­£ç¢ºç´¯åŠ å™¨å¢å¼·
                correct_boost = pt.where(pt.eq(stimulus_type, acc), drift_correct_boost, 0.0)
                
                # å°ç¨±æ€§æ•ˆæ‡‰
                sym_boost = symmetry_boost * is_symmetric * pt.where(pt.eq(stimulus_type, acc), 1.0, 0.0)
                
                # ç›¸é—œæ€§æ•ˆæ‡‰ï¼ˆç°¡åŒ–ï¼‰
                rho_trial = rho_base + rho_symmetry_effect * is_symmetric
                correlation_effect = pt.abs(rho_trial) * 0.1
                
                # ç¸½drift rate
                final_drift = base_drift + correct_boost + sym_boost + correlation_effect
                final_drift = pt.maximum(final_drift, 0.05)
                
                drift_rates = pt.set_subtensor(drift_rates[:, acc], final_drift)
            
            return drift_rates
        
        drift_rates = compute_drift_rates_vectorized()
        
        # ç°¡åŒ–çš„ä¼¼ç„¶å‡½æ•¸ï¼ˆå‘é‡åŒ–ï¼‰
        def vectorized_lba_logp():
            # èª¿æ•´æ™‚é–“
            t_adj = pt.maximum(rt_obs - t0, 0.01)
            
            # é¸æ“‡çš„ç´¯åŠ å™¨drift rates
            chosen_drifts = drift_rates[pt.arange(n_trials), response_obs]
            
            # é–¾å€¼
            thresholds = b_base
            
            # æ™‚é–“ä¼¼ç„¶ï¼ˆç°¡åŒ–ç‚ºæŒ‡æ•¸åˆ†ä½ˆï¼‰
            lambda_param = chosen_drifts / thresholds
            time_logp = pt.log(lambda_param) - lambda_param * t_adj
            
            # é¸æ“‡ä¼¼ç„¶ï¼ˆsoftmaxï¼‰
            choice_logits = drift_rates * 3.0
            choice_logp = choice_logits[pt.arange(n_trials), response_obs] - pt.logsumexp(choice_logits, axis=1)
            
            return pt.sum(time_logp + choice_logp)
        
        # è§€æ¸¬ä¼¼ç„¶
        pm.Potential('likelihood', vectorized_lba_logp())
        
        # å„²å­˜é‡è¦è®Šæ•¸
        pm.Deterministic('rho_symmetric', rho_base + rho_symmetry_effect)
        pm.Deterministic('rho_asymmetric', rho_base)
        pm.Deterministic('rho_difference', rho_symmetry_effect)
    
    return model

# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¿«é€Ÿçµæœåˆ†æ
# ============================================================================

def analyze_fast_results(trace, data):
    """
    å¿«é€Ÿåˆ†ææ¨¡å‹çµæœ
    """
    
    print("\n" + "="*50)
    print("ğŸ“Š å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹çµæœï¼ˆå¿«é€Ÿç‰ˆï¼‰")
    print("="*50)
    
    # æå–å¾Œé©—æ¨£æœ¬
    posterior = trace.posterior
    
    # å”æ–¹å·®åˆ†æ
    print(f"\nğŸ“ˆ å”æ–¹å·®çŸ©é™£åˆ†æ:")
    
    rho_base = posterior['rho_base'].values.flatten()
    rho_symmetry_effect = posterior['rho_symmetry_effect'].values.flatten()
    rho_symmetric = posterior['rho_symmetric'].values.flatten()
    rho_asymmetric = posterior['rho_asymmetric'].values.flatten()
    
    print(f"åŸºç¤ç›¸é—œä¿‚æ•¸: {np.mean(rho_base):.3f} [{np.percentile(rho_base, 2.5):.3f}, {np.percentile(rho_base, 97.5):.3f}]")
    print(f"å°ç¨±æ€§æ•ˆæ‡‰: {np.mean(rho_symmetry_effect):.3f} [{np.percentile(rho_symmetry_effect, 2.5):.3f}, {np.percentile(rho_symmetry_effect, 97.5):.3f}]")
    print(f"å°ç¨±åˆºæ¿€ç›¸é—œä¿‚æ•¸: {np.mean(rho_symmetric):.3f}")
    print(f"éå°ç¨±åˆºæ¿€ç›¸é—œä¿‚æ•¸: {np.mean(rho_asymmetric):.3f}")
    
    # é¡¯è‘—æ€§æ¸¬è©¦
    rho_diff_significant = not (np.percentile(rho_symmetry_effect, 2.5) < 0 < np.percentile(rho_symmetry_effect, 97.5))
    print(f"å°ç¨±æ€§æ•ˆæ‡‰é¡¯è‘—æ€§: {'æ˜¯' if rho_diff_significant else 'å¦'}")
    
    # ç¨ç«‹æ€§å‡è¨­æª¢é©—
    independence_prob = np.mean(np.abs(rho_base) < 0.1)
    print(f"\nğŸ”¬ GRTç¨ç«‹æ€§å‡è¨­:")
    print(f"åŸºç¤ç¨ç«‹æ€§æ©Ÿç‡: {independence_prob:.3f}")
    
    if independence_prob < 0.05:
        print("ğŸš¨ å¼·çƒˆé•åGRTç¨ç«‹æ€§å‡è¨­")
    elif independence_prob < 0.2:
        print("âš ï¸  ä¸­ç­‰ç¨‹åº¦é•åç¨ç«‹æ€§å‡è¨­")
    else:
        print("âœ… åŸºæœ¬æ”¯æŒç¨ç«‹æ€§å‡è¨­")
    
    # ç†è«–è§£é‡‹
    print(f"\nğŸ’¡ ç†è«–è§£é‡‹:")
    model_rho_diff = np.mean(rho_symmetry_effect)
    
    if abs(model_rho_diff) > 0.05:
        if model_rho_diff > 0:
            print("âœ“ å°ç¨±åˆºæ¿€å¢åŠ sourceé–“ç›¸é—œæ€§")
            print("  â†’ æ”¯æŒé…ç½®æ€§è™•ç†å‡è¨­")
        else:
            print("âœ“ å°ç¨±åˆºæ¿€æ¸›å°‘sourceé–“ç›¸é—œæ€§")
            print("  â†’ æ”¯æŒç¨ç«‹è™•ç†å‡è¨­")
    else:
        print("â€¢ å°ç¨±æ€§å°ç›¸é—œæ€§å½±éŸ¿è¼ƒå°")
    
    return {
        'rho_base': rho_base,
        'rho_symmetry_effect': rho_symmetry_effect,
        'rho_symmetric': rho_symmetric,
        'rho_asymmetric': rho_asymmetric,
        'significant_symmetry_effect': rho_diff_significant,
        'independence_violation': independence_prob < 0.2
    }

# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šå¿«é€Ÿè¦–è¦ºåŒ–
# ============================================================================

def create_fast_visualization(trace, results, data):
    """
    å‰µå»ºå¿«é€Ÿçµæœè¦–è¦ºåŒ–
    """
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # 1. ç›¸é—œä¿‚æ•¸æ¯”è¼ƒ
    axes[0, 0].hist(results['rho_symmetric'], bins=20, alpha=0.6, label='å°ç¨±', color='green', density=True)
    axes[0, 0].hist(results['rho_asymmetric'], bins=20, alpha=0.6, label='éå°ç¨±', color='orange', density=True)
    axes[0, 0].axvline(0, color='red', linestyle='--', alpha=0.7, label='ç¨ç«‹æ€§')
    axes[0, 0].set_xlabel('ç›¸é—œä¿‚æ•¸ Ï')
    axes[0, 0].set_ylabel('å¯†åº¦')
    axes[0, 0].set_title('ç›¸é—œä¿‚æ•¸å¾Œé©—åˆ†ä½ˆ')
    axes[0, 0].legend()
    
    # 2. å°ç¨±æ€§æ•ˆæ‡‰
    axes[0, 1].hist(results['rho_symmetry_effect'], bins=20, alpha=0.7, color='purple', density=True)
    axes[0, 1].axvline(0, color='red', linestyle='--', alpha=0.7)
    axes[0, 1].axvline(np.mean(results['rho_symmetry_effect']), color='black', linestyle='-', label='å¹³å‡')
    axes[0, 1].set_xlabel('ç›¸é—œä¿‚æ•¸å·®ç•°')
    axes[0, 1].set_ylabel('å¯†åº¦')
    axes[0, 1].set_title('å°ç¨±æ€§æ•ˆæ‡‰')
    axes[0, 1].legend()
    
    # 3. è¡Œç‚ºè³‡æ–™å°æ¯”
    symmetric_data = data[data['is_symmetric'] == 1]
    asymmetric_data = data[data['is_symmetric'] == 0]
    
    if len(symmetric_data) > 0 and len(asymmetric_data) > 0:
        categories = ['æº–ç¢ºç‡', 'åæ‡‰æ™‚é–“']
        sym_values = [symmetric_data['accuracy'].mean(), symmetric_data['rt'].mean()]
        asym_values = [asymmetric_data['accuracy'].mean(), asymmetric_data['rt'].mean()]
        
        x = np.arange(len(categories))
        width = 0.35
        
        axes[1, 0].bar(x - width/2, sym_values, width, label='å°ç¨±', color='green', alpha=0.7)
        axes[1, 0].bar(x + width/2, asym_values, width, label='éå°ç¨±', color='orange', alpha=0.7)
        axes[1, 0].set_ylabel('å€¼')
        axes[1, 0].set_title('è¡Œç‚ºè³‡æ–™å°æ¯”')
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels(categories)
        axes[1, 0].legend()
    
    # 4. ç¸½çµ
    axes[1, 1].text(0.1, 0.9, 'æ¨¡å‹ç¸½çµ', fontsize=14, fontweight='bold', transform=axes[1, 1].transAxes)
    
    summary_text = f"""
ç¨ç«‹æ€§é•å: {'æ˜¯' if results['independence_violation'] else 'å¦'}
å°ç¨±æ€§æ•ˆæ‡‰: {'é¡¯è‘—' if results['significant_symmetry_effect'] else 'ä¸é¡¯è‘—'}

åŸºç¤ç›¸é—œ: {np.mean(results['rho_base']):.3f}
å°ç¨±å¢å¼·: {np.mean(results['rho_symmetry_effect']):.3f}

è§£é‡‹: {'é…ç½®æ€§è™•ç†' if np.mean(results['rho_symmetry_effect']) > 0.05 else 'ç¨ç«‹è™•ç†' if np.mean(results['rho_symmetry_effect']) < -0.05 else 'å½±éŸ¿è¼ƒå°'}
"""
    
    axes[1, 1].text(0.1, 0.7, summary_text, fontsize=10, transform=axes[1, 1].transAxes,
                    verticalalignment='top', fontfamily='monospace')
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    plt.savefig('fast_covariance_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("ğŸ“Š åˆ†æçµæœå·²å„²å­˜ç‚º 'fast_covariance_analysis.png'")

# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šå¿«é€ŸåŸ·è¡Œå‡½æ•¸
# ============================================================================

def run_fast_covariance_analysis(csv_file_path='GRT_LBA.csv', subject_id=None, max_trials=200):
    """
    åŸ·è¡Œå¿«é€Ÿå”æ–¹å·®çŸ©é™£åˆ†æ
    """
    
    print("ğŸš€ é–‹å§‹å¿«é€Ÿå”æ–¹å·®çŸ©é™£LBAåˆ†æ")
    print("="*60)
    
    # 1. è¼‰å…¥è³‡æ–™
    data = load_real_subject_data(csv_file_path, max_trials_per_subject=max_trials)
    if data is None:
        return None, None, None
    
    # 2. é¸æ“‡å—è©¦è€…
    if subject_id is None:
        if 'subject_id' in data.columns:
            subject_counts = data['subject_id'].value_counts()
            selected_subject = subject_counts.index[0]
            print(f"\nğŸ¯ è‡ªå‹•é¸æ“‡å—è©¦è€… {selected_subject} (è©¦é©—æ•¸: {subject_counts.iloc[0]})")
        else:
            selected_subject = 1
            print(f"ğŸ¯ ä½¿ç”¨é è¨­å—è©¦è€… {selected_subject}")
    else:
        selected_subject = subject_id
        print(f"ğŸ¯ é¸æ“‡å—è©¦è€… {selected_subject}")
    
    # æå–å—è©¦è€…è³‡æ–™
    subject_data = data[data['subject_id'] == selected_subject].copy()
    
    # 3. é™åˆ¶è©¦é©—æ•¸
    if len(subject_data) > max_trials:
        print(f"âš ï¸  é™åˆ¶è©¦é©—æ•¸ç‚º {max_trials}")
        subject_data = subject_data.sample(n=max_trials, random_state=42).reset_index(drop=True)
    
    print(f"\nğŸ“Š åˆ†æè³‡æ–™æ‘˜è¦ï¼š")
    print(f"  å—è©¦è€…: {selected_subject}")
    print(f"  è©¦é©—æ•¸: {len(subject_data)}")
    print(f"  æº–ç¢ºç‡: {subject_data['accuracy'].mean():.3f}")
    print(f"  å¹³å‡RT: {subject_data['rt'].mean():.3f}s")
    
    # 4. å»ºæ§‹æ¨¡å‹
    print(f"\nğŸ”§ å»ºæ§‹å¿«é€Ÿæ¨¡å‹...")
    model = build_fast_covariance_lba_model(subject_data)
    
    # 5. åŸ·è¡Œå¿«é€Ÿæ¡æ¨£
    print("â³ é–‹å§‹å¿«é€ŸMCMCæ¡æ¨£...")
    print("æ¡æ¨£åƒæ•¸ï¼š200 draws + 100 tune, 2 chains")
    
    try:
        with model:
            trace = pm.sample(
                draws=200,
                tune=100,
                chains=2,
                cores=1,  # å–®æ ¸å¿ƒé¿å…ä¸¦è¡Œå•é¡Œ
                target_accept=0.8,
                return_inferencedata=True,
                random_seed=456,
                progressbar=True,
                compute_convergence_checks=False  # è·³éæ”¶æ–‚æª¢æŸ¥åŠ å¿«é€Ÿåº¦
            )
        
        print("âœ… å¿«é€ŸMCMCæ¡æ¨£å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ MCMCæ¡æ¨£å¤±æ•—: {e}")
        print("å˜—è©¦æ›´ç°¡å–®çš„æ¡æ¨£è¨­å®š...")
        
        try:
            with model:
                trace = pm.sample(
                    draws=100,
                    tune=50,
                    chains=1,
                    cores=1,
                    return_inferencedata=True,
                    random_seed=456,
                    progressbar=True,
                    compute_convergence_checks=False
                )
            print("âœ… ç°¡åŒ–æ¡æ¨£å®Œæˆï¼")
        except Exception as e2:
            print(f"âŒ ç°¡åŒ–æ¡æ¨£ä¹Ÿå¤±æ•—: {e2}")
            return None, None, None
    
    # 6. åˆ†æçµæœ
    print(f"\nğŸ“ˆ åˆ†æçµæœ...")
    results = analyze_fast_results(trace, subject_data)
    
    # 7. å‰µå»ºè¦–è¦ºåŒ–
    print(f"\nğŸ¨ ç”Ÿæˆè¦–è¦ºåŒ–...")
    create_fast_visualization(trace, results, subject_data)
    
    print(f"\nğŸ‰ å¿«é€Ÿå”æ–¹å·®çŸ©é™£LBAåˆ†æå®Œæˆï¼")
    
    return trace, subject_data, model

# ============================================================================
# å¿«é€Ÿæ¸¬è©¦å‡½æ•¸
# ============================================================================

def quick_test():
    """
    å¿«é€Ÿæ¸¬è©¦å‡½æ•¸
    """
    print("ğŸ§ª åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦...")
    return run_fast_covariance_analysis(max_trials=100)

# ============================================================================
# åŸ·è¡Œåˆ†æ
# ============================================================================

if __name__ == "__main__":
    print("ğŸ”¬ å„ªåŒ–ç‰ˆå”æ–¹å·®çŸ©é™£LBAæ¨¡å‹")
    print("è§£æ±ºæ€§èƒ½å•é¡Œå’Œå…¼å®¹æ€§å•é¡Œ")
    print("-" * 60)
    
    # åŸ·è¡Œå¿«é€Ÿåˆ†æ
    trace, data, model = run_fast_covariance_analysis()
    
    if trace is not None:
        print("\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆï¼")
        print("å¦‚æœéœ€è¦æ›´è©³ç´°çš„åˆ†æï¼Œå¯ä»¥å¢åŠ  draws å’Œ tune åƒæ•¸")
    else:
        print("\nâŒ åˆ†æå¤±æ•—")
        print("å˜—è©¦åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦: quick_test()")

# ============================================================================
# ä½¿ç”¨èªªæ˜
# ============================================================================

"""
å¿«é€Ÿä½¿ç”¨æ–¹æ³•ï¼š

1. åŸºæœ¬åŸ·è¡Œï¼š
   trace, data, model = run_fast_covariance_analysis()

2. æŒ‡å®šåƒæ•¸ï¼š
   trace, data, model = run_fast_covariance_analysis(
       csv_file_path='your_file.csv',
       subject_id=1,
       max_trials=150
   )

3. å¿«é€Ÿæ¸¬è©¦ï¼ˆä½¿ç”¨æ¨¡æ“¬è³‡æ–™ï¼‰ï¼š
   trace, data, model = quick_test()

4. å¦‚æœä»ç„¶å¾ˆæ…¢ï¼Œå¯ä»¥é€²ä¸€æ­¥æ¸›å°‘åƒæ•¸ï¼š
   # ä¿®æ”¹ run_fast_covariance_analysis ä¸­çš„æ¡æ¨£åƒæ•¸
   # draws=100, tune=50, chains=1
""""""
å„ªåŒ–ç‰ˆå”æ–¹å·®çŸ©é™£LBAåˆ†æ
è§£æ±ºæ€§èƒ½å•é¡Œå’ŒPyTensorå…¼å®¹æ€§å•é¡Œ
"""

import numpy as np
import pandas as pd
import pymc as pm
import pytensor.tensor as pt
import arviz as az
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# è¨­ç½®æ›´é«˜æ•ˆçš„æ¡æ¨£åƒæ•¸
import pytensor
pytensor.config.floatX = 'float32'  # ä½¿ç”¨float32æé«˜é€Ÿåº¦

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šè³‡æ–™è¼‰å…¥å‡½æ•¸ï¼ˆå„ªåŒ–ç‰ˆï¼‰
# ============================================================================

def load_real_subject_data(csv_file_path='GRT_LBA.csv', max_trials_per_subject=300):
    """
    è¼‰å…¥çœŸå¯¦å—è©¦è€…è³‡æ–™ï¼ˆé™åˆ¶è©¦é©—æ•¸é‡ä»¥æé«˜é€Ÿåº¦ï¼‰
    """
    
    print("è¼‰å…¥çœŸå¯¦å—è©¦è€…è³‡æ–™...")
    
    try:
        raw_data = pd.read_csv(csv_file_path)
        print(f"æˆåŠŸè®€å– {csv_file_path}")
        print(f"åŸå§‹è³‡æ–™ç¶­åº¦ï¼š{raw_data.shape}")
        
    except FileNotFoundError:
        print(f"æ‰¾ä¸åˆ°æª”æ¡ˆ {csv_file_path}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    except Exception as e:
        print(f"è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    
    # æª¢æŸ¥å¿…è¦çš„æ¬„ä½
    required_columns = ['Chanel1', 'Chanel2', 'Response', 'RT', 'acc']
    missing_columns = [col for col in required_columns if col not in raw_data.columns]
    
    if missing_columns:
        print(f"ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{missing_columns}")
        print(f"å¯ç”¨æ¬„ä½ï¼š{list(raw_data.columns)}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    
    # è³‡æ–™æ¸…ç†å’Œè½‰æ›
    print("è½‰æ›è³‡æ–™æ ¼å¼...")
    
    # ç§»é™¤ç¼ºå¤±å€¼
    clean_data = raw_data.dropna(subset=required_columns)
    print(f"æ¸…ç†å¾Œè³‡æ–™ç¶­åº¦ï¼š{clean_data.shape}")
    
    # ç§»é™¤æ¥µç«¯åæ‡‰æ™‚é–“
    clean_data = clean_data[(clean_data['RT'] > 0.1) & (clean_data['RT'] < 3.0)]
    print(f"ç§»é™¤æ¥µç«¯RTå¾Œç¶­åº¦ï¼š{clean_data.shape}")
    
    # é™åˆ¶æ¯å€‹å—è©¦è€…çš„è©¦é©—æ•¸é‡
    if 'Subject' in clean_data.columns:
        clean_data = clean_data.groupby('Subject').head(max_trials_per_subject)
    elif 'participant' in clean_data.columns:
        clean_data = clean_data.groupby('participant').head(max_trials_per_subject)
    
    # è½‰æ›ç‚ºæ¨¡å‹éœ€è¦çš„æ ¼å¼
    converted_data = []
    
    for idx, row in clean_data.iterrows():
        # ç¢ºä¿Responseåœ¨0-3ç¯„åœå…§
        if not (0 <= row['Response'] <= 3):
            continue
            
        # ç¢ºä¿Chanelå€¼ç‚º0æˆ–1
        if row['Chanel1'] not in [0, 1] or row['Chanel2'] not in [0, 1]:
            continue
        
        converted_row = {
            'subject_id': row.get('Subject', row.get('participant', 1)),
            'trial': len(converted_data) + 1,
            'left_pattern': int(row['Chanel1']),
            'right_pattern': int(row['Chanel2']),
            'response': int(row['Response']),
            'rt': float(row['RT']),
            'accuracy': int(row['acc']),
            'stimulus_type': int(row['Chanel1']) * 2 + int(row['Chanel2']),
            'is_symmetric': 1 if row['Chanel1'] == row['Chanel2'] else 0
        }
        
        converted_data.append(converted_row)
    
    # è½‰æ›ç‚ºDataFrame
    df = pd.DataFrame(converted_data)
    
    if len(df) == 0:
        print("âŒ æ²’æœ‰æœ‰æ•ˆçš„è³‡æ–™è¡Œï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™")
        return generate_test_data()
    
    # è³‡æ–™çµ±è¨ˆ
    print(f"\nâœ… çœŸå¯¦è³‡æ–™è¼‰å…¥å®Œæˆï¼š")
    print(f"  æœ‰æ•ˆè©¦é©—æ•¸ï¼š{len(df)}")
    print(f"  å—è©¦è€…æ•¸ï¼š{df['subject_id'].nunique()}")
    print(f"  æ•´é«”æº–ç¢ºç‡ï¼š{df['accuracy'].mean():.3f}")
    print(f"  å¹³å‡RTï¼š{df['rt'].mean():.3f}ç§’")
    print(f"  å°ç¨±è©¦é©—æ¯”ä¾‹ï¼š{df['is_symmetric'].mean():.3f}")
    
    return df

def generate_test_data(n_trials=200):
    """
    ç”Ÿæˆæ¸¬è©¦ç”¨çš„æ¨¡æ“¬è³‡æ–™
    """
    print("ç”Ÿæˆæ¨¡æ“¬æ¸¬è©¦è³‡æ–™...")
    
    np.random.seed(42)
    data = []
    
    for trial in range(n_trials):
        # éš¨æ©Ÿåˆºæ¿€
        left_pattern = np.random.choice([0, 1])
        right_pattern = np.random.choice([0, 1])
        is_symmetric = int(left_pattern == right_pattern)
        stimulus_type = left_pattern * 2 + right_pattern
        
        # æ¨¡æ“¬åæ‡‰
        # å°ç¨±åˆºæ¿€æœ‰è¼ƒé«˜æº–ç¢ºç‡
        base_accuracy = 0.8 if is_symmetric else 0.7
        accuracy = int(np.random.random() < base_accuracy)
        
        # æ­£ç¢ºåæ‡‰
        if accuracy:
            response = stimulus_type
        else:
            response = np.random.choice([i for i in range(4) if i != stimulus_type])
        
        # åæ‡‰æ™‚é–“
        base_rt = 0.8 if is_symmetric else 0.9
        rt = np.random.gamma(2, base_rt/2)
        rt = np.clip(rt, 0.2, 3.0)
        
        data.append({
            'subject_id': 1,
            'trial': trial + 1,
            'left_pattern': left_pattern,
            'right_pattern': right_pattern,
            'response': response,
            'rt': rt,
            'accuracy': accuracy,
            'stimulus_type': stimulus_type,
            'is_symmetric': is_symmetric
        })
    
    df = pd.DataFrame(data)
    print(f"âœ… æ¨¡æ“¬è³‡æ–™ç”Ÿæˆå®Œæˆï¼š{len(df)} è©¦é©—")
    return df

# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šé«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹
# ============================================================================

def build_fast_covariance_lba_model(data):
    """
    å»ºæ§‹é«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹
    """
    
    print("å»ºæ§‹é«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹...")
    
    # æº–å‚™è³‡æ–™ï¼ˆè½‰ç‚ºnumpyé™£åˆ—æé«˜æ•ˆç‡ï¼‰
    rt_obs = data['rt'].values.astype('float32')
    response_obs = data['response'].values.astype('int32')
    is_symmetric = data['is_symmetric'].values.astype('float32')
    stimulus_type = data['stimulus_type'].values.astype('int32')
    
    n_trials = len(data)
    print(f"è™•ç† {n_trials} å€‹è©¦é©—")
    
    with pm.Model() as model:
        
        # å”æ–¹å·®åƒæ•¸ï¼ˆç°¡åŒ–ï¼‰
        rho_base = pm.Uniform('rho_base', lower=-0.5, upper=0.5)
        rho_symmetry_effect = pm.Normal('rho_symmetry_effect', mu=0, sigma=0.2)
        
        # LBAåƒæ•¸ï¼ˆç°¡åŒ–ï¼‰
        A = pm.HalfNormal('A', sigma=0.2)
        b_base = pm.HalfNormal('b_base', sigma=0.3)
        s = pm.HalfNormal('s', sigma=0.2)
        t0 = pm.HalfNormal('t0', sigma=0.1)
        
        # åŸºç¤drift rate
        drift_base = pm.HalfNormal('drift_base', sigma=0.5)
        drift_correct_boost = pm.HalfNormal('drift_correct_boost', sigma=0.3)
        
        # å°ç¨±æ€§æ•ˆæ‡‰
        symmetry_boost = pm.Normal('symmetry_boost', mu=0, sigma=0.1)
        
        # å‘é‡åŒ–è¨ˆç®—drift rates
        def compute_drift_rates_vectorized():
            # ç‚ºæ¯å€‹ç´¯åŠ å™¨è¨ˆç®—drift rate
            drift_rates = pt.zeros((n_trials, 4))
            
            for acc in range(4):
                # åŸºç¤drift
                base_drift = drift_base
                
                # æ­£ç¢ºç´¯åŠ å™¨å¢å¼·
                correct_boost = pt.where(pt.eq(stimulus_type, acc), drift_correct_boost, 0.0)
                
                # å°ç¨±æ€§æ•ˆæ‡‰
                sym_boost = symmetry_boost * is_symmetric * pt.where(pt.eq(stimulus_type, acc), 1.0, 0.0)
                
                # ç›¸é—œæ€§æ•ˆæ‡‰ï¼ˆç°¡åŒ–ï¼‰
                rho_trial = rho_base + rho_symmetry_effect * is_symmetric
                correlation_effect = pt.abs(rho_trial) * 0.1
                
                # ç¸½drift rate
                final_drift = base_drift + correct_boost + sym_boost + correlation_effect
                final_drift = pt.maximum(final_drift, 0.05)
                
                drift_rates = pt.set_subtensor(drift_rates[:, acc], final_drift)
            
            return drift_rates
        
        drift_rates = compute_drift_rates_vectorized()
        
        # ç°¡åŒ–çš„ä¼¼ç„¶å‡½æ•¸ï¼ˆå‘é‡åŒ–ï¼‰
        def vectorized_lba_logp():
            # èª¿æ•´æ™‚é–“
            t_adj = pt.maximum(rt_obs - t0, 0.01)
            
            # é¸æ“‡çš„ç´¯åŠ å™¨drift rates
            chosen_drifts = drift_rates[pt.arange(n_trials), response_obs]
            
            # é–¾å€¼
            thresholds = b_base
            
            # æ™‚é–“ä¼¼ç„¶ï¼ˆç°¡åŒ–ç‚ºæŒ‡æ•¸åˆ†ä½ˆï¼‰
            lambda_param = chosen_drifts / thresholds
            time_logp = pt.log(lambda_param) - lambda_param * t_adj
            
            # é¸æ“‡ä¼¼ç„¶ï¼ˆsoftmaxï¼‰
            choice_logits = drift_rates * 3.0
            choice_logp = choice_logits[pt.arange(n_trials), response_obs] - pt.logsumexp(choice_logits, axis=1)
            
            return pt.sum(time_logp + choice_logp)
        
        # è§€æ¸¬ä¼¼ç„¶
        pm.Potential('likelihood', vectorized_lba_logp())
        
        # å„²å­˜é‡è¦è®Šæ•¸
        pm.Deterministic('rho_symmetric', rho_base + rho_symmetry_effect)
        pm.Deterministic('rho_asymmetric', rho_base)
        pm.Deterministic('rho_difference', rho_symmetry_effect)
    
    return model

# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¿«é€Ÿçµæœåˆ†æ
# ============================================================================

def analyze_fast_results(trace, data):
    """
    å¿«é€Ÿåˆ†ææ¨¡å‹çµæœ
    """
    
    print("\n" + "="*50)
    print("ğŸ“Š å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹çµæœï¼ˆå¿«é€Ÿç‰ˆï¼‰")
    print("="*50)
    
    # æå–å¾Œé©—æ¨£æœ¬
    posterior = trace.posterior
    
    # å”æ–¹å·®åˆ†æ
    print(f"\nğŸ“ˆ å”æ–¹å·®çŸ©é™£åˆ†æ:")
    
    rho_base = posterior['rho_base'].values.flatten()
    rho_symmetry_effect = posterior['rho_symmetry_effect'].values.flatten()
    rho_symmetric = posterior['rho_symmetric'].values.flatten()
    rho_asymmetric = posterior['rho_asymmetric'].values.flatten()
    
    print(f"åŸºç¤ç›¸é—œä¿‚æ•¸: {np.mean(rho_base):.3f} [{np.percentile(rho_base, 2.5):.3f}, {np.percentile(rho_base, 97.5):.3f}]")
    print(f"å°ç¨±æ€§æ•ˆæ‡‰: {np.mean(rho_symmetry_effect):.3f} [{np.percentile(rho_symmetry_effect, 2.5):.3f}, {np.percentile(rho_symmetry_effect, 97.5):.3f}]")
    print(f"å°ç¨±åˆºæ¿€ç›¸é—œä¿‚æ•¸: {np.mean(rho_symmetric):.3f}")
    print(f"éå°ç¨±åˆºæ¿€ç›¸é—œä¿‚æ•¸: {np.mean(rho_asymmetric):.3f}")
    
    # é¡¯è‘—æ€§æ¸¬è©¦
    rho_diff_significant = not (np.percentile(rho_symmetry_effect, 2.5) < 0 < np.percentile(rho_symmetry_effect, 97.5))
    print(f"å°ç¨±æ€§æ•ˆæ‡‰é¡¯è‘—æ€§: {'æ˜¯' if rho_diff_significant else 'å¦'}")
    
    # ç¨ç«‹æ€§å‡è¨­æª¢é©—
    independence_prob = np.mean(np.abs(rho_base) < 0.1)
    print(f"\nğŸ”¬ GRTç¨ç«‹æ€§å‡è¨­:")
    print(f"åŸºç¤ç¨ç«‹æ€§æ©Ÿç‡: {independence_prob:.3f}")
    
    if independence_prob < 0.05:
        print("ğŸš¨ å¼·çƒˆé•åGRTç¨ç«‹æ€§å‡è¨­")
    elif independence_prob < 0.2:
        print("âš ï¸  ä¸­ç­‰ç¨‹åº¦é•åç¨ç«‹æ€§å‡è¨­")
    else:
        print("âœ… åŸºæœ¬æ”¯æŒç¨ç«‹æ€§å‡è¨­")
    
    # ç†è«–è§£é‡‹
    print(f"\nğŸ’¡ ç†è«–è§£é‡‹:")
    model_rho_diff = np.mean(rho_symmetry_effect)
    
    if abs(model_rho_diff) > 0.05:
        if model_rho_diff > 0:
            print("âœ“ å°ç¨±åˆºæ¿€å¢åŠ sourceé–“ç›¸é—œæ€§")
            print("  â†’ æ”¯æŒé…ç½®æ€§è™•ç†å‡è¨­")
        else:
            print("âœ“ å°ç¨±åˆºæ¿€æ¸›å°‘sourceé–“ç›¸é—œæ€§")
            print("  â†’ æ”¯æŒç¨ç«‹è™•ç†å‡è¨­")
    else:
        print("â€¢ å°ç¨±æ€§å°ç›¸é—œæ€§å½±éŸ¿è¼ƒå°")
    
    return {
        'rho_base': rho_base,
        'rho_symmetry_effect': rho_symmetry_effect,
        'rho_symmetric': rho_symmetric,
        'rho_asymmetric': rho_asymmetric,
        'significant_symmetry_effect': rho_diff_significant,
        'independence_violation': independence_prob < 0.2
    }

# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šå¿«é€Ÿè¦–è¦ºåŒ–
# ============================================================================

def create_fast_visualization(trace, results, data):
    """
    å‰µå»ºå¿«é€Ÿçµæœè¦–è¦ºåŒ–
    """
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # 1. ç›¸é—œä¿‚æ•¸æ¯”è¼ƒ
    axes[0, 0].hist(results['rho_symmetric'], bins=20, alpha=0.6, label='å°ç¨±', color='green', density=True)
    axes[0, 0].hist(results['rho_asymmetric'], bins=20, alpha=0.6, label='éå°ç¨±', color='orange', density=True)
    axes[0, 0].axvline(0, color='red', linestyle='--', alpha=0.7, label='ç¨ç«‹æ€§')
    axes[0, 0].set_xlabel('ç›¸é—œä¿‚æ•¸ Ï')
    axes[0, 0].set_ylabel('å¯†åº¦')
    axes[0, 0].set_title('ç›¸é—œä¿‚æ•¸å¾Œé©—åˆ†ä½ˆ')
    axes[0, 0].legend()
    
    # 2. å°ç¨±æ€§æ•ˆæ‡‰
    axes[0, 1].hist(results['rho_symmetry_effect'], bins=20, alpha=0.7, color='purple', density=True)
    axes[0, 1].axvline(0, color='red', linestyle='--', alpha=0.7)
    axes[0, 1].axvline(np.mean(results['rho_symmetry_effect']), color='black', linestyle='-', label='å¹³å‡')
    axes[0, 1].set_xlabel('ç›¸é—œä¿‚æ•¸å·®ç•°')
    axes[0, 1].set_ylabel('å¯†åº¦')
    axes[0, 1].set_title('å°ç¨±æ€§æ•ˆæ‡‰')
    axes[0, 1].legend()
    
    # 3. è¡Œç‚ºè³‡æ–™å°æ¯”
    symmetric_data = data[data['is_symmetric'] == 1]
    asymmetric_data = data[data['is_symmetric'] == 0]
    
    if len(symmetric_data) > 0 and len(asymmetric_data) > 0:
        categories = ['æº–ç¢ºç‡', 'åæ‡‰æ™‚é–“']
        sym_values = [symmetric_data['accuracy'].mean(), symmetric_data['rt'].mean()]
        asym_values = [asymmetric_data['accuracy'].mean(), asymmetric_data['rt'].mean()]
        
        x = np.arange(len(categories))
        width = 0.35
        
        axes[1, 0].bar(x - width/2, sym_values, width, label='å°ç¨±', color='green', alpha=0.7)
        axes[1, 0].bar(x + width/2, asym_values, width, label='éå°ç¨±', color='orange', alpha=0.7)
        axes[1, 0].set_ylabel('å€¼')
        axes[1, 0].set_title('è¡Œç‚ºè³‡æ–™å°æ¯”')
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels(categories)
        axes[1, 0].legend()
    
    # 4. ç¸½çµ
    axes[1, 1].text(0.1, 0.9, 'æ¨¡å‹ç¸½çµ', fontsize=14, fontweight='bold', transform=axes[1, 1].transAxes)
    
    summary_text = f"""
ç¨ç«‹æ€§é•å: {'æ˜¯' if results['independence_violation'] else 'å¦'}
å°ç¨±æ€§æ•ˆæ‡‰: {'é¡¯è‘—' if results['significant_symmetry_effect'] else 'ä¸é¡¯è‘—'}

åŸºç¤ç›¸é—œ: {np.mean(results['rho_base']):.3f}
å°ç¨±å¢å¼·: {np.mean(results['rho_symmetry_effect']):.3f}

è§£é‡‹: {'é…ç½®æ€§è™•ç†' if np.mean(results['rho_symmetry_effect']) > 0.05 else 'ç¨ç«‹è™•ç†' if np.mean(results['rho_symmetry_effect']) < -0.05 else 'å½±éŸ¿è¼ƒå°'}
"""
    
    axes[1, 1].text(0.1, 0.7, summary_text, fontsize=10, transform=axes[1, 1].transAxes,
                    verticalalignment='top', fontfamily='monospace')
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    plt.savefig('fast_covariance_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("ğŸ“Š åˆ†æçµæœå·²å„²å­˜ç‚º 'fast_covariance_analysis.png'")

# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šå¿«é€ŸåŸ·è¡Œå‡½æ•¸
# ============================================================================

def run_fast_covariance_analysis(csv_file_path='GRT_LBA.csv', subject_id=None, max_trials=200):
    """
    åŸ·è¡Œå¿«é€Ÿå”æ–¹å·®çŸ©é™£åˆ†æ
    """
    
    print("ğŸš€ é–‹å§‹å¿«é€Ÿå”æ–¹å·®çŸ©é™£LBAåˆ†æ")
    print("="*60)
    
    # 1. è¼‰å…¥è³‡æ–™
    data = load_real_subject_data(csv_file_path, max_trials_per_subject=max_trials)
    if data is None:
        return None, None, None
    
    # 2. é¸æ“‡å—è©¦è€…
    if subject_id is None:
        if 'subject_id' in data.columns:
            subject_counts = data['subject_id'].value_counts()
            selected_subject = subject_counts.index[0]
            print(f"\nğŸ¯ è‡ªå‹•é¸æ“‡å—è©¦è€… {selected_subject} (è©¦é©—æ•¸: {subject_counts.iloc[0]})")
        else:
            selected_subject = 1
            print(f"ğŸ¯ ä½¿ç”¨é è¨­å—è©¦è€… {selected_subject}")
    else:
        selected_subject = subject_id
        print(f"ğŸ¯ é¸æ“‡å—è©¦è€… {selected_subject}")
    
    # æå–å—è©¦è€…è³‡æ–™
    subject_data = data[data['subject_id'] == selected_subject].copy()
    
    # 3. é™åˆ¶è©¦é©—æ•¸
    if len(subject_data) > max_trials:
        print(f"âš ï¸  é™åˆ¶è©¦é©—æ•¸ç‚º {max_trials}")
        subject_data = subject_data.sample(n=max_trials, random_state=42).reset_index(drop=True)
    
    print(f"\nğŸ“Š åˆ†æè³‡æ–™æ‘˜è¦ï¼š")
    print(f"  å—è©¦è€…: {selected_subject}")
    print(f"  è©¦é©—æ•¸: {len(subject_data)}")
    print(f"  æº–ç¢ºç‡: {subject_data['accuracy'].mean():.3f}")
    print(f"  å¹³å‡RT: {subject_data['rt'].mean():.3f}s")
    
    # 4. å»ºæ§‹æ¨¡å‹
    print(f"\nğŸ”§ å»ºæ§‹å¿«é€Ÿæ¨¡å‹...")
    model = build_fast_covariance_lba_model(subject_data)
    
    # 5. åŸ·è¡Œå¿«é€Ÿæ¡æ¨£
    print("â³ é–‹å§‹å¿«é€ŸMCMCæ¡æ¨£...")
    print("æ¡æ¨£åƒæ•¸ï¼š200 draws + 100 tune, 2 chains")
    
    try:
        with model:
            trace = pm.sample(
                draws=200,
                tune=100,
                chains=2,
                cores=1,  # å–®æ ¸å¿ƒé¿å…ä¸¦è¡Œå•é¡Œ
                target_accept=0.8,
                return_inferencedata=True,
                random_seed=456,
                progressbar=True,
                compute_convergence_checks=False  # è·³éæ”¶æ–‚æª¢æŸ¥åŠ å¿«é€Ÿåº¦
            )
        
        print("âœ… å¿«é€ŸMCMCæ¡æ¨£å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ MCMCæ¡æ¨£å¤±æ•—: {e}")
        print("å˜—è©¦æ›´ç°¡å–®çš„æ¡æ¨£è¨­å®š...")
        
        try:
            with model:
                trace = pm.sample(
                    draws=100,
                    tune=50,
                    chains=1,
                    cores=1,
                    return_inferencedata=True,
                    random_seed=456,
                    progressbar=True,
                    compute_convergence_checks=False
                )
            print("âœ… ç°¡åŒ–æ¡æ¨£å®Œæˆï¼")
        except Exception as e2:
            print(f"âŒ ç°¡åŒ–æ¡æ¨£ä¹Ÿå¤±æ•—: {e2}")
            return None, None, None
    
    # 6. åˆ†æçµæœ
    print(f"\nğŸ“ˆ åˆ†æçµæœ...")
    results = analyze_fast_results(trace, subject_data)
    
    # 7. å‰µå»ºè¦–è¦ºåŒ–
    print(f"\nğŸ¨ ç”Ÿæˆè¦–è¦ºåŒ–...")
    create_fast_visualization(trace, results, subject_data)
    
    print(f"\nğŸ‰ å¿«é€Ÿå”æ–¹å·®çŸ©é™£LBAåˆ†æå®Œæˆï¼")
    
    return trace, subject_data, model

# ============================================================================
# å¿«é€Ÿæ¸¬è©¦å‡½æ•¸
# ============================================================================

def quick_test():
    """
    å¿«é€Ÿæ¸¬è©¦å‡½æ•¸
    """
    print("ğŸ§ª åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦...")
    return run_fast_covariance_analysis(max_trials=100)

# ============================================================================
# åŸ·è¡Œåˆ†æ
# ============================================================================

if __name__ == "__main__":
    print("ğŸ”¬ å„ªåŒ–ç‰ˆå”æ–¹å·®çŸ©é™£LBAæ¨¡å‹")
    print("è§£æ±ºæ€§èƒ½å•é¡Œå’Œå…¼å®¹æ€§å•é¡Œ")
    print("-" * 60)
    
    # åŸ·è¡Œå¿«é€Ÿåˆ†æ
    trace, data, model = run_fast_covariance_analysis()
    
    if trace is not None:
        print("\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆï¼")
        print("å¦‚æœéœ€è¦æ›´è©³ç´°çš„åˆ†æï¼Œå¯ä»¥å¢åŠ  draws å’Œ tune åƒæ•¸")
    else:
        print("\nâŒ åˆ†æå¤±æ•—")
        print("å˜—è©¦åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦: quick_test()")

# ============================================================================
# ä½¿ç”¨èªªæ˜
# ============================================================================

"""
å¿«é€Ÿä½¿ç”¨æ–¹æ³•ï¼š

1. åŸºæœ¬åŸ·è¡Œï¼š
   trace, data, model = run_fast_covariance_analysis()

2. æŒ‡å®šåƒæ•¸ï¼š
   trace, data, model = run_fast_covariance_analysis(
       csv_file_path='your_file.csv',
       subject_id=1,
       max_trials=150
   )

3. å¿«é€Ÿæ¸¬è©¦ï¼ˆä½¿ç”¨æ¨¡æ“¬è³‡æ–™ï¼‰ï¼š
   trace, data, model = quick_test()

4. å¦‚æœä»ç„¶å¾ˆæ…¢ï¼Œå¯ä»¥é€²ä¸€æ­¥æ¸›å°‘åƒæ•¸ï¼š
   # ä¿®æ”¹ run_fast_covariance_analysis ä¸­çš„æ¡æ¨£åƒæ•¸
   # draws=100, tune=50, chains=1
""""""
å„ªåŒ–ç‰ˆå”æ–¹å·®çŸ©é™£LBAåˆ†æ
è§£æ±ºæ€§èƒ½å•é¡Œå’ŒPyTensorå…¼å®¹æ€§å•é¡Œ
"""

import numpy as np
import pandas as pd
import pymc as pm
import pytensor.tensor as pt
import arviz as az
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# è¨­ç½®æ›´é«˜æ•ˆçš„æ¡æ¨£åƒæ•¸
import pytensor
pytensor.config.floatX = 'float32'  # ä½¿ç”¨float32æé«˜é€Ÿåº¦

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šè³‡æ–™è¼‰å…¥å‡½æ•¸ï¼ˆå„ªåŒ–ç‰ˆï¼‰
# ============================================================================

def load_real_subject_data(csv_file_path='GRT_LBA.csv', max_trials_per_subject=300):
    """
    è¼‰å…¥çœŸå¯¦å—è©¦è€…è³‡æ–™ï¼ˆé™åˆ¶è©¦é©—æ•¸é‡ä»¥æé«˜é€Ÿåº¦ï¼‰
    """
    
    print("è¼‰å…¥çœŸå¯¦å—è©¦è€…è³‡æ–™...")
    
    try:
        raw_data = pd.read_csv(csv_file_path)
        print(f"æˆåŠŸè®€å– {csv_file_path}")
        print(f"åŸå§‹è³‡æ–™ç¶­åº¦ï¼š{raw_data.shape}")
        
    except FileNotFoundError:
        print(f"æ‰¾ä¸åˆ°æª”æ¡ˆ {csv_file_path}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    except Exception as e:
        print(f"è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    
    # æª¢æŸ¥å¿…è¦çš„æ¬„ä½
    required_columns = ['Chanel1', 'Chanel2', 'Response', 'RT', 'acc']
    missing_columns = [col for col in required_columns if col not in raw_data.columns]
    
    if missing_columns:
        print(f"ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{missing_columns}")
        print(f"å¯ç”¨æ¬„ä½ï¼š{list(raw_data.columns)}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    
    # è³‡æ–™æ¸…ç†å’Œè½‰æ›
    print("è½‰æ›è³‡æ–™æ ¼å¼...")
    
    # ç§»é™¤ç¼ºå¤±å€¼
    clean_data = raw_data.dropna(subset=required_columns)
    print(f"æ¸…ç†å¾Œè³‡æ–™ç¶­åº¦ï¼š{clean_data.shape}")
    
    # ç§»é™¤æ¥µç«¯åæ‡‰æ™‚é–“
    clean_data = clean_data[(clean_data['RT'] > 0.1) & (clean_data['RT'] < 3.0)]
    print(f"ç§»é™¤æ¥µç«¯RTå¾Œç¶­åº¦ï¼š{clean_data.shape}")
    
    # é™åˆ¶æ¯å€‹å—è©¦è€…çš„è©¦é©—æ•¸é‡
    if 'Subject' in clean_data.columns:
        clean_data = clean_data.groupby('Subject').head(max_trials_per_subject)
    elif 'participant' in clean_data.columns:
        clean_data = clean_data.groupby('participant').head(max_trials_per_subject)
    
    # è½‰æ›ç‚ºæ¨¡å‹éœ€è¦çš„æ ¼å¼
    converted_data = []
    
    for idx, row in clean_data.iterrows():
        # ç¢ºä¿Responseåœ¨0-3ç¯„åœå…§
        if not (0 <= row['Response'] <= 3):
            continue
            
        # ç¢ºä¿Chanelå€¼ç‚º0æˆ–1
        if row['Chanel1'] not in [0, 1] or row['Chanel2'] not in [0, 1]:
            continue
        
        converted_row = {
            'subject_id': row.get('Subject', row.get('participant', 1)),
            'trial': len(converted_data) + 1,
            'left_pattern': int(row['Chanel1']),
            'right_pattern': int(row['Chanel2']),
            'response': int(row['Response']),
            'rt': float(row['RT']),
            'accuracy': int(row['acc']),
            'stimulus_type': int(row['Chanel1']) * 2 + int(row['Chanel2']),
            'is_symmetric': 1 if row['Chanel1'] == row['Chanel2'] else 0
        }
        
        converted_data.append(converted_row)
    
    # è½‰æ›ç‚ºDataFrame
    df = pd.DataFrame(converted_data)
    
    if len(df) == 0:
        print("âŒ æ²’æœ‰æœ‰æ•ˆçš„è³‡æ–™è¡Œï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™")
        return generate_test_data()
    
    # è³‡æ–™çµ±è¨ˆ
    print(f"\nâœ… çœŸå¯¦è³‡æ–™è¼‰å…¥å®Œæˆï¼š")
    print(f"  æœ‰æ•ˆè©¦é©—æ•¸ï¼š{len(df)}")
    print(f"  å—è©¦è€…æ•¸ï¼š{df['subject_id'].nunique()}")
    print(f"  æ•´é«”æº–ç¢ºç‡ï¼š{df['accuracy'].mean():.3f}")
    print(f"  å¹³å‡RTï¼š{df['rt'].mean():.3f}ç§’")
    print(f"  å°ç¨±è©¦é©—æ¯”ä¾‹ï¼š{df['is_symmetric'].mean():.3f}")
    
    return df

def generate_test_data(n_trials=200):
    """
    ç”Ÿæˆæ¸¬è©¦ç”¨çš„æ¨¡æ“¬è³‡æ–™
    """
    print("ç”Ÿæˆæ¨¡æ“¬æ¸¬è©¦è³‡æ–™...")
    
    np.random.seed(42)
    data = []
    
    for trial in range(n_trials):
        # éš¨æ©Ÿåˆºæ¿€
        left_pattern = np.random.choice([0, 1])
        right_pattern = np.random.choice([0, 1])
        is_symmetric = int(left_pattern == right_pattern)
        stimulus_type = left_pattern * 2 + right_pattern
        
        # æ¨¡æ“¬åæ‡‰
        # å°ç¨±åˆºæ¿€æœ‰è¼ƒé«˜æº–ç¢ºç‡
        base_accuracy = 0.8 if is_symmetric else 0.7
        accuracy = int(np.random.random() < base_accuracy)
        
        # æ­£ç¢ºåæ‡‰
        if accuracy:
            response = stimulus_type
        else:
            response = np.random.choice([i for i in range(4) if i != stimulus_type])
        
        # åæ‡‰æ™‚é–“
        base_rt = 0.8 if is_symmetric else 0.9
        rt = np.random.gamma(2, base_rt/2)
        rt = np.clip(rt, 0.2, 3.0)
        
        data.append({
            'subject_id': 1,
            'trial': trial + 1,
            'left_pattern': left_pattern,
            'right_pattern': right_pattern,
            'response': response,
            'rt': rt,
            'accuracy': accuracy,
            'stimulus_type': stimulus_type,
            'is_symmetric': is_symmetric
        })
    
    df = pd.DataFrame(data)
    print(f"âœ… æ¨¡æ“¬è³‡æ–™ç”Ÿæˆå®Œæˆï¼š{len(df)} è©¦é©—")
    return df

# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šé«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹
# ============================================================================

def build_fast_covariance_lba_model(data):
    """
    å»ºæ§‹é«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹
    """
    
    print("å»ºæ§‹é«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹...")
    
    # æº–å‚™è³‡æ–™ï¼ˆè½‰ç‚ºnumpyé™£åˆ—æé«˜æ•ˆç‡ï¼‰
    rt_obs = data['rt'].values.astype('float32')
    response_obs = data['response'].values.astype('int32')
    is_symmetric = data['is_symmetric'].values.astype('float32')
    stimulus_type = data['stimulus_type'].values.astype('int32')
    
    n_trials = len(data)
    print(f"è™•ç† {n_trials} å€‹è©¦é©—")
    
    with pm.Model() as model:
        
        # å”æ–¹å·®åƒæ•¸ï¼ˆç°¡åŒ–ï¼‰
        rho_base = pm.Uniform('rho_base', lower=-0.5, upper=0.5)
        rho_symmetry_effect = pm.Normal('rho_symmetry_effect', mu=0, sigma=0.2)
        
        # LBAåƒæ•¸ï¼ˆç°¡åŒ–ï¼‰
        A = pm.HalfNormal('A', sigma=0.2)
        b_base = pm.HalfNormal('b_base', sigma=0.3)
        s = pm.HalfNormal('s', sigma=0.2)
        t0 = pm.HalfNormal('t0', sigma=0.1)
        
        # åŸºç¤drift rate
        drift_base = pm.HalfNormal('drift_base', sigma=0.5)
        drift_correct_boost = pm.HalfNormal('drift_correct_boost', sigma=0.3)
        
        # å°ç¨±æ€§æ•ˆæ‡‰
        symmetry_boost = pm.Normal('symmetry_boost', mu=0, sigma=0.1)
        
        # å‘é‡åŒ–è¨ˆç®—drift rates
        def compute_drift_rates_vectorized():
            # ç‚ºæ¯å€‹ç´¯åŠ å™¨è¨ˆç®—drift rate
            drift_rates = pt.zeros((n_trials, 4))
            
            for acc in range(4):
                # åŸºç¤drift
                base_drift = drift_base
                
                # æ­£ç¢ºç´¯åŠ å™¨å¢å¼·
                correct_boost = pt.where(pt.eq(stimulus_type, acc), drift_correct_boost, 0.0)
                
                # å°ç¨±æ€§æ•ˆæ‡‰
                sym_boost = symmetry_boost * is_symmetric * pt.where(pt.eq(stimulus_type, acc), 1.0, 0.0)
                
                # ç›¸é—œæ€§æ•ˆæ‡‰ï¼ˆç°¡åŒ–ï¼‰
                rho_trial = rho_base + rho_symmetry_effect * is_symmetric
                correlation_effect = pt.abs(rho_trial) * 0.1
                
                # ç¸½drift rate
                final_drift = base_drift + correct_boost + sym_boost + correlation_effect
                final_drift = pt.maximum(final_drift, 0.05)
                
                drift_rates = pt.set_subtensor(drift_rates[:, acc], final_drift)
            
            return drift_rates
        
        drift_rates = compute_drift_rates_vectorized()
        
        # ç°¡åŒ–çš„ä¼¼ç„¶å‡½æ•¸ï¼ˆå‘é‡åŒ–ï¼‰
        def vectorized_lba_logp():
            # èª¿æ•´æ™‚é–“
            t_adj = pt.maximum(rt_obs - t0, 0.01)
            
            # é¸æ“‡çš„ç´¯åŠ å™¨drift rates
            chosen_drifts = drift_rates[pt.arange(n_trials), response_obs]
            
            # é–¾å€¼
            thresholds = b_base
            
            # æ™‚é–“ä¼¼ç„¶ï¼ˆç°¡åŒ–ç‚ºæŒ‡æ•¸åˆ†ä½ˆï¼‰
            lambda_param = chosen_drifts / thresholds
            time_logp = pt.log(lambda_param) - lambda_param * t_adj
            
            # é¸æ“‡ä¼¼ç„¶ï¼ˆsoftmaxï¼‰
            choice_logits = drift_rates * 3.0
            choice_logp = choice_logits[pt.arange(n_trials), response_obs] - pt.logsumexp(choice_logits, axis=1)
            
            return pt.sum(time_logp + choice_logp)
        
        # è§€æ¸¬ä¼¼ç„¶
        pm.Potential('likelihood', vectorized_lba_logp())
        
        # å„²å­˜é‡è¦è®Šæ•¸
        pm.Deterministic('rho_symmetric', rho_base + rho_symmetry_effect)
        pm.Deterministic('rho_asymmetric', rho_base)
        pm.Deterministic('rho_difference', rho_symmetry_effect)
    
    return model

# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¿«é€Ÿçµæœåˆ†æ
# ============================================================================

def analyze_fast_results(trace, data):
    """
    å¿«é€Ÿåˆ†ææ¨¡å‹çµæœ
    """
    
    print("\n" + "="*50)
    print("ğŸ“Š å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹çµæœï¼ˆå¿«é€Ÿç‰ˆï¼‰")
    print("="*50)
    
    # æå–å¾Œé©—æ¨£æœ¬
    posterior = trace.posterior
    
    # å”æ–¹å·®åˆ†æ
    print(f"\nğŸ“ˆ å”æ–¹å·®çŸ©é™£åˆ†æ:")
    
    rho_base = posterior['rho_base'].values.flatten()
    rho_symmetry_effect = posterior['rho_symmetry_effect'].values.flatten()
    rho_symmetric = posterior['rho_symmetric'].values.flatten()
    rho_asymmetric = posterior['rho_asymmetric'].values.flatten()
    
    print(f"åŸºç¤ç›¸é—œä¿‚æ•¸: {np.mean(rho_base):.3f} [{np.percentile(rho_base, 2.5):.3f}, {np.percentile(rho_base, 97.5):.3f}]")
    print(f"å°ç¨±æ€§æ•ˆæ‡‰: {np.mean(rho_symmetry_effect):.3f} [{np.percentile(rho_symmetry_effect, 2.5):.3f}, {np.percentile(rho_symmetry_effect, 97.5):.3f}]")
    print(f"å°ç¨±åˆºæ¿€ç›¸é—œä¿‚æ•¸: {np.mean(rho_symmetric):.3f}")
    print(f"éå°ç¨±åˆºæ¿€ç›¸é—œä¿‚æ•¸: {np.mean(rho_asymmetric):.3f}")
    
    # é¡¯è‘—æ€§æ¸¬è©¦
    rho_diff_significant = not (np.percentile(rho_symmetry_effect, 2.5) < 0 < np.percentile(rho_symmetry_effect, 97.5))
    print(f"å°ç¨±æ€§æ•ˆæ‡‰é¡¯è‘—æ€§: {'æ˜¯' if rho_diff_significant else 'å¦'}")
    
    # ç¨ç«‹æ€§å‡è¨­æª¢é©—
    independence_prob = np.mean(np.abs(rho_base) < 0.1)
    print(f"\nğŸ”¬ GRTç¨ç«‹æ€§å‡è¨­:")
    print(f"åŸºç¤ç¨ç«‹æ€§æ©Ÿç‡: {independence_prob:.3f}")
    
    if independence_prob < 0.05:
        print("ğŸš¨ å¼·çƒˆé•åGRTç¨ç«‹æ€§å‡è¨­")
    elif independence_prob < 0.2:
        print("âš ï¸  ä¸­ç­‰ç¨‹åº¦é•åç¨ç«‹æ€§å‡è¨­")
    else:
        print("âœ… åŸºæœ¬æ”¯æŒç¨ç«‹æ€§å‡è¨­")
    
    # ç†è«–è§£é‡‹
    print(f"\nğŸ’¡ ç†è«–è§£é‡‹:")
    model_rho_diff = np.mean(rho_symmetry_effect)
    
    if abs(model_rho_diff) > 0.05:
        if model_rho_diff > 0:
            print("âœ“ å°ç¨±åˆºæ¿€å¢åŠ sourceé–“ç›¸é—œæ€§")
            print("  â†’ æ”¯æŒé…ç½®æ€§è™•ç†å‡è¨­")
        else:
            print("âœ“ å°ç¨±åˆºæ¿€æ¸›å°‘sourceé–“ç›¸é—œæ€§")
            print("  â†’ æ”¯æŒç¨ç«‹è™•ç†å‡è¨­")
    else:
        print("â€¢ å°ç¨±æ€§å°ç›¸é—œæ€§å½±éŸ¿è¼ƒå°")
    
    return {
        'rho_base': rho_base,
        'rho_symmetry_effect': rho_symmetry_effect,
        'rho_symmetric': rho_symmetric,
        'rho_asymmetric': rho_asymmetric,
        'significant_symmetry_effect': rho_diff_significant,
        'independence_violation': independence_prob < 0.2
    }

# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šå¿«é€Ÿè¦–è¦ºåŒ–
# ============================================================================

def create_fast_visualization(trace, results, data):
    """
    å‰µå»ºå¿«é€Ÿçµæœè¦–è¦ºåŒ–
    """
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # 1. ç›¸é—œä¿‚æ•¸æ¯”è¼ƒ
    axes[0, 0].hist(results['rho_symmetric'], bins=20, alpha=0.6, label='å°ç¨±', color='green', density=True)
    axes[0, 0].hist(results['rho_asymmetric'], bins=20, alpha=0.6, label='éå°ç¨±', color='orange', density=True)
    axes[0, 0].axvline(0, color='red', linestyle='--', alpha=0.7, label='ç¨ç«‹æ€§')
    axes[0, 0].set_xlabel('ç›¸é—œä¿‚æ•¸ Ï')
    axes[0, 0].set_ylabel('å¯†åº¦')
    axes[0, 0].set_title('ç›¸é—œä¿‚æ•¸å¾Œé©—åˆ†ä½ˆ')
    axes[0, 0].legend()
    
    # 2. å°ç¨±æ€§æ•ˆæ‡‰
    axes[0, 1].hist(results['rho_symmetry_effect'], bins=20, alpha=0.7, color='purple', density=True)
    axes[0, 1].axvline(0, color='red', linestyle='--', alpha=0.7)
    axes[0, 1].axvline(np.mean(results['rho_symmetry_effect']), color='black', linestyle='-', label='å¹³å‡')
    axes[0, 1].set_xlabel('ç›¸é—œä¿‚æ•¸å·®ç•°')
    axes[0, 1].set_ylabel('å¯†åº¦')
    axes[0, 1].set_title('å°ç¨±æ€§æ•ˆæ‡‰')
    axes[0, 1].legend()
    
    # 3. è¡Œç‚ºè³‡æ–™å°æ¯”
    symmetric_data = data[data['is_symmetric'] == 1]
    asymmetric_data = data[data['is_symmetric'] == 0]
    
    if len(symmetric_data) > 0 and len(asymmetric_data) > 0:
        categories = ['æº–ç¢ºç‡', 'åæ‡‰æ™‚é–“']
        sym_values = [symmetric_data['accuracy'].mean(), symmetric_data['rt'].mean()]
        asym_values = [asymmetric_data['accuracy'].mean(), asymmetric_data['rt'].mean()]
        
        x = np.arange(len(categories))
        width = 0.35
        
        axes[1, 0].bar(x - width/2, sym_values, width, label='å°ç¨±', color='green', alpha=0.7)
        axes[1, 0].bar(x + width/2, asym_values, width, label='éå°ç¨±', color='orange', alpha=0.7)
        axes[1, 0].set_ylabel('å€¼')
        axes[1, 0].set_title('è¡Œç‚ºè³‡æ–™å°æ¯”')
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels(categories)
        axes[1, 0].legend()
    
    # 4. ç¸½çµ
    axes[1, 1].text(0.1, 0.9, 'æ¨¡å‹ç¸½çµ', fontsize=14, fontweight='bold', transform=axes[1, 1].transAxes)
    
    summary_text = f"""
ç¨ç«‹æ€§é•å: {'æ˜¯' if results['independence_violation'] else 'å¦'}
å°ç¨±æ€§æ•ˆæ‡‰: {'é¡¯è‘—' if results['significant_symmetry_effect'] else 'ä¸é¡¯è‘—'}

åŸºç¤ç›¸é—œ: {np.mean(results['rho_base']):.3f}
å°ç¨±å¢å¼·: {np.mean(results['rho_symmetry_effect']):.3f}

è§£é‡‹: {'é…ç½®æ€§è™•ç†' if np.mean(results['rho_symmetry_effect']) > 0.05 else 'ç¨ç«‹è™•ç†' if np.mean(results['rho_symmetry_effect']) < -0.05 else 'å½±éŸ¿è¼ƒå°'}
"""
    
    axes[1, 1].text(0.1, 0.7, summary_text, fontsize=10, transform=axes[1, 1].transAxes,
                    verticalalignment='top', fontfamily='monospace')
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    plt.savefig('fast_covariance_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("ğŸ“Š åˆ†æçµæœå·²å„²å­˜ç‚º 'fast_covariance_analysis.png'")

# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šå¿«é€ŸåŸ·è¡Œå‡½æ•¸
# ============================================================================

def run_fast_covariance_analysis(csv_file_path='GRT_LBA.csv', subject_id=None, max_trials=200):
    """
    åŸ·è¡Œå¿«é€Ÿå”æ–¹å·®çŸ©é™£åˆ†æ
    """
    
    print("ğŸš€ é–‹å§‹å¿«é€Ÿå”æ–¹å·®çŸ©é™£LBAåˆ†æ")
    print("="*60)
    
    # 1. è¼‰å…¥è³‡æ–™
    data = load_real_subject_data(csv_file_path, max_trials_per_subject=max_trials)
    if data is None:
        return None, None, None
    
    # 2. é¸æ“‡å—è©¦è€…
    if subject_id is None:
        if 'subject_id' in data.columns:
            subject_counts = data['subject_id'].value_counts()
            selected_subject = subject_counts.index[0]
            print(f"\nğŸ¯ è‡ªå‹•é¸æ“‡å—è©¦è€… {selected_subject} (è©¦é©—æ•¸: {subject_counts.iloc[0]})")
        else:
            selected_subject = 1
            print(f"ğŸ¯ ä½¿ç”¨é è¨­å—è©¦è€… {selected_subject}")
    else:
        selected_subject = subject_id
        print(f"ğŸ¯ é¸æ“‡å—è©¦è€… {selected_subject}")
    
    # æå–å—è©¦è€…è³‡æ–™
    subject_data = data[data['subject_id'] == selected_subject].copy()
    
    # 3. é™åˆ¶è©¦é©—æ•¸
    if len(subject_data) > max_trials:
        print(f"âš ï¸  é™åˆ¶è©¦é©—æ•¸ç‚º {max_trials}")
        subject_data = subject_data.sample(n=max_trials, random_state=42).reset_index(drop=True)
    
    print(f"\nğŸ“Š åˆ†æè³‡æ–™æ‘˜è¦ï¼š")
    print(f"  å—è©¦è€…: {selected_subject}")
    print(f"  è©¦é©—æ•¸: {len(subject_data)}")
    print(f"  æº–ç¢ºç‡: {subject_data['accuracy'].mean():.3f}")
    print(f"  å¹³å‡RT: {subject_data['rt'].mean():.3f}s")
    
    # 4. å»ºæ§‹æ¨¡å‹
    print(f"\nğŸ”§ å»ºæ§‹å¿«é€Ÿæ¨¡å‹...")
    model = build_fast_covariance_lba_model(subject_data)
    
    # 5. åŸ·è¡Œå¿«é€Ÿæ¡æ¨£
    print("â³ é–‹å§‹å¿«é€ŸMCMCæ¡æ¨£...")
    print("æ¡æ¨£åƒæ•¸ï¼š200 draws + 100 tune, 2 chains")
    
    try:
        with model:
            trace = pm.sample(
                draws=200,
                tune=100,
                chains=2,
                cores=1,  # å–®æ ¸å¿ƒé¿å…ä¸¦è¡Œå•é¡Œ
                target_accept=0.8,
                return_inferencedata=True,
                random_seed=456,
                progressbar=True,
                compute_convergence_checks=False  # è·³éæ”¶æ–‚æª¢æŸ¥åŠ å¿«é€Ÿåº¦
            )
        
        print("âœ… å¿«é€ŸMCMCæ¡æ¨£å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ MCMCæ¡æ¨£å¤±æ•—: {e}")
        print("å˜—è©¦æ›´ç°¡å–®çš„æ¡æ¨£è¨­å®š...")
        
        try:
            with model:
                trace = pm.sample(
                    draws=100,
                    tune=50,
                    chains=1,
                    cores=1,
                    return_inferencedata=True,
                    random_seed=456,
                    progressbar=True,
                    compute_convergence_checks=False
                )
            print("âœ… ç°¡åŒ–æ¡æ¨£å®Œæˆï¼")
        except Exception as e2:
            print(f"âŒ ç°¡åŒ–æ¡æ¨£ä¹Ÿå¤±æ•—: {e2}")
            return None, None, None
    
    # 6. åˆ†æçµæœ
    print(f"\nğŸ“ˆ åˆ†æçµæœ...")
    results = analyze_fast_results(trace, subject_data)
    
    # 7. å‰µå»ºè¦–è¦ºåŒ–
    print(f"\nğŸ¨ ç”Ÿæˆè¦–è¦ºåŒ–...")
    create_fast_visualization(trace, results, subject_data)
    
    print(f"\nğŸ‰ å¿«é€Ÿå”æ–¹å·®çŸ©é™£LBAåˆ†æå®Œæˆï¼")
    
    return trace, subject_data, model

# ============================================================================
# å¿«é€Ÿæ¸¬è©¦å‡½æ•¸
# ============================================================================

def quick_test():
    """
    å¿«é€Ÿæ¸¬è©¦å‡½æ•¸
    """
    print("ğŸ§ª åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦...")
    return run_fast_covariance_analysis(max_trials=100)

# ============================================================================
# åŸ·è¡Œåˆ†æ
# ============================================================================

if __name__ == "__main__":
    print("ğŸ”¬ å„ªåŒ–ç‰ˆå”æ–¹å·®çŸ©é™£LBAæ¨¡å‹")
    print("è§£æ±ºæ€§èƒ½å•é¡Œå’Œå…¼å®¹æ€§å•é¡Œ")
    print("-" * 60)
    
    # åŸ·è¡Œå¿«é€Ÿåˆ†æ
    trace, data, model = run_fast_covariance_analysis()
    
    if trace is not None:
        print("\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆï¼")
        print("å¦‚æœéœ€è¦æ›´è©³ç´°çš„åˆ†æï¼Œå¯ä»¥å¢åŠ  draws å’Œ tune åƒæ•¸")
    else:
        print("\nâŒ åˆ†æå¤±æ•—")
        print("å˜—è©¦åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦: quick_test()")

# ============================================================================
# ä½¿ç”¨èªªæ˜
# ============================================================================

"""
å¿«é€Ÿä½¿ç”¨æ–¹æ³•ï¼š

1. åŸºæœ¬åŸ·è¡Œï¼š
   trace, data, model = run_fast_covariance_analysis()

2. æŒ‡å®šåƒæ•¸ï¼š
   trace, data, model = run_fast_covariance_analysis(
       csv_file_path='your_file.csv',
       subject_id=1,
       max_trials=150
   )

3. å¿«é€Ÿæ¸¬è©¦ï¼ˆä½¿ç”¨æ¨¡æ“¬è³‡æ–™ï¼‰ï¼š
   trace, data, model = quick_test()

4. å¦‚æœä»ç„¶å¾ˆæ…¢ï¼Œå¯ä»¥é€²ä¸€æ­¥æ¸›å°‘åƒæ•¸ï¼š
   # ä¿®æ”¹ run_fast_covariance_analysis ä¸­çš„æ¡æ¨£åƒæ•¸
   # draws=100, tune=50, chains=1
""""""
å„ªåŒ–ç‰ˆå”æ–¹å·®çŸ©é™£LBAåˆ†æ
è§£æ±ºæ€§èƒ½å•é¡Œå’ŒPyTensorå…¼å®¹æ€§å•é¡Œ
"""

import numpy as np
import pandas as pd
import pymc as pm
import pytensor.tensor as pt
import arviz as az
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# è¨­ç½®æ›´é«˜æ•ˆçš„æ¡æ¨£åƒæ•¸
import pytensor
pytensor.config.floatX = 'float32'  # ä½¿ç”¨float32æé«˜é€Ÿåº¦

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šè³‡æ–™è¼‰å…¥å‡½æ•¸ï¼ˆå„ªåŒ–ç‰ˆï¼‰
# ============================================================================

def load_real_subject_data(csv_file_path='GRT_LBA.csv', max_trials_per_subject=300):
    """
    è¼‰å…¥çœŸå¯¦å—è©¦è€…è³‡æ–™ï¼ˆé™åˆ¶è©¦é©—æ•¸é‡ä»¥æé«˜é€Ÿåº¦ï¼‰
    """
    
    print("è¼‰å…¥çœŸå¯¦å—è©¦è€…è³‡æ–™...")
    
    try:
        raw_data = pd.read_csv(csv_file_path)
        print(f"æˆåŠŸè®€å– {csv_file_path}")
        print(f"åŸå§‹è³‡æ–™ç¶­åº¦ï¼š{raw_data.shape}")
        
    except FileNotFoundError:
        print(f"æ‰¾ä¸åˆ°æª”æ¡ˆ {csv_file_path}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    except Exception as e:
        print(f"è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    
    # æª¢æŸ¥å¿…è¦çš„æ¬„ä½
    required_columns = ['Chanel1', 'Chanel2', 'Response', 'RT', 'acc']
    missing_columns = [col for col in required_columns if col not in raw_data.columns]
    
    if missing_columns:
        print(f"ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{missing_columns}")
        print(f"å¯ç”¨æ¬„ä½ï¼š{list(raw_data.columns)}")
        print("ç”Ÿæˆæ¨¡æ“¬è³‡æ–™é€²è¡Œæ¸¬è©¦...")
        return generate_test_data()
    
    # è³‡æ–™æ¸…ç†å’Œè½‰æ›
    print("è½‰æ›è³‡æ–™æ ¼å¼...")
    
    # ç§»é™¤ç¼ºå¤±å€¼
    clean_data = raw_data.dropna(subset=required_columns)
    print(f"æ¸…ç†å¾Œè³‡æ–™ç¶­åº¦ï¼š{clean_data.shape}")
    
    # ç§»é™¤æ¥µç«¯åæ‡‰æ™‚é–“
    clean_data = clean_data[(clean_data['RT'] > 0.1) & (clean_data['RT'] < 3.0)]
    print(f"ç§»é™¤æ¥µç«¯RTå¾Œç¶­åº¦ï¼š{clean_data.shape}")
    
    # é™åˆ¶æ¯å€‹å—è©¦è€…çš„è©¦é©—æ•¸é‡
    if 'Subject' in clean_data.columns:
        clean_data = clean_data.groupby('Subject').head(max_trials_per_subject)
    elif 'participant' in clean_data.columns:
        clean_data = clean_data.groupby('participant').head(max_trials_per_subject)
    
    # è½‰æ›ç‚ºæ¨¡å‹éœ€è¦çš„æ ¼å¼
    converted_data = []
    
    for idx, row in clean_data.iterrows():
        # ç¢ºä¿Responseåœ¨0-3ç¯„åœå…§
        if not (0 <= row['Response'] <= 3):
            continue
            
        # ç¢ºä¿Chanelå€¼ç‚º0æˆ–1
        if row['Chanel1'] not in [0, 1] or row['Chanel2'] not in [0, 1]:
            continue
        
        converted_row = {
            'subject_id': row.get('Subject', row.get('participant', 1)),
            'trial': len(converted_data) + 1,
            'left_pattern': int(row['Chanel1']),
            'right_pattern': int(row['Chanel2']),
            'response': int(row['Response']),
            'rt': float(row['RT']),
            'accuracy': int(row['acc']),
            'stimulus_type': int(row['Chanel1']) * 2 + int(row['Chanel2']),
            'is_symmetric': 1 if row['Chanel1'] == row['Chanel2'] else 0
        }
        
        converted_data.append(converted_row)
    
    # è½‰æ›ç‚ºDataFrame
    df = pd.DataFrame(converted_data)
    
    if len(df) == 0:
        print("âŒ æ²’æœ‰æœ‰æ•ˆçš„è³‡æ–™è¡Œï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™")
        return generate_test_data()
    
    # è³‡æ–™çµ±è¨ˆ
    print(f"\nâœ… çœŸå¯¦è³‡æ–™è¼‰å…¥å®Œæˆï¼š")
    print(f"  æœ‰æ•ˆè©¦é©—æ•¸ï¼š{len(df)}")
    print(f"  å—è©¦è€…æ•¸ï¼š{df['subject_id'].nunique()}")
    print(f"  æ•´é«”æº–ç¢ºç‡ï¼š{df['accuracy'].mean():.3f}")
    print(f"  å¹³å‡RTï¼š{df['rt'].mean():.3f}ç§’")
    print(f"  å°ç¨±è©¦é©—æ¯”ä¾‹ï¼š{df['is_symmetric'].mean():.3f}")
    
    return df

def generate_test_data(n_trials=200):
    """
    ç”Ÿæˆæ¸¬è©¦ç”¨çš„æ¨¡æ“¬è³‡æ–™
    """
    print("ç”Ÿæˆæ¨¡æ“¬æ¸¬è©¦è³‡æ–™...")
    
    np.random.seed(42)
    data = []
    
    for trial in range(n_trials):
        # éš¨æ©Ÿåˆºæ¿€
        left_pattern = np.random.choice([0, 1])
        right_pattern = np.random.choice([0, 1])
        is_symmetric = int(left_pattern == right_pattern)
        stimulus_type = left_pattern * 2 + right_pattern
        
        # æ¨¡æ“¬åæ‡‰
        # å°ç¨±åˆºæ¿€æœ‰è¼ƒé«˜æº–ç¢ºç‡
        base_accuracy = 0.8 if is_symmetric else 0.7
        accuracy = int(np.random.random() < base_accuracy)
        
        # æ­£ç¢ºåæ‡‰
        if accuracy:
            response = stimulus_type
        else:
            response = np.random.choice([i for i in range(4) if i != stimulus_type])
        
        # åæ‡‰æ™‚é–“
        base_rt = 0.8 if is_symmetric else 0.9
        rt = np.random.gamma(2, base_rt/2)
        rt = np.clip(rt, 0.2, 3.0)
        
        data.append({
            'subject_id': 1,
            'trial': trial + 1,
            'left_pattern': left_pattern,
            'right_pattern': right_pattern,
            'response': response,
            'rt': rt,
            'accuracy': accuracy,
            'stimulus_type': stimulus_type,
            'is_symmetric': is_symmetric
        })
    
    df = pd.DataFrame(data)
    print(f"âœ… æ¨¡æ“¬è³‡æ–™ç”Ÿæˆå®Œæˆï¼š{len(df)} è©¦é©—")
    return df

# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šé«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹
# ============================================================================

def build_fast_covariance_lba_model(data):
    """
    å»ºæ§‹é«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹
    """
    
    print("å»ºæ§‹é«˜æ•ˆèƒ½å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹...")
    
    # æº–å‚™è³‡æ–™ï¼ˆè½‰ç‚ºnumpyé™£åˆ—æé«˜æ•ˆç‡ï¼‰
    rt_obs = data['rt'].values.astype('float32')
    response_obs = data['response'].values.astype('int32')
    is_symmetric = data['is_symmetric'].values.astype('float32')
    stimulus_type = data['stimulus_type'].values.astype('int32')
    
    n_trials = len(data)
    print(f"è™•ç† {n_trials} å€‹è©¦é©—")
    
    with pm.Model() as model:
        
        # å”æ–¹å·®åƒæ•¸ï¼ˆç°¡åŒ–ï¼‰
        rho_base = pm.Uniform('rho_base', lower=-0.5, upper=0.5)
        rho_symmetry_effect = pm.Normal('rho_symmetry_effect', mu=0, sigma=0.2)
        
        # LBAåƒæ•¸ï¼ˆç°¡åŒ–ï¼‰
        A = pm.HalfNormal('A', sigma=0.2)
        b_base = pm.HalfNormal('b_base', sigma=0.3)
        s = pm.HalfNormal('s', sigma=0.2)
        t0 = pm.HalfNormal('t0', sigma=0.1)
        
        # åŸºç¤drift rate
        drift_base = pm.HalfNormal('drift_base', sigma=0.5)
        drift_correct_boost = pm.HalfNormal('drift_correct_boost', sigma=0.3)
        
        # å°ç¨±æ€§æ•ˆæ‡‰
        symmetry_boost = pm.Normal('symmetry_boost', mu=0, sigma=0.1)
        
        # å‘é‡åŒ–è¨ˆç®—drift rates
        def compute_drift_rates_vectorized():
            # ç‚ºæ¯å€‹ç´¯åŠ å™¨è¨ˆç®—drift rate
            drift_rates = pt.zeros((n_trials, 4))
            
            for acc in range(4):
                # åŸºç¤drift
                base_drift = drift_base
                
                # æ­£ç¢ºç´¯åŠ å™¨å¢å¼·
                correct_boost = pt.where(pt.eq(stimulus_type, acc), drift_correct_boost, 0.0)
                
                # å°ç¨±æ€§æ•ˆæ‡‰
                sym_boost = symmetry_boost * is_symmetric * pt.where(pt.eq(stimulus_type, acc), 1.0, 0.0)
                
                # ç›¸é—œæ€§æ•ˆæ‡‰ï¼ˆç°¡åŒ–ï¼‰
                rho_trial = rho_base + rho_symmetry_effect * is_symmetric
                correlation_effect = pt.abs(rho_trial) * 0.1
                
                # ç¸½drift rate
                final_drift = base_drift + correct_boost + sym_boost + correlation_effect
                final_drift = pt.maximum(final_drift, 0.05)
                
                drift_rates = pt.set_subtensor(drift_rates[:, acc], final_drift)
            
            return drift_rates
        
        drift_rates = compute_drift_rates_vectorized()
        
        # ç°¡åŒ–çš„ä¼¼ç„¶å‡½æ•¸ï¼ˆå‘é‡åŒ–ï¼‰
        def vectorized_lba_logp():
            # èª¿æ•´æ™‚é–“
            t_adj = pt.maximum(rt_obs - t0, 0.01)
            
            # é¸æ“‡çš„ç´¯åŠ å™¨drift rates
            chosen_drifts = drift_rates[pt.arange(n_trials), response_obs]
            
            # é–¾å€¼
            thresholds = b_base
            
            # æ™‚é–“ä¼¼ç„¶ï¼ˆç°¡åŒ–ç‚ºæŒ‡æ•¸åˆ†ä½ˆï¼‰
            lambda_param = chosen_drifts / thresholds
            time_logp = pt.log(lambda_param) - lambda_param * t_adj
            
            # é¸æ“‡ä¼¼ç„¶ï¼ˆsoftmaxï¼‰
            choice_logits = drift_rates * 3.0
            choice_logp = choice_logits[pt.arange(n_trials), response_obs] - pt.logsumexp(choice_logits, axis=1)
            
            return pt.sum(time_logp + choice_logp)
        
        # è§€æ¸¬ä¼¼ç„¶
        pm.Potential('likelihood', vectorized_lba_logp())
        
        # å„²å­˜é‡è¦è®Šæ•¸
        pm.Deterministic('rho_symmetric', rho_base + rho_symmetry_effect)
        pm.Deterministic('rho_asymmetric', rho_base)
        pm.Deterministic('rho_difference', rho_symmetry_effect)
    
    return model

# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¿«é€Ÿçµæœåˆ†æ
# ============================================================================

def analyze_fast_results(trace, data):
    """
    å¿«é€Ÿåˆ†ææ¨¡å‹çµæœ
    """
    
    print("\n" + "="*50)
    print("ğŸ“Š å”æ–¹å·®çŸ©é™£LBAæ¨¡å‹çµæœï¼ˆå¿«é€Ÿç‰ˆï¼‰")
    print("="*50)
    
    # æå–å¾Œé©—æ¨£æœ¬
    posterior = trace.posterior
    
    # å”æ–¹å·®åˆ†æ
    print(f"\nğŸ“ˆ å”æ–¹å·®çŸ©é™£åˆ†æ:")
    
    rho_base = posterior['rho_base'].values.flatten()
    rho_symmetry_effect = posterior['rho_symmetry_effect'].values.flatten()
    rho_symmetric = posterior['rho_symmetric'].values.flatten()
    rho_asymmetric = posterior['rho_asymmetric'].values.flatten()
    
    print(f"åŸºç¤ç›¸é—œä¿‚æ•¸: {np.mean(rho_base):.3f} [{np.percentile(rho_base, 2.5):.3f}, {np.percentile(rho_base, 97.5):.3f}]")
    print(f"å°ç¨±æ€§æ•ˆæ‡‰: {np.mean(rho_symmetry_effect):.3f} [{np.percentile(rho_symmetry_effect, 2.5):.3f}, {np.percentile(rho_symmetry_effect, 97.5):.3f}]")
    print(f"å°ç¨±åˆºæ¿€ç›¸é—œä¿‚æ•¸: {np.mean(rho_symmetric):.3f}")
    print(f"éå°ç¨±åˆºæ¿€ç›¸é—œä¿‚æ•¸: {np.mean(rho_asymmetric):.3f}")
    
    # é¡¯è‘—æ€§æ¸¬è©¦
    rho_diff_significant = not (np.percentile(rho_symmetry_effect, 2.5) < 0 < np.percentile(rho_symmetry_effect, 97.5))
    print(f"å°ç¨±æ€§æ•ˆæ‡‰é¡¯è‘—æ€§: {'æ˜¯' if rho_diff_significant else 'å¦'}")
    
    # ç¨ç«‹æ€§å‡è¨­æª¢é©—
    independence_prob = np.mean(np.abs(rho_base) < 0.1)
    print(f"\nğŸ”¬ GRTç¨ç«‹æ€§å‡è¨­:")
    print(f"åŸºç¤ç¨ç«‹æ€§æ©Ÿç‡: {independence_prob:.3f}")
    
    if independence_prob < 0.05:
        print("ğŸš¨ å¼·çƒˆé•åGRTç¨ç«‹æ€§å‡è¨­")
    elif independence_prob < 0.2:
        print("âš ï¸  ä¸­ç­‰ç¨‹åº¦é•åç¨ç«‹æ€§å‡è¨­")
    else:
        print("âœ… åŸºæœ¬æ”¯æŒç¨ç«‹æ€§å‡è¨­")
    
    # ç†è«–è§£é‡‹
    print(f"\nğŸ’¡ ç†è«–è§£é‡‹:")
    model_rho_diff = np.mean(rho_symmetry_effect)
    
    if abs(model_rho_diff) > 0.05:
        if model_rho_diff > 0:
            print("âœ“ å°ç¨±åˆºæ¿€å¢åŠ sourceé–“ç›¸é—œæ€§")
            print("  â†’ æ”¯æŒé…ç½®æ€§è™•ç†å‡è¨­")
        else:
            print("âœ“ å°ç¨±åˆºæ¿€æ¸›å°‘sourceé–“ç›¸é—œæ€§")
            print("  â†’ æ”¯æŒç¨ç«‹è™•ç†å‡è¨­")
    else:
        print("â€¢ å°ç¨±æ€§å°ç›¸é—œæ€§å½±éŸ¿è¼ƒå°")
    
    return {
        'rho_base': rho_base,
        'rho_symmetry_effect': rho_symmetry_effect,
        'rho_symmetric': rho_symmetric,
        'rho_asymmetric': rho_asymmetric,
        'significant_symmetry_effect': rho_diff_significant,
        'independence_violation': independence_prob < 0.2
    }

# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šå¿«é€Ÿè¦–è¦ºåŒ–
# ============================================================================

def create_fast_visualization(trace, results, data):
    """
    å‰µå»ºå¿«é€Ÿçµæœè¦–è¦ºåŒ–
    """
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # 1. ç›¸é—œä¿‚æ•¸æ¯”è¼ƒ
    axes[0, 0].hist(results['rho_symmetric'], bins=20, alpha=0.6, label='å°ç¨±', color='green', density=True)
    axes[0, 0].hist(results['rho_asymmetric'], bins=20, alpha=0.6, label='éå°ç¨±', color='orange', density=True)
    axes[0, 0].axvline(0, color='red', linestyle='--', alpha=0.7, label='ç¨ç«‹æ€§')
    axes[0, 0].set_xlabel('ç›¸é—œä¿‚æ•¸ Ï')
    axes[0, 0].set_ylabel('å¯†åº¦')
    axes[0, 0].set_title('ç›¸é—œä¿‚æ•¸å¾Œé©—åˆ†ä½ˆ')
    axes[0, 0].legend()
    
    # 2. å°ç¨±æ€§æ•ˆæ‡‰
    axes[0, 1].hist(results['rho_symmetry_effect'], bins=20, alpha=0.7, color='purple', density=True)
    axes[0, 1].axvline(0, color='red', linestyle='--', alpha=0.7)
    axes[0, 1].axvline(np.mean(results['rho_symmetry_effect']), color='black', linestyle='-', label='å¹³å‡')
    axes[0, 1].set_xlabel('ç›¸é—œä¿‚æ•¸å·®ç•°')
    axes[0, 1].set_ylabel('å¯†åº¦')
    axes[0, 1].set_title('å°ç¨±æ€§æ•ˆæ‡‰')
    axes[0, 1].legend()
    
    # 3. è¡Œç‚ºè³‡æ–™å°æ¯”
    symmetric_data = data[data['is_symmetric'] == 1]
    asymmetric_data = data[data['is_symmetric'] == 0]
    
    if len(symmetric_data) > 0 and len(asymmetric_data) > 0:
        categories = ['æº–ç¢ºç‡', 'åæ‡‰æ™‚é–“']
        sym_values = [symmetric_data['accuracy'].mean(), symmetric_data['rt'].mean()]
        asym_values = [asymmetric_data['accuracy'].mean(), asymmetric_data['rt'].mean()]
        
        x = np.arange(len(categories))
        width = 0.35
        
        axes[1, 0].bar(x - width/2, sym_values, width, label='å°ç¨±', color='green', alpha=0.7)
        axes[1, 0].bar(x + width/2, asym_values, width, label='éå°ç¨±', color='orange', alpha=0.7)
        axes[1, 0].set_ylabel('å€¼')
        axes[1, 0].set_title('è¡Œç‚ºè³‡æ–™å°æ¯”')
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels(categories)
        axes[1, 0].legend()
    
    # 4. ç¸½çµ
    axes[1, 1].text(0.1, 0.9, 'æ¨¡å‹ç¸½çµ', fontsize=14, fontweight='bold', transform=axes[1, 1].transAxes)
    
    summary_text = f"""
ç¨ç«‹æ€§é•å: {'æ˜¯' if results['independence_violation'] else 'å¦'}
å°ç¨±æ€§æ•ˆæ‡‰: {'é¡¯è‘—' if results['significant_symmetry_effect'] else 'ä¸é¡¯è‘—'}

åŸºç¤ç›¸é—œ: {np.mean(results['rho_base']):.3f}
å°ç¨±å¢å¼·: {np.mean(results['rho_symmetry_effect']):.3f}

è§£é‡‹: {'é…ç½®æ€§è™•ç†' if np.mean(results['rho_symmetry_effect']) > 0.05 else 'ç¨ç«‹è™•ç†' if np.mean(results['rho_symmetry_effect']) < -0.05 else 'å½±éŸ¿è¼ƒå°'}
"""
    
    axes[1, 1].text(0.1, 0.7, summary_text, fontsize=10, transform=axes[1, 1].transAxes,
                    verticalalignment='top', fontfamily='monospace')
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    plt.savefig('fast_covariance_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("ğŸ“Š åˆ†æçµæœå·²å„²å­˜ç‚º 'fast_covariance_analysis.png'")

# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šå¿«é€ŸåŸ·è¡Œå‡½æ•¸
# ============================================================================

def run_fast_covariance_analysis(csv_file_path='GRT_LBA.csv', subject_id=None, max_trials=200):
    """
    åŸ·è¡Œå¿«é€Ÿå”æ–¹å·®çŸ©é™£åˆ†æ
    """
    
    print("ğŸš€ é–‹å§‹å¿«é€Ÿå”æ–¹å·®çŸ©é™£LBAåˆ†æ")
    print("="*60)
    
    # 1. è¼‰å…¥è³‡æ–™
    data = load_real_subject_data(csv_file_path, max_trials_per_subject=max_trials)
    if data is None:
        return None, None, None
    
    # 2. é¸æ“‡å—è©¦è€…
    if subject_id is None:
        if 'subject_id' in data.columns:
            subject_counts = data['subject_id'].value_counts()
            selected_subject = subject_counts.index[0]
            print(f"\nğŸ¯ è‡ªå‹•é¸æ“‡å—è©¦è€… {selected_subject} (è©¦é©—æ•¸: {subject_counts.iloc[0]})")
        else:
            selected_subject = 1
            print(f"ğŸ¯ ä½¿ç”¨é è¨­å—è©¦è€… {selected_subject}")
    else:
        selected_subject = subject_id
        print(f"ğŸ¯ é¸æ“‡å—è©¦è€… {selected_subject}")
    
    # æå–å—è©¦è€…è³‡æ–™
    subject_data = data[data['subject_id'] == selected_subject].copy()
    
    # 3. é™åˆ¶è©¦é©—æ•¸
    if len(subject_data) > max_trials:
        print(f"âš ï¸  é™åˆ¶è©¦é©—æ•¸ç‚º {max_trials}")
        subject_data = subject_data.sample(n=max_trials, random_state=42).reset_index(drop=True)
    
    print(f"\nğŸ“Š åˆ†æè³‡æ–™æ‘˜è¦ï¼š")
    print(f"  å—è©¦è€…: {selected_subject}")
    print(f"  è©¦é©—æ•¸: {len(subject_data)}")
    print(f"  æº–ç¢ºç‡: {subject_data['accuracy'].mean():.3f}")
    print(f"  å¹³å‡RT: {subject_data['rt'].mean():.3f}s")
    
    # 4. å»ºæ§‹æ¨¡å‹
    print(f"\nğŸ”§ å»ºæ§‹å¿«é€Ÿæ¨¡å‹...")
    model = build_fast_covariance_lba_model(subject_data)
    
    # 5. åŸ·è¡Œå¿«é€Ÿæ¡æ¨£
    print("â³ é–‹å§‹å¿«é€ŸMCMCæ¡æ¨£...")
    print("æ¡æ¨£åƒæ•¸ï¼š200 draws + 100 tune, 2 chains")
    
    try:
        with model:
            trace = pm.sample(
                draws=200,
                tune=100,
                chains=2,
                cores=1,  # å–®æ ¸å¿ƒé¿å…ä¸¦è¡Œå•é¡Œ
                target_accept=0.8,
                return_inferencedata=True,
                random_seed=456,
                progressbar=True,
                compute_convergence_checks=False  # è·³éæ”¶æ–‚æª¢æŸ¥åŠ å¿«é€Ÿåº¦
            )
        
        print("âœ… å¿«é€ŸMCMCæ¡æ¨£å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ MCMCæ¡æ¨£å¤±æ•—: {e}")
        print("å˜—è©¦æ›´ç°¡å–®çš„æ¡æ¨£è¨­å®š...")
        
        try:
            with model:
                trace = pm.sample(
                    draws=100,
                    tune=50,
                    chains=1,
                    cores=1,
                    return_inferencedata=True,
                    random_seed=456,
                    progressbar=True,
                    compute_convergence_checks=False
                )
            print("âœ… ç°¡åŒ–æ¡æ¨£å®Œæˆï¼")
        except Exception as e2:
            print(f"âŒ ç°¡åŒ–æ¡æ¨£ä¹Ÿå¤±æ•—: {e2}")
            return None, None, None
    
    # 6. åˆ†æçµæœ
    print(f"\nğŸ“ˆ åˆ†æçµæœ...")
    results = analyze_fast_results(trace, subject_data)
    
    # 7. å‰µå»ºè¦–è¦ºåŒ–
    print(f"\nğŸ¨ ç”Ÿæˆè¦–è¦ºåŒ–...")
    create_fast_visualization(trace, results, subject_data)
    
    print(f"\nğŸ‰ å¿«é€Ÿå”æ–¹å·®çŸ©é™£LBAåˆ†æå®Œæˆï¼")
    
    return trace, subject_data, model

# ============================================================================
# å¿«é€Ÿæ¸¬è©¦å‡½æ•¸
# ============================================================================

def quick_test():
    """
    å¿«é€Ÿæ¸¬è©¦å‡½æ•¸
    """
    print("ğŸ§ª åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦...")
    return run_fast_covariance_analysis(max_trials=100)

# ============================================================================
# åŸ·è¡Œåˆ†æ
# ============================================================================

if __name__ == "__main__":
    print("ğŸ”¬ å„ªåŒ–ç‰ˆå”æ–¹å·®çŸ©é™£LBAæ¨¡å‹")
    print("è§£æ±ºæ€§èƒ½å•é¡Œå’Œå…¼å®¹æ€§å•é¡Œ")
    print("-" * 60)
    
    # åŸ·è¡Œå¿«é€Ÿåˆ†æ
    trace, data, model = run_fast_covariance_analysis()
    
    if trace is not None:
        print("\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆï¼")
        print("å¦‚æœéœ€è¦æ›´è©³ç´°çš„åˆ†æï¼Œå¯ä»¥å¢åŠ  draws å’Œ tune åƒæ•¸")
    else:
        print("\nâŒ åˆ†æå¤±æ•—")
        print("å˜—è©¦åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦: quick_test()")

# ============================================================================
# ä½¿ç”¨èªªæ˜
# ============================================================================

"""
å¿«é€Ÿä½¿ç”¨æ–¹æ³•ï¼š

1. åŸºæœ¬åŸ·è¡Œï¼š
   trace, data, model = run_fast_covariance_analysis()

2. æŒ‡å®šåƒæ•¸ï¼š
   trace, data, model = run_fast_covariance_analysis(
       csv_file_path='your_file.csv',
       subject_id=1,
       max_trials=150
   )

3. å¿«é€Ÿæ¸¬è©¦ï¼ˆä½¿ç”¨æ¨¡æ“¬è³‡æ–™ï¼‰ï¼š
   trace, data, model = quick_test()

4. å¦‚æœä»ç„¶å¾ˆæ…¢ï¼Œå¯ä»¥é€²ä¸€æ­¥æ¸›å°‘åƒæ•¸ï¼š
   # ä¿®æ”¹ run_fast_covariance_analysis ä¸­çš„æ¡æ¨£åƒæ•¸
   # draws=100, tune=50, chains=1
"""
