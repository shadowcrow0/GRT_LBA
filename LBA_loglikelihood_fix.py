# -*- coding: utf-8 -*-
"""
LBA Log-likelihood ä¿®å¾©å·¥å…·
è§£æ±º LBA_main.py ä¸­ loglikelihood è¨ˆç®—å¤±æ•—çš„å•é¡Œ
"""

import numpy as np
import pymc as pm
import pytensor.tensor as pt
import arviz as az
import warnings
import os
from datetime import datetime

def diagnose_loglikelihood_issue(data_file='model_data.npz'):
    """
    è¨ºæ–· loglikelihood è¨ˆç®—å•é¡Œçš„æ ¹æœ¬åŸå› 
    """
    print("ğŸ” è¨ºæ–· loglikelihood å•é¡Œ...")
    
    issues_found = []
    
    try:
        # 1. æª¢æŸ¥æ•¸æ“šæª”æ¡ˆ
        if not os.path.exists(data_file):
            issues_found.append(f"æ•¸æ“šæª”æ¡ˆä¸å­˜åœ¨: {data_file}")
            return issues_found
        
        data = np.load(data_file, allow_pickle=True)
        observed_value = data['observed_value']
        
        # 2. æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§
        if np.any(np.isnan(observed_value)):
            issues_found.append("æ•¸æ“šåŒ…å« NaN å€¼")
        
        if np.any(observed_value[:, 0] <= 0):
            issues_found.append("ç™¼ç¾éæ­£æ•¸ RT å€¼")
        
        # 3. æª¢æŸ¥ RT å–®ä½
        rt_mean = observed_value[:, 0].mean()
        print(f"å¹³å‡ RT: {rt_mean:.3f}")
        
        if rt_mean < 10:
            issues_found.append("RT å¯èƒ½æ˜¯ç§’å–®ä½ï¼Œéœ€è¦è½‰æ›ç‚ºæ¯«ç§’")
        elif rt_mean > 10000:
            issues_found.append("RT å€¼éå¤§ï¼Œå¯èƒ½æœ‰å–®ä½å•é¡Œ")
        
        # 4. æª¢æŸ¥åæ‡‰ç¯„åœ
        responses = observed_value[:, 1]
        unique_responses = np.unique(responses)
        if not np.array_equal(unique_responses, [0., 1.]) and not np.array_equal(unique_responses, [0]) and not np.array_equal(unique_responses, [1]):
            issues_found.append(f"åæ‡‰å€¼ä¸æ˜¯ 0/1: {unique_responses}")
        
        print(f"æ•¸æ“šæª¢æŸ¥å®Œæˆï¼Œç™¼ç¾ {len(issues_found)} å€‹å•é¡Œ")
        for issue in issues_found:
            print(f"  âŒ {issue}")
        
        if not issues_found:
            print("âœ… æ•¸æ“šæª¢æŸ¥é€šé")
        
        return issues_found
        
    except Exception as e:
        issues_found.append(f"æ•¸æ“šæª¢æŸ¥å¤±æ•—: {e}")
        return issues_found

def create_fixed_lba_logp(value, v_rates, start_var, b_safe, non_decision):
    """
    ä¿®å¾©ç‰ˆ LBA log-likelihood å‡½æ•¸
    è§£æ±ºæ•¸å€¼ç©©å®šæ€§å•é¡Œ
    """
    
    # æå–æ•¸æ“š
    rt = value[:, 0] / 1000.0  # è½‰æ›ç‚ºç§’
    response = value[:, 1].astype(int)
    
    # ç¢ºä¿åƒæ•¸åœ¨åˆç†ç¯„åœå…§
    start_var = pt.clip(start_var, 0.01, 5.0)
    b_safe = pt.clip(b_safe, start_var + 0.01, 10.0)
    non_decision = pt.clip(non_decision, 0.01, 1.0)
    
    # æ±ºç­–æ™‚é–“ï¼ˆå¿…é ˆç‚ºæ­£ï¼‰
    decision_time = pt.maximum(rt - non_decision, 0.001)
    
    # æå–æ¼‚ç§»ç‡
    v_correct = pt.clip(v_rates[:, 0], 0.01, 20.0)
    v_incorrect = pt.clip(v_rates[:, 1], 0.01, 20.0)
    
    # ç°¡åŒ–çš„ LBA ä¼¼ç„¶è¨ˆç®—ï¼ˆä½¿ç”¨ Wald è¿‘ä¼¼ï¼‰
    def wald_logpdf(t, v, A, b):
        """Wald åˆ†å¸ƒçš„å°æ•¸å¯†åº¦å‡½æ•¸"""
        mu = (b - A/2) / pt.maximum(v, 0.01)
        lambda_param = (b - A/2)**2
        
        # Wald PDF: f(t) = sqrt(Î»/(2Ï€tÂ³)) * exp(-Î»(t-Î¼)Â²/(2Î¼Â²t))
        log_pdf = (0.5 * pt.log(lambda_param / (2 * np.pi * t**3)) - 
                   lambda_param * (t - mu)**2 / (2 * mu**2 * t))
        
        return log_pdf
    
    def survival_function(t, v, A, b):
        """ç°¡åŒ–çš„å­˜æ´»å‡½æ•¸"""
        mu = (b - A/2) / pt.maximum(v, 0.01)
        # ä½¿ç”¨æŒ‡æ•¸è¿‘ä¼¼
        survival_logp = -t / mu
        return survival_logp
    
    # è¨ˆç®—ç²å‹è€…çš„æ¦‚ç‡å¯†åº¦
    log_pdf_correct = wald_logpdf(decision_time, v_correct, start_var, b_safe)
    log_pdf_incorrect = wald_logpdf(decision_time, v_incorrect, start_var, b_safe)
    
    # è¨ˆç®—å¤±æ•—è€…çš„å­˜æ´»æ¦‚ç‡
    log_survival_incorrect = survival_function(decision_time, v_incorrect, start_var, b_safe)
    log_survival_correct = survival_function(decision_time, v_correct, start_var, b_safe)
    
    # æ ¹æ“šåæ‡‰é¸æ“‡å°æ‡‰çš„ä¼¼ç„¶
    log_likelihood = pt.switch(
        pt.eq(response, 1),
        log_pdf_correct + log_survival_incorrect,  # æ­£ç¢ºåæ‡‰
        log_pdf_incorrect + log_survival_correct   # éŒ¯èª¤åæ‡‰
    )
    
    # æ•¸å€¼ç©©å®šæ€§ä¿è­·
    log_likelihood = pt.switch(pt.isnan(log_likelihood), -100.0, log_likelihood)
    log_likelihood = pt.switch(pt.isinf(log_likelihood), -100.0, log_likelihood)
    log_likelihood = pt.clip(log_likelihood, -100.0, 10.0)
    
    return log_likelihood

def create_fixed_lba_random(v_rates, start_var, b_safe, non_decision, rng=None, size=None):
    """
    ä¿®å¾©ç‰ˆ LBA éš¨æ©Ÿæ¨£æœ¬ç”Ÿæˆå‡½æ•¸
    """
    if size is None:
        size = (1,)
    elif isinstance(size, int):
        size = (size,)
    
    n_samples = size[0]
    samples = np.zeros((n_samples, 2))
    
    # è½‰æ›åƒæ•¸ç‚ºæ•¸å€¼
    try:
        v_correct = float(np.asarray(v_rates[0]).item())
        v_incorrect = float(np.asarray(v_rates[1]).item())
        A_val = float(np.asarray(start_var).item())
        b_val = float(np.asarray(b_safe).item())
        t0_val = float(np.asarray(non_decision).item())
    except:
        # å¦‚æœç„¡æ³•è½‰æ›ï¼Œä½¿ç”¨é è¨­å€¼
        v_correct = 1.5
        v_incorrect = 0.8
        A_val = 0.5
        b_val = 1.5
        t0_val = 0.3
    
    for i in range(n_samples):
        # ä½¿ç”¨ç°¡åŒ–çš„ LBA æ¨¡æ“¬
        # æ¯å€‹ç´¯ç©å™¨çš„å®Œæˆæ™‚é–“ä½¿ç”¨ Wald åˆ†å¸ƒè¿‘ä¼¼
        mu_correct = (b_val - A_val/2) / max(v_correct, 0.01)
        mu_incorrect = (b_val - A_val/2) / max(v_incorrect, 0.01)
        
        # ä½¿ç”¨æŒ‡æ•¸åˆ†å¸ƒä½œç‚ºç°¡åŒ–
        t_correct = rng.exponential(mu_correct)
        t_incorrect = rng.exponential(mu_incorrect)
        
        if t_correct < t_incorrect:
            samples[i, 0] = (t_correct + t0_val) * 1000  # è½‰å›æ¯«ç§’
            samples[i, 1] = 1
        else:
            samples[i, 0] = (t_incorrect + t0_val) * 1000
            samples[i, 1] = 0
        
        # ç¢ºä¿ RT åœ¨åˆç†ç¯„åœå…§
        samples[i, 0] = np.clip(samples[i, 0], 200, 5000)
    
    return samples

def create_fixed_coactive_model(observed_data, input_data):
    """
    å‰µå»ºä¿®å¾©ç‰ˆ Coactive LBA æ¨¡å‹
    ç¢ºä¿ log_likelihood æ­£ç¢ºç”Ÿæˆ
    """
    with pm.Model() as model:
        
        # ä½¿ç”¨æ›´ä¿å®ˆçš„å…ˆé©—åˆ†å¸ƒ
        v_match = pm.TruncatedNormal('v_match', mu=1.5, sigma=0.5, lower=0.5, upper=4.0)
        v_mismatch = pm.TruncatedNormal('v_mismatch', mu=1.0, sigma=0.5, lower=0.2, upper=3.0)
        
        start_var = pm.TruncatedNormal('start_var', mu=0.5, sigma=0.2, lower=0.1, upper=1.0)
        boundary_offset = pm.TruncatedNormal('boundary_offset', mu=1.0, sigma=0.3, lower=0.5, upper=2.0)
        b_safe = pm.Deterministic('b_safe', start_var + boundary_offset)
        
        # å‹•æ…‹è¨ˆç®— non_decision ä¸Šç•Œ
        min_rt_seconds = np.min(observed_data[:, 0]) / 1000.0
        upper_bound = min(min_rt_seconds * 0.8, 0.5)  # æ›´ä¿å®ˆçš„ä¸Šç•Œ
        
        non_decision = pm.TruncatedNormal('non_decision', 
                                        mu=0.2, sigma=0.05,
                                        lower=0.05, upper=upper_bound)
        
        # è¨ˆç®—æ¼‚ç§»ç‡
        v_left = v_match * input_data['left_match'] + v_mismatch * (1 - input_data['left_match'])
        v_right = v_match * input_data['right_match'] + v_mismatch * (1 - input_data['right_match'])
        
        # Coactive æ•´åˆ
        v_final_correct = pm.Deterministic('v_final_correct', v_left + v_right)
        
        v_incorrect_base = pm.TruncatedNormal('v_incorrect_base', mu=0.8, sigma=0.3, lower=0.2, upper=2.0)
        v_final_incorrect = pm.Deterministic('v_final_incorrect', 
                                           pt.full_like(v_final_correct, v_incorrect_base))
        
        # å †ç–Šæ¼‚ç§»ç‡
        v_rates = pt.stack([v_final_correct, v_final_incorrect], axis=1)
        
        # ä½¿ç”¨ä¿®å¾©ç‰ˆä¼¼ç„¶å‡½æ•¸
        likelihood = pm.CustomDist('likelihood',
                                  v_rates, start_var, b_safe, non_decision,
                                  logp=create_fixed_lba_logp,
                                  random=create_fixed_lba_random,
                                  observed=observed_data)
        
        # æ‰‹å‹•è¨ˆç®— log_likelihood ç¢ºä¿ ArviZ å¯ä»¥è¨ªå•
        individual_logp = create_fixed_lba_logp(observed_data, v_rates, start_var, b_safe, non_decision)
        pm.Deterministic('log_likelihood_values', individual_logp)
    
    return model

def test_fixed_model(data_file='model_data_fixed.npz'):
    """
    æ¸¬è©¦ä¿®å¾©ç‰ˆæ¨¡å‹æ˜¯å¦èƒ½æ­£ç¢ºè¨ˆç®— log_likelihood
    """
    print("ğŸ§ª æ¸¬è©¦ä¿®å¾©ç‰ˆæ¨¡å‹...")
    
    try:
        # è¼‰å…¥æ•¸æ“š
        data = np.load(data_file, allow_pickle=True)
        observed_value = data['observed_value']
        participant_idx = data['participant_idx']
        model_input_data = data['model_input_data'].item()
        
        # é¸æ“‡ç¬¬ä¸€å€‹åƒèˆ‡è€…é€²è¡Œæ¸¬è©¦
        participant_id = np.unique(participant_idx)[0]
        mask = participant_idx == participant_id
        
        test_data = observed_value[mask][:30]  # åªå–å‰30å€‹è©¦é©—åŠ å¿«æ¸¬è©¦
        test_input = {
            'left_match': model_input_data['left_match'][mask][:30],
            'right_match': model_input_data['right_match'][mask][:30]
        }
        
        print(f"æ¸¬è©¦æ•¸æ“š: {len(test_data)} è©¦é©—ï¼Œåƒèˆ‡è€… {participant_id}")
        
        # å‰µå»ºæ¨¡å‹
        model = create_fixed_coactive_model(test_data, test_input)
        print("âœ“ æ¨¡å‹å‰µå»ºæˆåŠŸ")
        
        # æ¸¬è©¦æ¨¡å‹ç·¨è­¯
        with model:
            test_point = model.initial_point()
            logp = model.compile_logp()
            logp_val = logp(test_point)
            
            if np.isnan(logp_val) or np.isinf(logp_val):
                print(f"âŒ åˆå§‹ logp ç„¡æ•ˆ: {logp_val}")
                return False
            
            print(f"âœ“ æ¨¡å‹ç·¨è­¯æˆåŠŸï¼Œåˆå§‹ logp: {logp_val:.2f}")
        
        # å¿«é€Ÿæ¡æ¨£æ¸¬è©¦
        print("é€²è¡Œå¿«é€Ÿæ¡æ¨£æ¸¬è©¦...")
        with model:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                
                trace = pm.sample(
                    draws=100,
                    tune=100,
                    chains=2,
                    target_accept=0.90,
                    return_inferencedata=True,
                    progressbar=False,
                    random_seed=42,
                    cores=1
                )
        
        print("âœ“ æ¡æ¨£æˆåŠŸ")
        
        # æª¢æŸ¥ log_likelihood
        if 'log_likelihood_values' in trace.posterior:
            ll_values = trace.posterior['log_likelihood_values'].values
            if not np.any(np.isnan(ll_values)) and not np.any(np.isinf(ll_values)):
                print("âœ“ log_likelihood è¨ˆç®—æ­£å¸¸")
                
                # æ¸¬è©¦ WAIC è¨ˆç®—
                try:
                    # å‰µå»ºå‡çš„ log_likelihood ç”¨æ–¼ WAIC
                    import xarray as xr
                    log_likelihood = xr.Dataset({
                        'likelihood': trace.posterior['log_likelihood_values']
                    })
                    trace_with_ll = trace.assign(log_likelihood=log_likelihood)
                    
                    waic_result = az.waic(trace_with_ll)
                    print(f"âœ“ WAIC è¨ˆç®—æˆåŠŸ: {waic_result.elpd_waic:.2f}")
                    
                    return True
                except Exception as e:
                    print(f"âš ï¸ WAIC è¨ˆç®—å¤±æ•—: {e}")
                    return True  # æ¨¡å‹æœ¬èº«æ˜¯å¥½çš„
            else:
                print("âŒ log_likelihood åŒ…å«ç„¡æ•ˆå€¼")
                return False
        else:
            print("âŒ ç¼ºå°‘ log_likelihood_values")
            return False
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        return False

def fix_lba_main():
    """
    ç‚º LBA_main.py æä¾›ä¿®å¾©å»ºè­°
    """
    
    suggestions = """
    
    ğŸ“‹ ä¿®å¾© LBA_main.py çš„å»ºè­°ï¼š
    
    1. åœ¨æ–‡ä»¶é–‹é ­æ·»åŠ å°å…¥ï¼š
       from LBA_loglikelihood_fix import create_fixed_coactive_model, diagnose_loglikelihood_issue
    
    2. åœ¨ run_single_participant_analysis ä¸­æ›¿æ›æ¨¡å‹å‰µå»ºï¼š
       
       åŸä¾†ï¼š
       model = create_model_by_name(model_name, participant_data, participant_input)
       
       æ”¹ç‚ºï¼š
       if model_name == 'Coactive_Addition':
           model = create_fixed_coactive_model(participant_data, participant_input)
       else:
           model = create_model_by_name(model_name, participant_data, participant_input)
    
    3. åœ¨é–‹å§‹åˆ†æå‰æ·»åŠ è¨ºæ–·ï¼š
       issues = diagnose_loglikelihood_issue(self.data_file)
       if issues:
           print("ç™¼ç¾æ•¸æ“šå•é¡Œï¼Œå˜—è©¦ä½¿ç”¨ä¿®å¾©ç‰ˆæ•¸æ“š...")
           self.data_file = 'model_data_fixed.npz'
    
    4. èª¿æ•´æ¡æ¨£åƒæ•¸ä»¥æé«˜ç©©å®šæ€§ï¼š
       trace, diagnostics = sample_with_convergence_check(
           model, 
           max_attempts=3,  # å¢åŠ å˜—è©¦æ¬¡æ•¸
           draws=500,       # æ¸›å°‘ draws ä½†å¢åŠ ç©©å®šæ€§
           tune=500,        # å¢åŠ  tune
           chains=2         # æ¸›å°‘ chains
       )
    
    """
    
    print(suggestions)
    return suggestions

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    
    print("ğŸ”§ LBA Log-likelihood ä¿®å¾©å·¥å…·")
    print("=" * 50)
    
    # 1. è¨ºæ–·å•é¡Œ
    issues = diagnose_loglikelihood_issue()
    
    # 2. æ¸¬è©¦ä¿®å¾©ç‰ˆæ¨¡å‹
    if os.path.exists('model_data_fixed.npz'):
        success = test_fixed_model('model_data_fixed.npz')
    else:
        success = test_fixed_model('model_data.npz')
    
    # 3. æä¾›ä¿®å¾©å»ºè­°
    if success:
        print("\nâœ… ä¿®å¾©æ¸¬è©¦æˆåŠŸï¼")
        print("ä½ ç¾åœ¨å¯ä»¥ä½¿ç”¨ä¿®å¾©ç‰ˆå‡½æ•¸ä¾†è§£æ±º loglikelihood å•é¡Œã€‚")
        fix_lba_main()
    else:
        print("\nâŒ ä¿®å¾©æ¸¬è©¦å¤±æ•—")
        print("éœ€è¦é€²ä¸€æ­¥èª¿è©¦æ•¸æ“šæˆ–æ¨¡å‹è¨­ç½®ã€‚")
    
    return success

if __name__ == '__main__':
    main()