# -*- coding: utf-8 -*-
"""
Created on Sun Jun 29 04:07:28 2025

@author: spt904
"""

# LBA å…·é«”ä¿®å¾©æŒ‡å— - é€æ­¥æ›¿æ›èªªæ˜
# ===================================================================

import numpy as np
import pymc as pm
import pytensor.tensor as pt
import arviz as az
import time

# ===================================================================
# ä¿®å¾© 1: æ•¸æ“šå–®ä½æª¢æŸ¥å’Œä¿®å¾©
# ===================================================================

def fix_data_units(data_file='model_data.npz'):
    """ä¿®å¾©æ•¸æ“šå–®ä½å•é¡Œ - æª¢æŸ¥ RT æ˜¯å¦éœ€è¦è½‰æ›ç‚ºæ¯«ç§’"""
    
    print("æª¢æŸ¥æ•¸æ“šå–®ä½...")
    data = np.load(data_file, allow_pickle=True)
    observed_value = data['observed_value'].copy()
    
    # æª¢æŸ¥ RT å€¼ç¯„åœ
    rt_values = observed_value[:, 0]
    mean_rt = rt_values.mean()
    
    print(f"ç•¶å‰ RT å¹³å‡å€¼: {mean_rt:.3f}")
    
    if mean_rt < 10:  # å¯èƒ½æ˜¯ç§’
        print("æª¢æ¸¬åˆ° RT æ˜¯ç§’å–®ä½ï¼Œè½‰æ›ç‚ºæ¯«ç§’...")
        observed_value[:, 0] *= 1000
        
        # ä¿å­˜ä¿®æ­£å¾Œçš„æ•¸æ“š
        np.savez('model_data_fixed.npz', 
                observed_value=observed_value,
                participant_idx=data['participant_idx'],
                model_input_data=data['model_input_data'])
        print("âœ“ ä¿®æ­£å¾Œçš„æ•¸æ“šå·²ä¿å­˜ç‚º model_data_fixed.npz")
        return 'model_data_fixed.npz'
    else:
        print("RT å–®ä½æ­£ç¢ºï¼Œç„¡éœ€è½‰æ›")
        return data_file

# ===================================================================
# ä¿®å¾© 2: æ›¿æ› lba_models.py ä¸­çš„æ ¸å¿ƒå‡½æ•¸
# ===================================================================
def lba_logp_correct(value, v, A, b, t0, s=1.0):
    """
    åˆ†æä¸Šæ­£ç¢ºçš„ LBA å°æ•¸æ¦‚ä¼¼å‡½æ•¸ (é©ç”¨æ–¼å…©å€‹ç´¯ç©å™¨)ã€‚
    
    åƒæ•¸:
    value (Tensor): è§€æ¸¬æ•¸æ“š (rt, response), shape=(n_trials, 2)
    v (Tensor): æ¼‚ç§»ç‡å‘é‡ [v_correct, v_incorrect], shape=(n_trials, 2)
    A (Tensor): æœ€å¤§èµ·å§‹é» variability, shape=(n_trials,)
    b (Tensor): åæ‡‰é–¾å€¼, shape=(n_trials,)
    t0 (Tensor): éæ±ºç­–æ™‚é–“, shape=(n_trials,)
    s (float): æ¼‚ç§»ç‡æ¨™æº–å·® (é€šå¸¸å›ºå®šç‚º 1.0)
    """
    
    # ------------------------------------------------------------------
    # 1. å®šç¾© PyTensor/Aesara ä¸­çš„å¸¸æ…‹ PDF (phi) å’Œ CDF (Phi)
    # ------------------------------------------------------------------
    def phi(x):
        """å¸¸æ…‹åˆ†ä½ˆæ©Ÿç‡å¯†åº¦å‡½æ•¸ (PDF)"""
        return (1.0 / pt.sqrt(2.0 * np.pi)) * pt.exp(-0.5 * x**2)

    def Phi(x):
        """å¸¸æ…‹åˆ†ä½ˆç´¯ç©åˆ†å¸ƒå‡½æ•¸ (CDF)"""
        return 0.5 * (1.0 + pt.erf(x / pt.sqrt(2.0)))

    # ------------------------------------------------------------------
    # 2. ç‚ºå–®ä¸€ LBA ç´¯ç©å™¨è¨ˆç®— PDF g(t) å’Œå­˜æ´»å‡½æ•¸ S(t)
    #    é€™æ˜¯æ ¹æ“š Brown & Heathcote (2008) çš„è«–æ–‡
    # ------------------------------------------------------------------
    def get_pdf_and_survival(t, v_i, b, A, s):
        """è¨ˆç®—å–®ä¸€ç´¯ç©å™¨çš„ g(t) å’Œ S(t)"""
        
        # ç‚ºæ•¸å€¼ç©©å®šæ€§ï¼Œé¿å…é™¤ä»¥é›¶
        t_safe = pt.maximum(t, 1e-6)
        
        # è¨ˆç®—å››å€‹ä¸»è¦é …
        term1_den = s * t_safe
        term1_val = (b - A - t_safe * v_i) / term1_den
        
        term2_den = s * t_safe
        term2_val = (b - t_safe * v_i) / term2_den
        
        term3_den = A
        term3_val = v_i * (Phi(term2_val) - Phi(term1_val))
        
        term4_den = A
        term4_val = (s * t_safe) * (phi(term1_val) - phi(term2_val))

        # è¨ˆç®— PDF: g(t)
        pdf = (1.0 / A) * (-v_i * Phi(term1_val) + \
                           v_i * Phi(term2_val) + \
                           s * phi(term1_val) - \
                           s * phi(term2_val))
                           
        # è¨ˆç®— CDF: G(t)
        cdf = 1.0 + ((v_i * t_safe - b) / A) * Phi(term2_val) - \
              (((v_i * t_safe) - (b - A)) / A) * Phi(term1_val) + \
              (t_safe * s / A) * phi(term2_val) - \
              (t_safe * s / A) * phi(term1_val)

        # å­˜æ´»å‡½æ•¸ S(t) = 1 - G(t)
        survival = 1.0 - cdf

        # å†æ¬¡ç¢ºä¿æ•¸å€¼ç©©å®šæ€§
        pdf_stable = pt.maximum(pdf, 1e-8)
        survival_stable = pt.maximum(survival, 1e-8)
        
        return pdf_stable, survival_stable

    # ------------------------------------------------------------------
    # 3. ä¸»å‡½æ•¸é‚è¼¯
    # ------------------------------------------------------------------
    rt = value[:, 0] / 1000.0  # å¾æ¯«ç§’è½‰æ›ç‚ºç§’
    response = value[:, 1]    # 1=correct, 0=incorrect
    
    # æ±ºç­–æ™‚é–“ (å¿…é ˆç‚ºæ­£)
    decision_time = rt - t0
    
    # æ ¹æ“šåæ‡‰é¸æ“‡æ­£ç¢ºå’ŒéŒ¯èª¤çš„æ¼‚ç§»ç‡
    v_correct = v[:, 0]
    v_incorrect = v[:, 1]
    
    # è¨ˆç®—ç²å‹å’Œè½æ•—ç´¯ç©å™¨çš„ PDF/Survival
    g_correct, S_correct = get_pdf_and_survival(decision_time, v_correct, b, A, s)
    g_incorrect, S_incorrect = get_pdf_and_survival(decision_time, v_incorrect, b, A, s)
    
    # ç¸½ä¼¼ç„¶æ˜¯å…©å€‹ç¨ç«‹äº‹ä»¶çš„çµ„åˆï¼š
    # 1. å°æ–¼æ­£ç¢ºåæ‡‰ (response=1): ç²å‹è€…(correct)åœ¨ t æ™‚åˆ»å®Œæˆï¼Œè½æ•—è€…(incorrect)åœ¨ t æ™‚åˆ»å°šæœªå®Œæˆ
    #    likelihood = g_correct(t) * S_incorrect(t)
    # 2. å°æ–¼éŒ¯èª¤åæ‡‰ (response=0): ç²å‹è€…(incorrect)åœ¨ t æ™‚åˆ»å®Œæˆï¼Œè½æ•—è€…(correct)åœ¨ t æ™‚åˆ»å°šæœªå®Œæˆ
    #    likelihood = g_incorrect(t) * S_correct(t)
    
    # ä½¿ç”¨ pt.switch æ ¹æ“š response çš„å€¼é¸æ“‡å°æ‡‰çš„ä¼¼ç„¶
    likelihood = pt.switch(
        pt.eq(response, 1),
        g_correct * S_incorrect,
        g_incorrect * S_correct
    )
    
    # æœ€çµ‚çš„å°æ•¸æ¦‚ä¼¼
    logp = pt.log(pt.maximum(likelihood, 1e-8))
    
    # æœ€å¾Œçš„ä¿è­·ï¼Œé˜²æ­¢ä»»ä½• NaN æˆ– inf å€¼
    return pt.switch(pt.or_(pt.isnan(logp), pt.isinf(logp)), -100.0, logp)
def safe_lba_logp(value, v1, v2, A, b, t0, s=1.0):
    """
    æ›¿æ› lba_models.py ä¸­çš„ lba_logp å‡½æ•¸
    æ›´ç©©å®šçš„æ•¸å€¼è¨ˆç®—
    """
    
    # æå–æ•¸æ“š
    rt = value[:, 0] / 1000.0  # è½‰æ›ç‚ºç§’
    response = value[:, 1]
    
    # åƒæ•¸å®‰å…¨æ€§é™åˆ¶
    v1 = pt.clip(v1, 0.01, 20.0)
    v2 = pt.clip(v2, 0.01, 20.0)
    A = pt.clip(A, 0.01, 5.0)
    b = pt.clip(b, A + 0.01, 10.0)
    t0 = pt.clip(t0, 0.001, 2.0)
    
    # æ±ºç­–æ™‚é–“
    decision_time = pt.maximum(rt - t0, 0.001)
    
    # é¸æ“‡å°æ‡‰çš„æ¼‚ç§»ç‡
    v_chosen = pt.switch(pt.eq(response, 1), v1, v2)
    v_other = pt.switch(pt.eq(response, 1), v2, v1)
    
    # ç°¡åŒ–çš„ä¼¼ç„¶è¨ˆç®— (ä½¿ç”¨ Wald è¿‘ä¼¼)
    mean_time_chosen = (b - A/2) / pt.maximum(v_chosen, 0.01)
    
    # Wald åˆ†å¸ƒå°æ•¸å¯†åº¦
    log_pdf_winner = (-0.5 * pt.log(2 * np.pi * decision_time**3) - 
                      0.5 * (decision_time - mean_time_chosen)**2 / 
                      (decision_time * mean_time_chosen**2))
    
    # å­˜æ´»å‡½æ•¸ (ç°¡åŒ–)
    mean_time_other = (b - A/2) / pt.maximum(v_other, 0.01)
    survival_prob = pt.exp(-decision_time / mean_time_other)
    log_survival_loser = pt.log(pt.maximum(survival_prob, 1e-10))
    
    # è¯åˆå°æ•¸ä¼¼ç„¶
    logp = log_pdf_winner + log_survival_loser
    
    # æ•¸å€¼ç©©å®šæ€§
    logp = pt.switch(pt.isnan(logp), -100.0, logp)
    logp = pt.switch(pt.isinf(logp), -100.0, logp)
    logp = pt.clip(logp, -100.0, 10.0)
    
    return logp

def safe_lba_random(v1, v2, A, b, t0, s=1.0, rng=None, size=None):
    """
    æ›¿æ› lba_models.py ä¸­çš„ lba_random å‡½æ•¸
    """
    if size is None:
        size = (1,)
    elif isinstance(size, int):
        size = (size,)
    
    n_samples = size[0]
    samples = np.zeros((n_samples, 2))
    
    # è½‰æ›åƒæ•¸
    v1_val = float(np.asarray(v1).item())
    v2_val = float(np.asarray(v2).item())
    A_val = float(np.asarray(A).item())
    b_val = float(np.asarray(b).item())
    t0_val = float(np.asarray(t0).item())
    
    for i in range(n_samples):
        # ç°¡åŒ–æ¨¡æ“¬
        mean_t1 = (b_val - A_val/2) / max(v1_val, 0.01)
        mean_t2 = (b_val - A_val/2) / max(v2_val, 0.01)
        
        # ä½¿ç”¨æŒ‡æ•¸åˆ†å¸ƒè¿‘ä¼¼
        t1 = rng.exponential(mean_t1)
        t2 = rng.exponential(mean_t2)
        
        if t1 < t2:
            samples[i, 0] = (t1 + t0_val) * 1000  # è½‰å›æ¯«ç§’
            samples[i, 1] = 1
        else:
            samples[i, 0] = (t2 + t0_val) * 1000
            samples[i, 1] = 0
        
        samples[i, 0] = np.clip(samples[i, 0], 200, 5000)
    
    return samples

def create_robust_coactive_lba_model(observed_data, input_data):
    """
    æ›¿æ› lba_models.py ä¸­çš„ create_coactive_lba_model å‡½æ•¸
    ä½¿ç”¨æ›´ä¿å®ˆçš„å…ˆé©—åˆ†å¸ƒ
    """
    with pm.Model() as model:
        
        # æ›´ä¿å®ˆçš„å…ˆé©—åˆ†å¸ƒ
        v_match = pm.TruncatedNormal('v_match', 
                                   mu=1.5, sigma=0.5, 
                                   lower=0.5, upper=5.0)
        
        v_mismatch = pm.TruncatedNormal('v_mismatch', 
                                      mu=1.0, sigma=0.5, 
                                      lower=0.1, upper=3.0)
        
        # LBA åƒæ•¸ (ä½¿ç”¨æ›´åˆç†çš„ç¯„åœ)
        start_var = pm.TruncatedNormal('start_var', 
                                     mu=0.5, sigma=0.2, 
                                     lower=0.1, upper=1.5)
        
        boundary_offset = pm.TruncatedNormal('boundary_offset', 
                                           mu=1.0, sigma=0.3, 
                                           lower=0.3, upper=2.0)
        
        b_safe = pm.Deterministic('b_safe', start_var + boundary_offset)
        
        non_decision = pm.TruncatedNormal('non_decision', 
                                        mu=0.3, sigma=0.1, 
                                        lower=0.1, upper=0.8)
        
        # è¨ˆç®—æ¼‚ç§»ç‡
        v_left = (v_match * input_data['left_match'] + 
                  v_mismatch * (1 - input_data['left_match']))
        v_right = (v_match * input_data['right_match'] + 
                   v_mismatch * (1 - input_data['right_match']))
        
        # Coactive æ•´åˆ
        v_final_correct = pm.Deterministic('v_final_correct', v_left + v_right)
        
        v_final_incorrect = pm.TruncatedNormal('v_final_incorrect', 
                                             mu=0.5, sigma=0.3, 
                                             lower=0.1, upper=2.0)
        
        # ä½¿ç”¨å®‰å…¨çš„ä¼¼ç„¶å‡½æ•¸
        likelihood = pm.CustomDist('likelihood',
                                  v_final_correct, v_final_incorrect, 
                                  start_var, b_safe, non_decision,
                                  logp=safe_lba_logp,
                                  random=safe_lba_random,
                                  observed=observed_data)
    
    return model

# ===================================================================
# ä¿®å¾© 3: æ›¿æ› LBA tool.py ä¸­çš„æ¡æ¨£å‡½æ•¸
# ===================================================================

def robust_sample_with_convergence_check(model, max_attempts=2, 
                                        initial_draws=100, initial_tune=200):
    """
    Replaces sample_with_convergence_check in LBA_tool.py.
    Uses a more robust initialization strategy.
    """
    
    draws = initial_draws
    tune = initial_tune
    
    print(f"  é–‹å§‹ç©©å¥æ¡æ¨£ (draws={draws}, tune={tune})...")
    
    for attempt in range(max_attempts):
        try:
            print(f"    å˜—è©¦ {attempt + 1}/{max_attempts}")
            
            with model:
                # Attempt to find a good starting point using MAP estimation
                start_map = None
                try:
                    print("    å°‹æ‰¾ MAP ä¼°è¨ˆ...")
                    start_map = pm.find_MAP(method='BFGS', maxeval=5000) # Increased maxeval
                    print("    âœ“ MAP ä¼°è¨ˆæˆåŠŸ")
                except Exception as map_error:
                    print(f"    âš ï¸ MAP ä¼°è¨ˆå¤±æ•—: {map_error}. Using default initialization.")
                
                # Execute sampling with a more robust init strategy
                trace = pm.sample(
                    draws=draws,
                    tune=tune,
                    chains=2,
                    target_accept=0.95,
                    return_inferencedata=True,
                    progressbar=True,
                    random_seed=42 + attempt,
                    start=start_map,
                    init='jitter+adapt_diag'  # MORE ROBUST INITIALIZATION
                )
                
                # Simple convergence check
                try:
                    summary = az.summary(trace)
                    max_rhat = summary['r_hat'].max() if 'r_hat' in summary.columns else 1.0
                    min_ess = summary['ess_bulk'].min() if 'ess_bulk' in summary.columns else 100
                    
                    print(f"    æœ€å¤§ R-hat: {max_rhat:.4f}")
                    print(f"    æœ€å° ESS: {min_ess:.0f}")
                    
                    if max_rhat < 1.2 and min_ess > 50:
                        print(f"    âœ“ æ”¶æ–‚æˆåŠŸ")
                        return trace, {'max_rhat': max_rhat, 'min_ess': min_ess}
                    
                except Exception as e:
                    print(f"    âš ï¸ æ”¶æ–‚æª¢æŸ¥å¤±æ•—ï¼Œä½†æ¡æ¨£å®Œæˆ: {e}")
                    if trace is not None:
                        return trace, {'max_rhat': np.nan, 'min_ess': np.nan}
                
        except Exception as e:
            print(f"    âŒ æ¡æ¨£å¤±æ•—: {e}")
        
        # Adjust parameters for the next attempt
        if attempt < max_attempts - 1:
            draws = max(50, int(draws * 0.8))
            tune = max(50, int(tune * 0.8))
            print(f"    èª¿æ•´åƒæ•¸: draws={draws}, tune={tune}")
    
    print(f"    âŒ {max_attempts} æ¬¡å˜—è©¦å¾Œä»æœªæˆåŠŸ")
    return None, None
# ===================================================================
# ä¿®å¾© 4: å¿«é€Ÿæ¸¬è©¦å‡½æ•¸ (IMPROVED)
# ===================================================================

def quick_test():
    """å¿«é€Ÿæ¸¬è©¦æ‰€æœ‰ä¿®å¾©æ˜¯å¦æœ‰æ•ˆ"""
    
    print("ğŸ§ª å¿«é€Ÿæ¸¬è©¦ä¿®å¾©æ•ˆæœ...")
    
    try:
        # 1. ç‚ºæ¸¬è©¦å‰µå»ºä¸€å€‹è‡ªåŒ…å«çš„è™›æ“¬æ•¸æ“šæ–‡ä»¶
        n_trials = 30 # A slightly larger test set
        test_data = np.zeros((n_trials, 2))
        test_data[:, 0] = np.random.uniform(300, 1500, n_trials)
        test_data[:, 1] = np.random.binomial(1, 0.8, n_trials)
        participant_idx = np.zeros(n_trials, dtype=int)
        model_input_data = {
            'left_match': np.random.binomial(1, 0.5, n_trials).astype(float),
            'right_match': np.random.binomial(1, 0.5, n_trials).astype(float)
        }
        np.savez('test_model_data.npz',
                 observed_value=test_data,
                 participant_idx=participant_idx,
                 model_input_data=model_input_data)
        
        # 2. è¼‰å…¥å°æ¨£æœ¬æ¸¬è©¦
        data = np.load('test_model_data.npz', allow_pickle=True)
        observed_value = data['observed_value']
        participant_idx = data['participant_idx']
        model_input_data = data['model_input_data'].item()
        
        mask = participant_idx == 0
        test_data = observed_value[mask]
        test_input = {
            'left_match': model_input_data['left_match'][mask],
            'right_match': model_input_data['right_match'][mask]
        }
        print(f"âœ“ æ¸¬è©¦æ•¸æ“šæº–å‚™å®Œæˆ: {len(test_data)} è©¦é©—")
        
        # 3. æ¸¬è©¦æ¨¡å‹å‰µå»º
        model = create_robust_coactive_lba_model(test_data, test_input)
        print("âœ“ æ¨¡å‹å‰µå»ºæˆåŠŸ")
        
        # 4. æ¸¬è©¦æ¨¡å‹ç·¨è­¯
        with model:
            test_point = model.initial_point()
            logp = model.compile_logp()
            logp_val = logp(test_point)
            if np.isnan(logp_val) or np.isinf(logp_val):
                raise ValueError(f"Initial logp is invalid: {logp_val}, indicating a model setup problem.")
            print(f"âœ“ æ¨¡å‹ç·¨è­¯æˆåŠŸï¼Œlogp: {logp_val:.2f}")
        
        # 5. å¿«é€Ÿæ¡æ¨£æ¸¬è©¦ (Increased tune/draws for better stability)
        print("  Running quick sampling test...")
        trace, diagnostics = robust_sample_with_convergence_check(
            model, max_attempts=1, initial_draws=150, initial_tune=200
        )
        
        if trace is not None:
            print("\nâœ… å¿«é€Ÿæ¸¬è©¦å…¨éƒ¨é€šéï¼")
            return True
        else:
            print("\nâŒ æ¡æ¨£æ¸¬è©¦å¤±æ•—. The model compiled but failed the brief sampling/convergence check.")
            print("   This might be due to the inherent difficulty of the model or stochasticity.")
            print("   The main analysis in LBA_main.py, which uses more samples, may still succeed.")
            return False
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        return False

# ===================================================================
# ä¸»è¦æ›¿æ›èªªæ˜
# ===================================================================

def show_replacement_guide():
    """é¡¯ç¤ºå…·é«”çš„æ›¿æ›æŒ‡å—"""
    
    guide = """
    
    ğŸ“‹ å…·é«”æ›¿æ›æ­¥é©Ÿ:
    
    æ­¥é©Ÿ 1: ä¿å­˜ä¿®å¾©æ–‡ä»¶
    â”œâ”€â”€ å°‡æœ¬ä»£ç¢¼ä¿å­˜ç‚º lba_fixes.py
    
    æ­¥é©Ÿ 2: ä¿®æ”¹ lba_models.py
    â”œâ”€â”€ æ›¿æ› lba_logp â†’ safe_lba_logp
    â”œâ”€â”€ æ›¿æ› lba_random â†’ safe_lba_random  
    â”œâ”€â”€ æ›¿æ› create_coactive_lba_model â†’ create_robust_coactive_lba_model
    â””â”€â”€ åœ¨æ–‡ä»¶é–‹é ­æ·»åŠ : from lba_fixes import safe_lba_logp, safe_lba_random
    
    æ­¥é©Ÿ 3: ä¿®æ”¹ LBA tool.py
    â”œâ”€â”€ æ›¿æ› sample_with_convergence_check â†’ robust_sample_with_convergence_check
    â””â”€â”€ åœ¨æ–‡ä»¶é–‹é ­æ·»åŠ : from lba_fixes import robust_sample_with_convergence_check
    
    æ­¥é©Ÿ 4: ä¿®æ”¹ LBA_main.py  
    â”œâ”€â”€ åœ¨ setup_analysis() é–‹é ­æ·»åŠ æ•¸æ“šä¿®å¾©:
    â”‚   from lba_fixes import fix_data_units
    â”‚   self.data_file = fix_data_units(self.data_file)
    â””â”€â”€ é‹è¡Œå‰å…ˆæ¸¬è©¦: from lba_fixes import quick_test; quick_test()
    
    æ­¥é©Ÿ 5: é‹è¡Œæ¸¬è©¦
    â”œâ”€â”€ python -c "from lba_fixes import quick_test; quick_test()"
    â””â”€â”€ å¦‚æœæ¸¬è©¦é€šéï¼Œå†é‹è¡Œå®Œæ•´åˆ†æ
    
    """
    
    print(guide)
    return guide

if __name__ == '__main__':
    print("ğŸ”§ LBA ä¿®å¾©å·¥å…·")
    show_replacement_guide()
    
    # é‹è¡Œå¿«é€Ÿæ¸¬è©¦
    success = quick_test()
    if success:
        print("\nâœ… ä¿®å¾©æ¸¬è©¦é€šéï¼Œå¯ä»¥æ‡‰ç”¨åˆ°ä¸»ç¨‹å¼ï¼")
    else:
        print("\nâŒ ä¿®å¾©æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦é€²ä¸€æ­¥èª¿è©¦")