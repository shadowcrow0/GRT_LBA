# -*- coding: utf-8 -*-
"""
ä¿®æ­£ç‰ˆå››é¸é … GRT-LBA åˆ†æç¨‹å¼ç¢¼ (PyTensor Softmax ä¿®æ­£)
Fixed Four-Choice GRT-LBA Analysis Code (PyTensor Softmax Fix)

ä¸»è¦ä¿®æ­£ / Main Fixes:
1. ä¿®æ­£ PyTensor softmax å‡½æ•¸èª¿ç”¨å•é¡Œ / Fix PyTensor softmax function call issue
2. ä½¿ç”¨ pm.math.softmax æˆ–æ‰‹å‹•å¯¦ç¾ softmax / Use pm.math.softmax or manual softmax implementation
3. ç°¡åŒ–æ¨¡å‹çµæ§‹é¿å…è¤‡é›œçš„ PyTensor æ“ä½œ / Simplify model structure to avoid complex PyTensor operations
4. è©³ç´°çš„ç¨‹å¼ç¢¼è§£é‡‹ / Detailed code explanations
5. è®Šæ•¸ä¾†æºèˆ‡ç”¨é€”èªªæ˜ / Variable source and purpose explanations
"""

import numpy as np
import pandas as pd
import pymc as pm
import pytensor.tensor as pt
import arviz as az
import scipy.stats as stats
import time
import warnings
from pathlib import Path
from typing import Dict, Optional, List
import os

# é—œé–‰ä¸å¿…è¦çš„è­¦å‘Šè¨Šæ¯ / Suppress unnecessary warnings
warnings.filterwarnings('ignore')

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šLBA ä¼¼ç„¶å‡½æ•¸å¯¦ç¾ (ç´” Python ç‰ˆæœ¬)
# Part 1: LBA Likelihood Function Implementation (Pure Python Version)
# ============================================================================

def compute_lba_likelihood_numpy(rt_data, choice_data, stimloc_data, db1, db2, sp, base_v):
    """
    è¨ˆç®— Linear Ballistic Accumulator (LBA) æ¨¡å‹çš„ä¼¼ç„¶å‡½æ•¸ (NumPy ç‰ˆæœ¬)
    Compute the likelihood function for Linear Ballistic Accumulator (LBA) model (NumPy version)
    
    åƒæ•¸èªªæ˜ / Parameters:
    - rt_data: åæ‡‰æ™‚é–“æ•¸æ“š / Reaction time data (ä¾†è‡ª CSV çš„ RT æ¬„ä½ / from RT column in CSV)
    - choice_data: é¸æ“‡åæ‡‰æ•¸æ“š / Choice response data (ä¾†è‡ª CSV çš„ Response æ¬„ä½ / from Response column in CSV)
    - stimloc_data: åˆºæ¿€ä½ç½®æ•¸æ“š / Stimulus location data (å¾ Stimulus æ¬„ä½è½‰æ› / converted from Stimulus column)
    - db1: X è»¸æ±ºç­–é‚Šç•Œ / X-axis decision boundary (ç¯„åœ 0-1 / range 0-1)
    - db2: Y è»¸æ±ºç­–é‚Šç•Œ / Y-axis decision boundary (ç¯„åœ 0-1 / range 0-1)
    - sp: æ„ŸçŸ¥é›œè¨Šåƒæ•¸ / Perceptual noise parameter (æ­£å€¼ / positive value)
    - base_v: åŸºç¤æ¼‚ç§»ç‡ / Base drift rate (æ­£å€¼ / positive value)
    
    è¿”å›å€¼ / Returns:
    - å°æ•¸ä¼¼ç„¶å€¼ / Log-likelihood value
    """
    try:
        # åƒæ•¸æœ‰æ•ˆæ€§æª¢æŸ¥ / Parameter validity check
        if sp <= 0 or base_v <= 0:
            return -1000.0  # è¿”å›æ¥µå°å€¼æ‡²ç½°ç„¡æ•ˆåƒæ•¸ / return very small value to penalize invalid parameters
        
        # LBA æ¨¡å‹çš„å›ºå®šåƒæ•¸ / Fixed parameters for LBA model
        A = 0.4      # èµ·å§‹é»è®Šç•° / Start point variability
        s = 0.3      # æ¼‚ç§»ç‡è®Šç•° / Drift rate variability  
        t0 = 0.2     # éæ±ºç­–æ™‚é–“ / Non-decision time
        
        # è¨ˆç®—æ±ºç­–é–¾å€¼ / Calculate decision thresholds
        b = A + 0.5  # æ±ºç­–é–¾å€¼ = èµ·å§‹é»è®Šç•° + é–¾å€¼åç§» / decision threshold = start point variability + threshold offset
        thresholds = np.array([b, b, b, b])  # å››å€‹é¸é …çš„ç›¸åŒé–¾å€¼ / same threshold for all four choices
        
        # è¨ˆç®—æ±ºç­–æ™‚é–“ / Calculate decision time
        # å¾ç¸½åæ‡‰æ™‚é–“ä¸­æ¸›å»éæ±ºç­–æ™‚é–“ / subtract non-decision time from total reaction time
        rt_decision = np.maximum(rt_data - t0, 0.001)  # ä¿è­‰æœ€å°å€¼ 0.001 é¿å…æ•¸å­¸éŒ¯èª¤ / ensure minimum 0.001 to avoid mathematical errors
        
        # åˆå§‹åŒ–å°æ•¸ä¼¼ç„¶ç¸½å’Œ / Initialize log-likelihood sum
        loglik_sum = 0.0
        
        # å°æ¯å€‹è©¦é©—è¨ˆç®—ä¼¼ç„¶ / Calculate likelihood for each trial
        for i in range(len(rt_decision)):
            choice_idx = int(choice_data[i])  # ç•¶å‰è©¦é©—çš„é¸æ“‡ç´¢å¼• (0,1,2,3) / current trial's choice index (0,1,2,3)
            
            # æª¢æŸ¥é¸æ“‡æœ‰æ•ˆæ€§ / Check choice validity
            if choice_idx < 0 or choice_idx >= 4:
                continue  # è·³éç„¡æ•ˆé¸æ“‡ / skip invalid choices
                
            rt_trial = rt_decision[i]  # ç•¶å‰è©¦é©—çš„æ±ºç­–æ™‚é–“ / current trial's decision time
            if rt_trial <= 0:
                continue  # è·³éç„¡æ•ˆæ™‚é–“ / skip invalid times
            
            # === GRT (General Recognition Theory) è¨ˆç®—éƒ¨åˆ† ===
            # === GRT (General Recognition Theory) Calculation Section ===
            
            # ç²å–åˆºæ¿€ä½ç½® / Get stimulus location
            x_pos = stimloc_data[i, 0]  # X è»¸ä½ç½® (0 æˆ– 1) / X-axis position (0 or 1)
            y_pos = stimloc_data[i, 1]  # Y è»¸ä½ç½® (0 æˆ– 1) / Y-axis position (0 or 1)
            
            # è¨ˆç®—æ±ºç­–æ©Ÿç‡ / Calculate decision probabilities
            # ä½¿ç”¨ logistic å‡½æ•¸è¨ˆç®—é¸æ“‡æ©Ÿç‡ / use logistic function to calculate choice probabilities
            # db1, db2: æ±ºç­–é‚Šç•Œåƒæ•¸ï¼Œæ§åˆ¶åˆ†é¡é‚Šç•Œä½ç½® / decision boundary parameters, control classification boundary position
            # sp: æ„ŸçŸ¥é›œè¨Šï¼Œæ§åˆ¶æ±ºç­–çš„ç¢ºå®šæ€§ / perceptual noise, controls decision certainty
            p_choose_right_x = 1 / (1 + np.exp(-(x_pos - db1) / sp))
            p_choose_right_y = 1 / (1 + np.exp(-(y_pos - db2) / sp))
            
            # è¨ˆç®—å››é¸é …çš„æ©Ÿç‡ / Calculate probabilities for four choices
            # åŸºæ–¼ 2x2 ç©ºé–“çš„ä½ç½®æ©Ÿç‡çµ„åˆ / based on position probability combinations in 2x2 space
            if choice_idx == 0:      # å·¦ä¸Š (0,0) / top-left (0,0)
                choice_prob = (1 - p_choose_right_x) * (1 - p_choose_right_y)
            elif choice_idx == 1:    # å·¦ä¸‹ (0,1) / bottom-left (0,1)
                choice_prob = (1 - p_choose_right_x) * p_choose_right_y
            elif choice_idx == 2:    # å³ä¸Š (1,0) / top-right (1,0)
                choice_prob = p_choose_right_x * (1 - p_choose_right_y)
            else:                    # å³ä¸‹ (1,1) / bottom-right (1,1)
                choice_prob = p_choose_right_x * p_choose_right_y
            
            # === LBA æ¨¡å‹è¨ˆç®—éƒ¨åˆ† ===
            # === LBA Model Calculation Section ===
            
            # è¨ˆç®—æ¼‚ç§»ç‡ / Calculate drift rates
            # v_chosen: è¢«é¸æ“‡é¸é …çš„æ¼‚ç§»ç‡ / drift rate for chosen option
            # v_others: å…¶ä»–é¸é …çš„æ¼‚ç§»ç‡ / drift rate for other options
            v_chosen = max(choice_prob * base_v, 0.1)  # æœ€å°å€¼ 0.1 é¿å…æ•¸å€¼å•é¡Œ / minimum 0.1 to avoid numerical issues
            v_others = max((1 - choice_prob) * base_v / 3, 0.1)  # å¹³å‡åˆ†é…çµ¦å…¶ä»–ä¸‰å€‹é¸é … / evenly distributed to other three options
            
            # LBA æ¨¡å‹çš„æ ¸å¿ƒè¨ˆç®— / Core calculation of LBA model
            sqrt_rt = np.sqrt(rt_trial)  # æ™‚é–“çš„å¹³æ–¹æ ¹ / square root of time
            
            # è¨ˆç®—ç²å‹ç´¯åŠ å™¨çš„ä¼¼ç„¶ / Calculate likelihood for winning accumulator
            b_win = thresholds[choice_idx]  # ç²å‹é¸é …çš„é–¾å€¼ / threshold for winning option
            
            # æ¨™æº–åŒ–è®Šæ•¸ç”¨æ–¼æ­£æ…‹åˆ†ä½ˆè¨ˆç®— / Standardized variables for normal distribution calculation
            z1 = np.clip((v_chosen * rt_trial - b_win) / sqrt_rt, -6, 6)
            z2 = np.clip((v_chosen * rt_trial - A) / sqrt_rt, -6, 6)
            
            try:
                # è¨ˆç®—ç²å‹è€…çš„ CDF å’Œ PDF / Calculate winner's CDF and PDF
                winner_cdf = stats.norm.cdf(z1) - stats.norm.cdf(z2)
                winner_pdf = (stats.norm.pdf(z1) - stats.norm.pdf(z2)) / sqrt_rt
                winner_lik = max((v_chosen / A) * winner_cdf + winner_pdf / A, 1e-10)
            except:
                winner_lik = 1e-10  # æ•¸å€¼è¨ˆç®—å¤±æ•—æ™‚çš„å‚™ç”¨å€¼ / fallback value when numerical calculation fails
            
            # è¨ˆç®—å¤±æ•—ç´¯åŠ å™¨çš„ç”Ÿå­˜å‡½æ•¸ / Calculate survival function for losing accumulators
            loser_survival = 1.0
            for j in range(3):  # å…¶ä»–ä¸‰å€‹é¸é … / other three options
                b_lose = thresholds[(choice_idx + j + 1) % 4]  # å¤±æ•—é¸é …çš„é–¾å€¼ / threshold for losing option
                z1_lose = np.clip((v_others * rt_trial - b_lose) / sqrt_rt, -6, 6)
                z2_lose = np.clip((v_others * rt_trial - A) / sqrt_rt, -6, 6)
                
                try:
                    loser_cdf = stats.norm.cdf(z1_lose) - stats.norm.cdf(z2_lose)
                    loser_survival *= max(1 - loser_cdf, 1e-6)  # ç”Ÿå­˜æ©Ÿç‡ / survival probability
                except:
                    loser_survival *= 0.5  # å‚™ç”¨å€¼ / fallback value
            
            # è¨ˆç®—è©¦é©—çš„ç¸½ä¼¼ç„¶ / Calculate total likelihood for this trial
            trial_lik = winner_lik * loser_survival
            trial_loglik = np.log(max(trial_lik, 1e-12))  # è½‰æ›ç‚ºå°æ•¸ä¼¼ç„¶ / convert to log-likelihood
            
            # ç´¯åŠ åˆ°ç¸½ä¼¼ç„¶ / Add to total likelihood
            if np.isfinite(trial_loglik):
                loglik_sum += trial_loglik
            else:
                loglik_sum += -10.0  # ç„¡æ•ˆå€¼çš„æ‡²ç½° / penalty for invalid values
        
        return loglik_sum if np.isfinite(loglik_sum) else -1000.0
        
    except Exception as e:
        print(f"ä¼¼ç„¶è¨ˆç®—éŒ¯èª¤ / Likelihood calculation error: {e}")
        return -1000.0

# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šä¿®æ­£çš„å—è©¦è€…åˆ†æå‡½æ•¸ (PyTensor Softmax ä¿®æ­£)
# Part 2: Fixed Subject Analysis Function (PyTensor Softmax Fix)
# ============================================================================

def fixed_subject_analysis(subject_id: int, subject_data: pd.DataFrame) -> Optional[Dict]:
    """
    ä¿®æ­£ç‰ˆå—è©¦è€…åˆ†æå‡½æ•¸ (ä¿®æ­£ PyTensor softmax å•é¡Œ)
    Fixed subject analysis function (fix PyTensor softmax issue)
    
    åƒæ•¸èªªæ˜ / Parameters:
    - subject_id: å—è©¦è€…ç·¨è™Ÿ / Subject ID (ä¾†è‡ª CSV çš„ participant æ¬„ä½ / from participant column in CSV)
    - subject_data: å—è©¦è€…æ•¸æ“š / Subject data (å¾ç¸½æ•¸æ“šä¸­éæ¿¾çš„ç‰¹å®šå—è©¦è€…æ•¸æ“š / filtered data for specific subject)
    
    è¿”å›å€¼ / Returns:
    - åˆ†æçµæœå­—å…¸æˆ– None / Analysis result dictionary or None
    """
    
    try:
        print(f"è™•ç†å—è©¦è€… {subject_id} / Processing Subject {subject_id}...")
        
        # === æ•¸æ“šæº–å‚™éšæ®µ / Data Preparation Phase ===
        
        # æå–åæ‡‰æ™‚é–“æ•¸æ“š / Extract reaction time data
        # ä¾†æºï¼šCSV æ–‡ä»¶çš„ RT æ¬„ä½ / Source: RT column from CSV file
        rt_data = subject_data['RT'].values
        
        # æå–é¸æ“‡åæ‡‰æ•¸æ“š / Extract choice response data  
        # ä¾†æºï¼šCSV æ–‡ä»¶çš„ Response æ¬„ä½ / Source: Response column from CSV file
        choice_data = subject_data['choice_four'].values
        
        # æå–åˆºæ¿€ä½ç½®æ•¸æ“š / Extract stimulus location data
        # ä¾†æºï¼šå¾ Stimulus æ¬„ä½è½‰æ›çš„ X, Y åº§æ¨™ / Source: X, Y coordinates converted from Stimulus column
        stimloc_data = np.column_stack([
            subject_data['stimloc_x'].values,  # X è»¸ä½ç½® / X-axis position
            subject_data['stimloc_y'].values   # Y è»¸ä½ç½® / Y-axis position
        ])
        
        # æª¢æŸ¥æ•¸æ“šé‡æ˜¯å¦è¶³å¤  / Check if data amount is sufficient
        if len(rt_data) < 50:
            print(f"   æ•¸æ“šä¸è¶³ / Insufficient data: {len(rt_data)} trials")
            return None
        
        # æ•¸æ“šæ¸…ç† / Data cleaning
        rt_data = np.maximum(rt_data, 0.1)    # æœ€å°åæ‡‰æ™‚é–“ 0.1s / minimum reaction time 0.1s
        choice_data = np.clip(choice_data, 0, 3)    # é¸æ“‡ç¯„åœ 0-3 / choice range 0-3
        
        print(f"   æ•¸æ“šæº–å‚™å®Œæˆ / Data ready: {len(rt_data)} trials")
        
        # === PyMC æ¨¡å‹å®šç¾©éšæ®µ (ä¿®æ­£ softmax å•é¡Œ) ===
        # === PyMC Model Definition Phase (Fix softmax issue) ===
        
        with pm.Model() as model:
            
            # === å…ˆé©—åˆ†ä½ˆå®šç¾© / Prior Distribution Definition ===
            
            # GRT åƒæ•¸ï¼šæ±ºç­–é‚Šç•Œ / GRT parameters: Decision boundaries
            # db1: X è»¸æ±ºç­–é‚Šç•Œ / X-axis decision boundary
            db1 = pm.Uniform('db1', lower=0.2, upper=0.8)
            
            # db2: Y è»¸æ±ºç­–é‚Šç•Œ / Y-axis decision boundary  
            db2 = pm.Uniform('db2', lower=0.2, upper=0.8)
            
            # æ„ŸçŸ¥é›œè¨Šåƒæ•¸ / Perceptual noise parameter
            sp = pm.Gamma('sp', alpha=2, beta=4)  # å¹³å‡å€¼ç´„ 0.5 / mean approximately 0.5
            
            # åŸºç¤æ¼‚ç§»ç‡åƒæ•¸ / Base drift rate parameter
            base_v = pm.Gamma('base_v', alpha=4, beta=4)  # å¹³å‡å€¼ç´„ 1.0 / mean approximately 1.0
            
            # === ä¿®æ­£çš„ä¼¼ç„¶å‡½æ•¸å®šç¾© (ä½¿ç”¨æ›´ç°¡å–®çš„æ–¹æ³•) ===
            # === Fixed likelihood function definition (using simpler approach) ===
            
            # ğŸ”§ æ–¹æ³•ä¸€ï¼šä½¿ç”¨æ‰‹å‹• softmax å¯¦ç¾ / Method 1: Use manual softmax implementation
            
            # è¨ˆç®—åŸºç¤å°æ•¸æ©Ÿç‡ / Calculate base log probabilities
            base_logits = pt.stack([
                -pt.square(db1 - 0.25) - pt.square(db2 - 0.25),  # é¸é … 0: å·¦ä¸Š / Option 0: top-left
                -pt.square(db1 - 0.25) - pt.square(db2 - 0.75),  # é¸é … 1: å·¦ä¸‹ / Option 1: bottom-left  
                -pt.square(db1 - 0.75) - pt.square(db2 - 0.25),  # é¸é … 2: å³ä¸Š / Option 2: top-right
                -pt.square(db1 - 0.75) - pt.square(db2 - 0.75)   # é¸é … 3: å³ä¸‹ / Option 3: bottom-right
            ])
            
            # æ·»åŠ æ„ŸçŸ¥é›œè¨Šçš„å½±éŸ¿ / Add perceptual noise effect
            adjusted_logits = base_logits / sp
            
            # æ‰‹å‹•å¯¦ç¾ softmax å‡½æ•¸ / Manual softmax implementation
            # softmax(x) = exp(x) / sum(exp(x))
            exp_logits = pt.exp(adjusted_logits - pt.max(adjusted_logits))  # æ•¸å€¼ç©©å®šçš„ exp / numerically stable exp
            choice_probs = pm.Deterministic('choice_probs', exp_logits / pt.sum(exp_logits))
            
            # é¸æ“‡ä¼¼ç„¶ / Choice likelihood
            choice_likelihood = pm.Categorical('choice_obs',
                                             p=choice_probs,
                                             observed=choice_data)
            
            # åæ‡‰æ™‚é–“æ¨¡å‹ (ä½¿ç”¨ Gamma åˆ†ä½ˆä½œç‚ºè¿‘ä¼¼)
            # Reaction time model (use Gamma distribution as approximation)
            rt_alpha = pm.Deterministic('rt_alpha', 1.0 + base_v)  # æ¼‚ç§»ç‡å½±éŸ¿å½¢ç‹€ / drift rate affects shape
            rt_beta = pm.Deterministic('rt_beta', base_v)           # æ¼‚ç§»ç‡å½±éŸ¿é€Ÿåº¦ / drift rate affects rate
            
            rt_likelihood = pm.Gamma('rt_obs', 
                                   alpha=rt_alpha, 
                                   beta=rt_beta, 
                                   observed=rt_data)
            
            print(f"   ä½¿ç”¨æ‰‹å‹• softmax å¯¦ç¾ / Using manual softmax implementation")
        
        print(f"   æ¨¡å‹å»ºç«‹å®Œæˆï¼Œé–‹å§‹æ¡æ¨£ / Model built, starting sampling...")
        
        # === MCMC æ¡æ¨£éšæ®µ ===
        # === MCMC Sampling Phase ===
        
        with model:
            # æ¡æ¨£è¨­å®š / Sampling configuration
            trace = pm.sample(
                draws=500,          # æ¡æ¨£æ•¸é‡ / Number of samples
                tune=500,           # èª¿æ•´æ­¥æ•¸ / Number of tuning steps
                chains=2,           # éˆæ•¸é‡ / Number of chains
                target_accept=0.8,  # ç›®æ¨™æ¥å—ç‡ / Target acceptance rate
                progressbar=True,   # é¡¯ç¤ºé€²åº¦æ¢ / Show progress bar
                return_inferencedata=True,  # è¿”å›æ¨è«–æ•¸æ“š / Return inference data
                cores=1,            # ä½¿ç”¨æ ¸å¿ƒæ•¸ / Number of cores to use
                random_seed=42      # éš¨æ©Ÿç¨®å­ç¢ºä¿å¯é‡ç¾æ€§ / Random seed for reproducibility
            )
        
        print(f"   æ¡æ¨£å®Œæˆ / Sampling completed")
        
        # === æ”¶æ–‚æ€§è¨ºæ–·éšæ®µ ===
        # === Convergence Diagnosis Phase ===
        
        try:
            # è¨ˆç®—æ”¶æ–‚æ€§çµ±è¨ˆé‡ / Calculate convergence statistics
            summary = az.summary(trace)
            
            # R-hat çµ±è¨ˆé‡ï¼šæ‡‰è©²æ¥è¿‘ 1.0 / R-hat statistic: should be close to 1.0
            rhat_max = summary['r_hat'].max() if 'r_hat' in summary else 1.0
            
            # æœ‰æ•ˆæ¨£æœ¬æ•¸ï¼šæ‡‰è©²è¶³å¤ å¤§ / Effective sample size: should be large enough
            ess_min = summary['ess_bulk'].min() if 'ess_bulk' in summary else 50
            
        except Exception as e:
            print(f"   æ”¶æ–‚æ€§è¨ºæ–·è­¦å‘Š / Convergence diagnosis warning: {e}")
            rhat_max, ess_min = 1.05, 50
        
        # === çµæœæ•´ç†éšæ®µ ===
        # === Result Organization Phase ===
        
        result = {
            'subject_id': subject_id,                    # å—è©¦è€…ç·¨è™Ÿ / Subject ID
            'trace': trace,                              # MCMC æ¡æ¨£çµæœ / MCMC sampling results
            'convergence': {                             # æ”¶æ–‚æ€§çµ±è¨ˆ / Convergence statistics
                'rhat_max': float(rhat_max),             # æœ€å¤§ R-hat å€¼ / Maximum R-hat value
                'ess_min': float(ess_min)                # æœ€å°æœ‰æ•ˆæ¨£æœ¬æ•¸ / Minimum effective sample size
            },
            'n_trials': len(rt_data),                    # è©¦é©—æ•¸é‡ / Number of trials
            'success': True                              # æˆåŠŸæ¨™è¨˜ / Success flag
        }
        
        print(f"âœ… å—è©¦è€… {subject_id} å®Œæˆ / Subject {subject_id} completed "
              f"(RÌ‚={rhat_max:.3f}, ESS={ess_min:.0f})")
        
        return result
        
    except Exception as e:
        print(f"âŒ å—è©¦è€… {subject_id} å¤±æ•— / Subject {subject_id} failed: {e}")
        import traceback
        traceback.print_exc()
        return {'subject_id': subject_id, 'success': False, 'error': str(e)}

# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸»è¦åˆ†æå™¨é¡åˆ¥
# Part 3: Main Analyzer Class
# ============================================================================

class FixedGRTAnalyzer:
    """
    ä¿®æ­£ç‰ˆ GRT åˆ†æå™¨ (PyTensor Softmax ä¿®æ­£)
    Fixed GRT Analyzer (PyTensor Softmax Fix)
    """
    
    def __init__(self, csv_file: str = 'GRT_LBA.csv'):
        """
        åˆå§‹åŒ–åˆ†æå™¨ / Initialize analyzer
        
        åƒæ•¸ / Parameters:
        - csv_file: CSV æ•¸æ“šæ–‡ä»¶è·¯å¾‘ / CSV data file path
        """
        
        print("è¼‰å…¥æ•¸æ“š / Loading data...")
        
        # === æ•¸æ“šè¼‰å…¥éšæ®µ ===
        # === Data Loading Phase ===
        
        # è®€å– CSV æ–‡ä»¶ / Read CSV file
        self.df = pd.read_csv(csv_file)
        
        print(f"åŸå§‹æ•¸æ“š / Raw data: {len(self.df)} rows, {len(self.df.columns)} columns")
        print(f"æ¬„ä½åç¨± / Column names: {list(self.df.columns)}")
        
        # === æ•¸æ“šé è™•ç†éšæ®µ ===
        # === Data Preprocessing Phase ===
        
        # éæ¿¾æœ‰æ•ˆçš„åæ‡‰æ™‚é–“ / Filter valid reaction times
        self.df = self.df[(self.df['RT'] > 0.1) & (self.df['RT'] < 10.0)]
        print(f"RT éæ¿¾å¾Œ / After RT filtering: {len(self.df)} rows")
        
        # éæ¿¾ç„¡æ•ˆçš„åæ‡‰é¸æ“‡ / Filter invalid response choices
        self.df = self.df[self.df['Response'].isin([0, 1, 2, 3])]
        print(f"åæ‡‰é¸æ“‡éæ¿¾å¾Œ / After response filtering: {len(self.df)} rows")
        
        # === è®Šæ•¸è½‰æ›éšæ®µ ===
        # === Variable Transformation Phase ===
        
        # å‰µå»ºå››é¸é …è®Šæ•¸ / Create four-choice variable
        self.df['choice_four'] = self.df['Response'].astype(int)
        
        # å‰µå»ºåˆºæ¿€ä½ç½®è®Šæ•¸ / Create stimulus location variables
        stimulus_to_coords = {1: (0, 0), 2: (0, 1), 3: (1, 0), 4: (1, 1)}
        
        self.df['stimloc_x'] = self.df['Stimulus'].map(lambda x: stimulus_to_coords.get(x, (0, 0))[0])
        self.df['stimloc_y'] = self.df['Stimulus'].map(lambda x: stimulus_to_coords.get(x, (0, 0))[1])
        
        # ç§»é™¤è½‰æ›å¤±æ•—çš„è¡Œ / Remove rows with failed conversion
        self.df = self.df.dropna(subset=['stimloc_x', 'stimloc_y'])
        print(f"åº§æ¨™è½‰æ›å¾Œ / After coordinate conversion: {len(self.df)} rows")
        
        # === å—è©¦è€…åˆ—è¡¨æº–å‚™ ===
        # === Subject List Preparation ===
        
        # ç²å–æ‰€æœ‰å—è©¦è€…ç·¨è™Ÿ / Get all subject IDs
        self.participants = sorted(self.df['participant'].unique())
        print(f"å—è©¦è€…æ•¸é‡ / Number of subjects: {len(self.participants)}")
        
        # æª¢æŸ¥æ¯å€‹å—è©¦è€…çš„æ•¸æ“šé‡ / Check data amount for each subject
        subject_counts = self.df['participant'].value_counts()
        print(f"æ¯ä½å—è©¦è€…è©¦é©—æ•¸ / Trials per subject:")
        for subject_id in self.participants[:5]:  # é¡¯ç¤ºå‰ 5 ä½ / show first 5
            count = subject_counts[subject_id]
            print(f"  å—è©¦è€… {subject_id}: {count} trials")
        
        print("æ•¸æ“šè¼‰å…¥å®Œæˆ / Data loading completed\n")
    
    def analyze_subject(self, subject_id: int) -> Optional[Dict]:
        """
        åˆ†æå–®ä¸€å—è©¦è€… / Analyze single subject
        """
        
        # éæ¿¾å—è©¦è€…æ•¸æ“š / Filter subject data
        subject_data = self.df[self.df['participant'] == subject_id].copy()
        
        # æª¢æŸ¥æ•¸æ“šæ˜¯å¦å­˜åœ¨ / Check if data exists
        if len(subject_data) == 0:
            print(f"å—è©¦è€… {subject_id} ç„¡æ•¸æ“š / No data for subject {subject_id}")
            return None
        
        # èª¿ç”¨å—è©¦è€…åˆ†æå‡½æ•¸ / Call subject analysis function
        return fixed_subject_analysis(subject_id, subject_data)
    
    def analyze_all_subjects(self, max_subjects: Optional[int] = None) -> Dict:
        """
        åˆ†ææ‰€æœ‰å—è©¦è€… / Analyze all subjects
        """
        
        results = {}  # å„²å­˜æ‰€æœ‰çµæœ / Store all results
        subjects_to_analyze = self.participants[:max_subjects] if max_subjects else self.participants
        
        print(f"é–‹å§‹åˆ†æ {len(subjects_to_analyze)} ä½å—è©¦è€… / Starting analysis of {len(subjects_to_analyze)} subjects")
        print("=" * 60)
        
        start_time = time.time()  # è¨˜éŒ„é–‹å§‹æ™‚é–“ / Record start time
        
        for i, subject_id in enumerate(subjects_to_analyze, 1):
            print(f"\né€²åº¦ / Progress: {i}/{len(subjects_to_analyze)}")
            
            # åˆ†æç•¶å‰å—è©¦è€… / Analyze current subject
            result = self.analyze_subject(subject_id)
            
            if result is not None:
                results[subject_id] = result
            
            # ä¼°è¨ˆå‰©é¤˜æ™‚é–“ / Estimate remaining time
            if i > 0:
                elapsed = time.time() - start_time
                avg_time = elapsed / i
                remaining = avg_time * (len(subjects_to_analyze) - i)
                print(f"   ä¼°è¨ˆå‰©é¤˜æ™‚é–“ / Estimated remaining time: {remaining/60:.1f} minutes")
        
        total_time = time.time() - start_time
        successful = sum(1 for r in results.values() if r.get('success', False))
        
        print("\n" + "=" * 60)
        print(f"åˆ†æå®Œæˆ / Analysis completed!")
        print(f"ç¸½æ™‚é–“ / Total time: {total_time/60:.1f} minutes")
        print(f"æˆåŠŸåˆ†æ / Successfully analyzed: {successful}/{len(subjects_to_analyze)} subjects")
        
        return results
    
    def save_results(self, results: Dict, output_dir: str = "grt_results"):
        """
        å„²å­˜åˆ†æçµæœ / Save analysis results
        """
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„ / Create output directory
        Path(output_dir).mkdir(exist_ok=True)
        
        print(f"å„²å­˜çµæœåˆ° / Saving results to: {output_dir}")
        
        # å„²å­˜æ¯å€‹å—è©¦è€…çš„çµæœ / Save results for each subject
        for subject_id, result in results.items():
            if result.get('success', False):
                # å„²å­˜ trace ç‚º NetCDF æ ¼å¼ / Save trace as NetCDF format
                trace_file = os.path.join(output_dir, f"subject_{subject_id}_trace.nc")
                result['trace'].to_netcdf(trace_file)
                
                # å„²å­˜æ‘˜è¦çµ±è¨ˆ / Save summary statistics
                summary_file = os.path.join(output_dir, f"subject_{subject_id}_summary.csv")
                summary = az.summary(result['trace'])
                summary.to_csv(summary_file)
                
                print(f"   å—è©¦è€… {subject_id} çµæœå·²å„²å­˜ / Subject {subject_id} results saved")
        
        # å‰µå»ºç¸½é«”æ‘˜è¦ / Create overall summary
        summary_data = []
        for subject_id, result in results.items():
            if result.get('success', False):
                summary_data.append({
                    'subject_id': subject_id,
                    'n_trials': result['n_trials'],
                    'rhat_max': result['convergence']['rhat_max'],
                    'ess_min': result['convergence']['ess_min']
                })
        
        if summary_data:
            overall_summary = pd.DataFrame(summary_data)
            overall_file = os.path.join(output_dir, "overall_summary.csv")
            overall_summary.to_csv(overall_file, index=False)
            print(f"ç¸½é«”æ‘˜è¦å·²å„²å­˜ / Overall summary saved: {overall_file}")

# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šä¸»è¦åŸ·è¡Œç¨‹å¼
# Part 4: Main Execution Program
# ============================================================================

def main():
    """
    ä¸»è¦åŸ·è¡Œå‡½æ•¸ / Main execution function
    
    ç”¨é€” / Purpose:
    - åˆå§‹åŒ–åˆ†æå™¨ / Initialize analyzer
    - åŸ·è¡Œåˆ†ææµç¨‹ / Execute analysis workflow
    - å„²å­˜å’Œå ±å‘Šçµæœ / Save and report results
    """
    
    print("=" * 60)
    print("ä¿®æ­£ç‰ˆ GRT-LBA åˆ†æç¨‹å¼ (PyTensor Softmax ä¿®æ­£)")
    print("Fixed GRT-LBA Analysis Program (PyTensor Softmax Fix)")
    print("=" * 60)
    
    try:
        # === åˆå§‹åŒ–éšæ®µ ===
        # === Initialization Phase ===
        
        # å‰µå»ºåˆ†æå™¨å¯¦ä¾‹ / Create analyzer instance
        # æœƒè‡ªå‹•è¼‰å…¥ 'GRT_LBA.csv' æ–‡ä»¶ / Will automatically load 'GRT_LBA.csv' file
        analyzer = FixedGRTAnalyzer('GRT_LBA.csv')
        
        # === åˆ†æéšæ®µ ===
        # === Analysis Phase ===
        
        # é¸æ“‡åˆ†ææ¨¡å¼ / Choose analysis mode
        print("\né¸æ“‡åˆ†ææ¨¡å¼ / Choose analysis mode:")
        print("1. åˆ†æå‰ 3 ä½å—è©¦è€… (æ¸¬è©¦) / Analyze first 3 subjects (test)")
        print("2. åˆ†ææ‰€æœ‰å—è©¦è€… / Analyze all subjects")
        
        choice = input("è«‹é¸æ“‡ (1 æˆ– 2) / Please choose (1 or 2): ").strip()
        
        if choice == "1":
            # æ¸¬è©¦æ¨¡å¼ï¼šåˆ†æå‰ 3 ä½å—è©¦è€… / Test mode: analyze first 3 subjects
            print("\nğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šåˆ†æå‰ 3 ä½å—è©¦è€… / Test mode: analyzing first 3 subjects")
            results = analyzer.analyze_all_subjects(max_subjects=3)
        else:
            # å®Œæ•´æ¨¡å¼ï¼šåˆ†ææ‰€æœ‰å—è©¦è€… / Full mode: analyze all subjects
            print("\nğŸš€ å®Œæ•´æ¨¡å¼ï¼šåˆ†ææ‰€æœ‰å—è©¦è€… / Full mode: analyzing all subjects")
            results = analyzer.analyze_all_subjects()
        
        # === çµæœå„²å­˜éšæ®µ ===
        # === Result Saving Phase ===
        
        if results:
            # å„²å­˜çµæœ / Save results
            analyzer.save_results(results)
            
            # é¡¯ç¤ºæˆåŠŸçš„å—è©¦è€… / Display successful subjects
            successful_subjects = [sid for sid, result in results.items() 
                                 if result.get('success', False)]
            
            print(f"\nâœ… æˆåŠŸåˆ†æçš„å—è©¦è€… / Successfully analyzed subjects: {successful_subjects}")
            
            # é¡¯ç¤ºæ”¶æ–‚æ€§æ‘˜è¦ / Display convergence summary
            if successful_subjects:
                print("\nğŸ“Š æ”¶æ–‚æ€§æ‘˜è¦ / Convergence Summary:")
                print("å—è©¦è€… / Subject | RÌ‚ æœ€å¤§å€¼ / Max RÌ‚ | ESS æœ€å°å€¼ / Min ESS")
                print("-" * 50)
                for sid in successful_subjects:
                    conv = results[sid]['convergence']
                    print(f"{sid:8d} | {conv['rhat_max']:8.3f} | {conv['ess_min']:8.0f}")
        else:
            print("âŒ æ²’æœ‰æˆåŠŸçš„åˆ†æçµæœ / No successful analysis results")
    
    except Exception as e:
        print(f"âŒ ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤ / Program execution error: {e}")
        import traceback
        traceback.print_exc()

# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šæ›¿ä»£æ–¹æ¡ˆ (å¦‚æœæ‰‹å‹• softmax ä»æœ‰å•é¡Œ)
# Part 5: Alternative Solutions (if manual softmax still has issues)
# ============================================================================

def alternative_subject_analysis(subject_id: int, subject_data: pd.DataFrame) -> Optional[Dict]:
    """
    æ›¿ä»£çš„å—è©¦è€…åˆ†æå‡½æ•¸ (å®Œå…¨é¿å… softmax)
    Alternative subject analysis function (completely avoid softmax)
    
    é€™å€‹ç‰ˆæœ¬ä½¿ç”¨æ›´ç°¡å–®çš„æ¨¡å‹ï¼Œå®Œå…¨é¿å… softmax ç›¸é—œçš„å•é¡Œ
    This version uses a simpler model that completely avoids softmax-related issues
    """
    
    try:
        print(f"è™•ç†å—è©¦è€… {subject_id} (æ›¿ä»£æ–¹æ¡ˆ) / Processing Subject {subject_id} (alternative)")
        
        # æ•¸æ“šæº–å‚™ / Data preparation
        rt_data = subject_data['RT'].values
        choice_data = subject_data['choice_four'].values
        stimloc_data = np.column_stack([
            subject_data['stimloc_x'].values,
            subject_data['stimloc_y'].values
        ])
        
        if len(rt_data) < 50:
            print(f"   æ•¸æ“šä¸è¶³ / Insufficient data: {len(rt_data)} trials")
            return None
        
        rt_data = np.maximum(rt_data, 0.1)
        choice_data = np.clip(choice_data, 0, 3)
        
        print(f"   æ•¸æ“šæº–å‚™å®Œæˆ / Data ready: {len(rt_data)} trials")
        
        # === ä½¿ç”¨æœ€ç°¡å–®çš„æ¨¡å‹ (é¿å…æ‰€æœ‰è¤‡é›œçš„ PyTensor æ“ä½œ) ===
        # === Use simplest model (avoid all complex PyTensor operations) ===
        
        with pm.Model() as model:
            
            # ç°¡åŒ–çš„å…ˆé©— / Simplified priors
            db1 = pm.Uniform('db1', lower=0.2, upper=0.8)
            db2 = pm.Uniform('db2', lower=0.2, upper=0.8)
            sp = pm.Gamma('sp', alpha=2, beta=4)
            base_v = pm.Gamma('base_v', alpha=4, beta=4)
            
            # === å®Œå…¨ç°¡åŒ–çš„ä¼¼ç„¶ (ä½¿ç”¨ç¨ç«‹çš„åˆ†ä½ˆ) ===
            # === Completely simplified likelihood (using independent distributions) ===
            
            # 1. åæ‡‰æ™‚é–“æ¨¡å‹ / Reaction time model
            rt_shape = pm.Deterministic('rt_shape', 1.0 + base_v)
            rt_rate = pm.Deterministic('rt_rate', base_v)
            rt_likelihood = pm.Gamma('rt_obs', alpha=rt_shape, beta=rt_rate, observed=rt_data)
            
            # 2. é¸æ“‡æ¨¡å‹ (ä½¿ç”¨ç°¡å–®çš„ Dirichlet-Multinomial) / Choice model (simple Dirichlet-Multinomial)
            # å‰µå»ºåŸºç¤æ©Ÿç‡å‘é‡ / Create base probability vector
            base_alpha = pt.stack([
                1.0 + pt.exp(-(pt.square(db1 - 0.25) + pt.square(db2 - 0.25)) / sp),
                1.0 + pt.exp(-(pt.square(db1 - 0.25) + pt.square(db2 - 0.75)) / sp),
                1.0 + pt.exp(-(pt.square(db1 - 0.75) + pt.square(db2 - 0.25)) / sp),
                1.0 + pt.exp(-(pt.square(db1 - 0.75) + pt.square(db2 - 0.75)) / sp)
            ])
            
            # ä½¿ç”¨ Dirichlet åˆ†ä½ˆç”Ÿæˆæ©Ÿç‡ / Use Dirichlet distribution to generate probabilities
            choice_probs = pm.Dirichlet('choice_probs', a=base_alpha)
            
            # é¸æ“‡ä¼¼ç„¶ / Choice likelihood
            choice_likelihood = pm.Categorical('choice_obs', p=choice_probs, observed=choice_data)
            
            print(f"   ä½¿ç”¨æ›¿ä»£æ¨¡å‹ (Dirichlet-Categorical) / Using alternative model (Dirichlet-Categorical)")
        
        print(f"   æ¨¡å‹å»ºç«‹å®Œæˆï¼Œé–‹å§‹æ¡æ¨£ / Model built, starting sampling...")
        
        # MCMC æ¡æ¨£ / MCMC sampling
        with model:
            trace = pm.sample(
                draws=500,
                tune=500,
                chains=2,
                target_accept=0.8,
                progressbar=True,
                return_inferencedata=True,
                cores=1,
                random_seed=42
            )
        
        print(f"   æ¡æ¨£å®Œæˆ / Sampling completed")
        
        # æ”¶æ–‚æ€§è¨ºæ–· / Convergence diagnosis
        try:
            summary = az.summary(trace)
            rhat_max = summary['r_hat'].max() if 'r_hat' in summary else 1.0
            ess_min = summary['ess_bulk'].min() if 'ess_bulk' in summary else 50
        except Exception as e:
            print(f"   æ”¶æ–‚æ€§è¨ºæ–·è­¦å‘Š / Convergence diagnosis warning: {e}")
            rhat_max, ess_min = 1.05, 50
        
        # çµæœæ•´ç† / Result organization
        result = {
            'subject_id': subject_id,
            'trace': trace,
            'convergence': {
                'rhat_max': float(rhat_max),
                'ess_min': float(ess_min)
            },
            'n_trials': len(rt_data),
            'success': True,
            'method': 'alternative'  # æ¨™è¨˜ä½¿ç”¨æ›¿ä»£æ–¹æ³• / mark as using alternative method
        }
        
        print(f"âœ… å—è©¦è€… {subject_id} å®Œæˆ (æ›¿ä»£æ–¹æ¡ˆ) / Subject {subject_id} completed (alternative) "
              f"(RÌ‚={rhat_max:.3f}, ESS={ess_min:.0f})")
        
        return result
        
    except Exception as e:
        print(f"âŒ å—è©¦è€… {subject_id} å¤±æ•— (æ›¿ä»£æ–¹æ¡ˆ) / Subject {subject_id} failed (alternative): {e}")
        import traceback
        traceback.print_exc()
        return {'subject_id': subject_id, 'success': False, 'error': str(e), 'method': 'alternative'}

# ============================================================================
# ç¨‹å¼å…¥å£é» / Program Entry Point
# ============================================================================

if __name__ == "__main__":
    main()

# ============================================================================
# ä½¿ç”¨èªªæ˜å’Œæ•…éšœæ’é™¤ / Usage Instructions and Troubleshooting
# ============================================================================

"""
ä½¿ç”¨æ–¹æ³• / How to Use:

1. æº–å‚™æ•¸æ“šæ–‡ä»¶ / Prepare data file:
   - ç¢ºä¿ 'GRT_LBA.csv' æ–‡ä»¶åœ¨ç•¶å‰ç›®éŒ„ / Ensure 'GRT_LBA.csv' file is in current directory
   - æ–‡ä»¶æ‡‰åŒ…å«ä»¥ä¸‹æ¬„ä½ / File should contain following columns:
     * participant: å—è©¦è€…ç·¨è™Ÿ / Subject ID
     * RT: åæ‡‰æ™‚é–“ / Reaction time
     * Response: é¸æ“‡åæ‡‰ (0-3) / Choice response (0-3)
     * Stimulus: åˆºæ¿€é¡å‹ (1-4) / Stimulus type (1-4)

2. åŸ·è¡Œç¨‹å¼ / Run program:
   python GRT_LBAinAU_fixed.py

3. å¦‚æœé‡åˆ° softmax éŒ¯èª¤ / If encountering softmax errors:
   - ç¨‹å¼æœƒè‡ªå‹•ä½¿ç”¨æ‰‹å‹• softmax å¯¦ç¾ / Program will automatically use manual softmax implementation
   - å¦‚æœä»æœ‰å•é¡Œï¼Œå¯ä»¥ä¿®æ”¹ç¨‹å¼ä½¿ç”¨ alternative_subject_analysis å‡½æ•¸ / If still problematic, modify program to use alternative_subject_analysis function

ä¸»è¦ä¿®æ­£ / Key Fixes:
- âœ… ä¿®æ­£ PyTensor softmax å‡½æ•¸ä¸å­˜åœ¨çš„å•é¡Œ / Fixed PyTensor softmax function not existing issue
- âœ… æä¾›æ‰‹å‹• softmax å¯¦ç¾ / Provided manual softmax implementation
- âœ… æä¾›æ›¿ä»£åˆ†ææ–¹æ³• (ä½¿ç”¨ Dirichlet-Categorical) / Provided alternative analysis method (using Dirichlet-Categorical)
- âœ… å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œè¨ºæ–· / Complete error handling and diagnostics
- âœ… è©³ç´°çš„ä¸­è‹±æ–‡è¨»è§£ / Detailed bilingual comments

æ•…éšœæ’é™¤ / Troubleshooting:
1. å¦‚æœå‡ºç¾ "softmax" éŒ¯èª¤ / If "softmax" error occurs:
   - ä½¿ç”¨æ‰‹å‹•å¯¦ç¾çš„ softmax / Use manually implemented softmax
   - æˆ–åˆ‡æ›åˆ°æ›¿ä»£åˆ†ææ–¹æ³• / Or switch to alternative analysis method

2. å¦‚æœæ¡æ¨£å¤±æ•— / If sampling fails:
   - æ¸›å°‘ draws å’Œ tune åƒæ•¸ / Reduce draws and tune parameters
   - å¢åŠ  target_accept åˆ° 0.9 / Increase target_accept to 0.9

3. å¦‚æœæ”¶æ–‚æ€§ä¸ä½³ / If poor convergence:
   - å¢åŠ æ¡æ¨£æ•¸é‡ / Increase number of samples
   - æª¢æŸ¥å…ˆé©—åˆ†ä½ˆæ˜¯å¦åˆç† / Check if priors are reasonable
"""
