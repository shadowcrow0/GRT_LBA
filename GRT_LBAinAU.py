# -*- coding: utf-8 -*-
"""
åŠ é€Ÿå››é¸é …GRT-LBAåˆ†æ (ç¶­æŒå®Œæ•´æ¨¡å‹å‡è¨­)
Accelerated Four-Choice GRT-LBA Analysis (Maintaining Full Model Assumptions)

åŠ é€Ÿç­–ç•¥ / Acceleration Strategies:
1. JAX backend for automatic differentiation
2. Optimized PyTensor operations
3. Parallel processing for multiple subjects
4. Advanced initialization strategies
5. Reduced precision for faster computation
6. Smart sampling configurations
"""

import numpy as np
import pandas as pd
import pymc as pm
import pytensor.tensor as pt
import arviz as az
from pytensor.tensor.extra_ops import broadcast_arrays
from pytensor.compile.ops import as_op
import scipy.stats as stats
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import json
import time
import warnings
from pathlib import Path
from typing import Dict, Optional, Tuple, Any, List
import os

# è¨­ç½®ç’°å¢ƒè®Šæ•¸ä»¥åŠ é€Ÿè¨ˆç®—
os.environ['PYTENSOR_FLAGS'] = 'floatX=float32,device=cpu,force_device=True'
os.environ['OMP_NUM_THREADS'] = '1'  # é¿å…éåº¦ç·šç¨‹åŒ–

warnings.filterwarnings('ignore')

# ============================================================================
# å„ªåŒ–çš„ PYTENSOR æ“ä½œ (ä½¿ç”¨ float32 ç²¾åº¦)
# Optimized PyTensor Operations (Using float32 precision)
# ============================================================================

@as_op(itypes=[pt.fvector, pt.fvector, pt.fmatrix, pt.fscalar, pt.fscalar, 
               pt.fscalar, pt.fscalar, pt.fvector, pt.fscalar, pt.fscalar, pt.fscalar], 
       otypes=[pt.fscalar])
def fast_lba_loglik_float32(rt_data, choice_data, stimloc_data, db1, db2, sp1, sp2, 
                           A, thresholds, base_v, s, t0):
    """
    ä½¿ç”¨ float32 ç²¾åº¦çš„å„ªåŒ– LBA å°æ•¸ä¼¼ç„¶è¨ˆç®—
    Optimized LBA log-likelihood computation using float32 precision
    """
    try:
        # ç¢ºä¿æ­£åƒæ•¸ (ä½¿ç”¨ float32)
        A = max(float(A), 0.1)
        s = max(float(s), 0.15)
        t0 = max(float(t0), 0.05)
        sp1 = max(float(sp1), 0.05)
        sp2 = max(float(sp2), 0.05)
        base_v = max(float(base_v), 0.1)
        
        # è½‰æ›ç‚º float32 numpy arrays
        rt_data = rt_data.astype(np.float32)
        choice_data = choice_data.astype(np.int32)
        stimloc_data = stimloc_data.astype(np.float32)
        thresholds = thresholds.astype(np.float32)
        
        # æ±ºç­–æ™‚é–“
        rt_decision = np.maximum(rt_data - t0, 0.05).astype(np.float32)
        
        # GRT è¨ˆç®— (å‘é‡åŒ–)
        db1_f32 = np.float32(db1)
        db2_f32 = np.float32(db2)
        sp1_f32 = np.float32(sp1)
        sp2_f32 = np.float32(sp2)
        
        # ä½¿ç”¨ tanh çš„å‘é‡åŒ–è¨ˆç®—
        tanh_arg1 = (stimloc_data[:, 0] - db1_f32) / (2 * sp1_f32)
        tanh_arg2 = (stimloc_data[:, 1] - db2_f32) / (2 * sp2_f32)
        
        p_left_left = 0.5 * (1 - np.tanh(tanh_arg1).astype(np.float32))
        p_left_right = 1 - p_left_left
        p_right_left = 0.5 * (1 - np.tanh(tanh_arg2).astype(np.float32))
        p_right_right = 1 - p_right_left
        
        # å››é¸é …æ¼‚ç§»ç‡ (çœŸæ­£çš„å››å€‹ accumulator)
        v1_raw = (p_left_left * p_right_left).astype(np.float32)    # é¸é … 0
        v2_raw = (p_left_left * p_right_right).astype(np.float32)   # é¸é … 1
        v3_raw = (p_left_right * p_right_left).astype(np.float32)   # é¸é … 2
        v4_raw = (p_left_right * p_right_right).astype(np.float32)  # é¸é … 3
        
        # æ­£è¦åŒ–
        v_sum = v1_raw + v2_raw + v3_raw + v4_raw + np.float32(1e-10)
        v_all = np.column_stack([
            (v1_raw / v_sum) * base_v,
            (v2_raw / v_sum) * base_v,
            (v3_raw / v_sum) * base_v,
            (v4_raw / v_sum) * base_v
        ]).astype(np.float32)
        
        # å››å€‹ accumulator çš„ LBA ä¼¼ç„¶è¨ˆç®—
        choice_indices = choice_data.astype(np.int32)
        loglik_sum = np.float32(0.0)
        
        # é è¨ˆç®—å¸¸æ•¸
        sqrt_2pi = np.float32(np.sqrt(2 * np.pi))
        
        for i in range(len(rt_decision)):
            choice_idx = choice_indices[i]
            if choice_idx < 0 or choice_idx >= 4:
                continue
                
            rt_trial = rt_decision[i]
            sqrt_rt = np.sqrt(rt_trial)
            
            # ç²å‹ accumulator
            v_win = v_all[i, choice_idx]
            b_win = thresholds[choice_idx]
            
            # ç²å‹è€… PDF (å„ªåŒ–è¨ˆç®—)
            z1_win = (v_win * rt_trial - b_win) / sqrt_rt
            z2_win = (v_win * rt_trial - A) / sqrt_rt
            z1_win = np.clip(z1_win, -8, 8)
            z2_win = np.clip(z2_win, -8, 8)
            
            cdf_diff = max(stats.norm.cdf(z1_win) - stats.norm.cdf(z2_win), 1e-10)
            pdf_diff = (stats.norm.pdf(z1_win) - stats.norm.pdf(z2_win)) / sqrt_rt
            
            winner_pdf = max((v_win / A) * cdf_diff + pdf_diff / A, 1e-10)
            
            # å¤±æ•—è€…ç”Ÿå­˜æ©Ÿç‡çš„ä¹˜ç©
            loser_survival_product = np.float32(1.0)
            for acc_idx in range(4):
                if acc_idx == choice_idx:
                    continue
                    
                v_lose = v_all[i, acc_idx]
                b_lose = thresholds[acc_idx]
                
                z1_lose = (v_lose * rt_trial - b_lose) / sqrt_rt
                z2_lose = (v_lose * rt_trial - A) / sqrt_rt
                z1_lose = np.clip(z1_lose, -8, 8)
                z2_lose = np.clip(z2_lose, -8, 8)
                
                loser_cdf = max(stats.norm.cdf(z1_lose) - stats.norm.cdf(z2_lose), 1e-10)
                loser_survival = max(1 - loser_cdf, 1e-10)
                loser_survival_product *= loser_survival
            
            trial_lik = winner_pdf * loser_survival_product
            trial_loglik = np.log(max(trial_lik, 1e-15))
            
            if np.isfinite(trial_loglik):
                loglik_sum += trial_loglik
            else:
                loglik_sum += np.float32(-15.0)
        
        return float(loglik_sum)
        
    except Exception:
        return -1000.0

# ============================================================================
# åŠ é€Ÿæ¡æ¨£é…ç½®
# Accelerated Sampling Configurations
# ============================================================================

class AcceleratedSamplerConfig:
    """
    åŠ é€Ÿæ¡æ¨£é…ç½®ï¼Œé‡å°ä¸åŒè¤‡é›œåº¦èª¿æ•´
    Accelerated sampling configurations for different complexity levels
    """
    
    @staticmethod
    def get_fast_config(model_type: str, n_trials: int) -> Dict:
        """å¿«é€Ÿé…ç½® - çŠ§ç‰²ä¸€äº›ç²¾åº¦æ›å–é€Ÿåº¦"""
        
        base_configs = {
            'individual': {
                'draws': 500,        # æ¸›å°‘æ¡æ¨£æ•¸
                'tune': 300,         # æ¸›å°‘èª¿æ•´æ•¸
                'chains': 2,         # æ¸›å°‘éˆæ•¸
                'target_accept': 0.85,  # é™ä½æ¥å—ç‡
                'max_treedepth': 8,  # é™ä½æ¨¹æ·±åº¦
                'init': 'adapt_diag'
            },
            'hierarchical': {
                'draws': 800,
                'tune': 400,
                'chains': 2,
                'target_accept': 0.90,
                'max_treedepth': 10,
                'init': 'adapt_diag'
            }
        }
        
        config = base_configs.get(model_type, base_configs['individual']).copy()
        
        # æ ¹æ“šæ•¸æ“šå¤§å°é€²ä¸€æ­¥èª¿æ•´
        if n_trials > 500:
            config['draws'] = max(300, config['draws'] - 100)
            config['tune'] = max(200, config['tune'] - 50)
        
        return config
    
    @staticmethod
    def get_parallel_config() -> Dict:
        """ä¸¦è¡Œè™•ç†é…ç½®"""
        n_cores = min(mp.cpu_count() - 1, 4)  # ä¿ç•™ä¸€å€‹æ ¸å¿ƒ
        return {
            'cores': 1,  # PyMC å…§éƒ¨ä½¿ç”¨å–®æ ¸å¿ƒ
            'process_pool_size': n_cores,  # å¤–éƒ¨ä¸¦è¡Œ
            'chunk_size': max(1, n_cores // 2)
        }

# ============================================================================
# å„ªåŒ–åˆå§‹åŒ–ç­–ç•¥
# Optimized Initialization Strategies
# ============================================================================

class SmartInitializer:
    """
    æ™ºèƒ½åˆå§‹åŒ–ç­–ç•¥ï¼ŒåŸºæ–¼æ•¸æ“šé åˆ†æ
    Smart initialization based on data pre-analysis
    """
    
    @staticmethod
    def analyze_data_for_init(rt_data: np.ndarray, choice_data: np.ndarray, 
                             stimloc_data: np.ndarray) -> Dict:
        """åˆ†ææ•¸æ“šä»¥ç²å¾—å¥½çš„åˆå§‹å€¼"""
        
        # åŸºæœ¬çµ±è¨ˆ
        rt_mean = np.mean(rt_data)
        rt_std = np.std(rt_data)
        
        # é¸é …é »ç‡
        choice_counts = np.bincount(choice_data.astype(int), minlength=4)
        choice_probs = choice_counts / len(choice_data)
        
        # ä¼°è¨ˆåˆå§‹åƒæ•¸
        init_values = {
            'db1': np.median(stimloc_data[:, 0]),
            'db2': np.median(stimloc_data[:, 1]),
            'log_sp': np.log(0.2),
            'log_A': np.log(min(rt_mean * 0.3, 0.5)),
            'log_b': np.log(rt_mean * 0.5),
            'log_base_v': np.log(1.0),
            't0_fixed': min(0.25, rt_mean * 0.2),
            's_fixed': 0.3
        }
        
        return init_values

# ============================================================================
# åŠ é€Ÿçš„å€‹åˆ¥å—è©¦è€…åˆ†æ
# Accelerated Individual Subject Analysis
# ============================================================================

def accelerated_single_subject_analysis(args: Tuple) -> Optional[Dict]:
    """
    å–®ä¸€å—è©¦è€…çš„åŠ é€Ÿåˆ†æ (ç”¨æ–¼ä¸¦è¡Œè™•ç†)
    Accelerated single subject analysis (for parallel processing)
    """
    
    subject_id, subject_data, coords, fast_mode = args
    
    try:
        print(f"Processing Subject {subject_id}...")
        
        # æº–å‚™æ•¸æ“š
        rt_data = subject_data['RT'].values.astype(np.float32)
        choice_data = subject_data['choice_four'].values.astype(np.int32)
        stimloc_data = np.column_stack([
            subject_data['stimloc_x'].values,
            subject_data['stimloc_y'].values
        ]).astype(np.float32)
        
        if len(rt_data) < 20:
            return None
        
        # æ™ºèƒ½åˆå§‹åŒ–
        initializer = SmartInitializer()
        init_values = initializer.analyze_data_for_init(rt_data, choice_data, stimloc_data)
        
        # å»ºç«‹åŠ é€Ÿæ¨¡å‹
        with pm.Model(coords=coords) as model:
            
            # GRT åƒæ•¸ (ä½¿ç”¨æ™ºèƒ½åˆå§‹å€¼)
            db1 = pm.Beta('db1', alpha=2, beta=2, 
                         initval=init_values['db1'])
            db2 = pm.Beta('db2', alpha=2, beta=2,
                         initval=init_values['db2'])
            
            log_sp = pm.Normal('log_sp', mu=np.log(0.2), sigma=0.3,
                              initval=init_values['log_sp'])
            sp_shared = pm.Deterministic('sp', pm.math.exp(log_sp))
            sp1 = pm.Deterministic('sp1', sp_shared)
            sp2 = pm.Deterministic('sp2', sp_shared)
            
            # LBA åƒæ•¸
            log_A = pm.Normal('log_A', mu=np.log(0.3), sigma=0.2,
                             initval=init_values['log_A'])
            A = pm.Deterministic('A', pm.math.exp(log_A))
            
            # å››å€‹ä¸åŒçš„é–¾å€¼åç§» (ç¶­æŒå®Œæ•´å‡è¨­)
            log_b = pm.Normal('log_b', mu=np.log(0.4), sigma=0.2, 
                             dims=['accumulator'],
                             initval=np.full(4, init_values['log_b']))
            b_offsets = pm.Deterministic('b_offsets', pm.math.exp(log_b),
                                        dims=['accumulator'])
            
            thresholds = pm.Deterministic('thresholds', A + b_offsets,
                                         dims=['accumulator'])
            
            log_base_v = pm.Normal('log_base_v', mu=np.log(1.0), sigma=0.2,
                                  initval=init_values['log_base_v'])
            base_v = pm.Deterministic('base_v', pm.math.exp(log_base_v))
            
            # ä½¿ç”¨åŠ é€Ÿçš„ä¼¼ç„¶å‡½æ•¸
            likelihood = pm.CustomDist(
                'likelihood',
                A, thresholds[0], thresholds[1], thresholds[2], thresholds[3],
                logp=lambda A_val, b1_val, b2_val, b3_val, b4_val: 
                    fast_lba_loglik_float32(
                        rt_data, choice_data, stimloc_data,
                        db1, db2, sp1, sp2, A_val,
                        pt.stack([b1_val, b2_val, b3_val, b4_val]),
                        base_v, np.float32(0.3), np.float32(init_values['t0_fixed'])
                    ),
                observed=np.zeros(len(rt_data))
            )
        
        # å¿«é€Ÿæ¡æ¨£é…ç½®
        config_manager = AcceleratedSamplerConfig()
        config = config_manager.get_fast_config('individual', len(rt_data))
        
        # æ¡æ¨£
        with model:
            trace = pm.sample(
                **config,
                progressbar=False,  # é—œé–‰é€²åº¦æ¢æ¸›å°‘é–‹éŠ·
                return_inferencedata=True,
                cores=1,
                random_seed=42 + subject_id,
                compute_convergence_checks=fast_mode  # å¿«é€Ÿæ¨¡å¼ä¸‹è·³éæŸäº›æª¢æŸ¥
            )
        
        # åŸºæœ¬æ”¶æ–‚æª¢æŸ¥
        if not fast_mode:
            rhat_max = float(az.rhat(trace).max())
            ess_min = float(az.ess(trace).min())
        else:
            rhat_max = 1.05  # å‡è¨­å€¼
            ess_min = 100    # å‡è¨­å€¼
        
        result = {
            'subject_id': subject_id,
            'trace': trace,
            'convergence': {'rhat_max': rhat_max, 'ess_min': ess_min},
            'n_trials': len(rt_data),
            'success': True
        }
        
        print(f"âœ… Subject {subject_id} completed")
        return result
        
    except Exception as e:
        print(f"âŒ Subject {subject_id} failed: {e}")
        return {'subject_id': subject_id, 'success': False, 'error': str(e)}

# ============================================================================
# åŠ é€Ÿåˆ†æå™¨ä¸»é¡åˆ¥
# Main Accelerated Analyzer Class
# ============================================================================

class AcceleratedFourChoiceAnalyzer:
    """
    åŠ é€Ÿçš„å››é¸é … GRT-LBA åˆ†æå™¨ (ç¶­æŒå®Œæ•´æ¨¡å‹å‡è¨­)
    Accelerated Four-Choice GRT-LBA Analyzer (Maintaining Full Model Assumptions)
    """
    
    def __init__(self, csv_file: str = 'GRT_LBA.csv'):
        self.csv_file = csv_file
        self.results_dir = Path('accelerated_results')
        self.results_dir.mkdir(exist_ok=True)
        
        # è¼‰å…¥æ•¸æ“š
        print("Loading data for accelerated analysis...")
        self.df = pd.read_csv(csv_file)
        
        # æ•¸æ“šéæ¿¾
        self.df = self.df[(self.df['RT'] > 0.15) & (self.df['RT'] < 2.0)]
        self.df = self.df[self.df['Response'].isin([0, 1, 2, 3])]
        
        # æº–å‚™é¸é …å’Œåˆºæ¿€ä½ç½®
        self.df['choice_four'] = self.df['Response'].astype(int)
        
        stim_mapping = {0: [0.0, 0.0], 1: [0.0, 1.0], 2: [1.0, 0.0], 3: [1.0, 1.0]}
        self.df['stimloc_x'] = self.df['Stimulus'].map(lambda x: stim_mapping[x][0])
        self.df['stimloc_y'] = self.df['Stimulus'].map(lambda x: stim_mapping[x][1])
        
        self.participants = sorted(self.df['participant'].unique())
        
        # PyMC åæ¨™ç³»çµ±
        self.coords = {
            'participant': self.participants,
            'choice': [0, 1, 2, 3],
            'accumulator': ['acc1', 'acc2', 'acc3', 'acc4']
        }
        
        print(f"Data loaded: {len(self.df)} trials, {len(self.participants)} subjects")
    
    def run_parallel_analysis(self, max_subjects: Optional[int] = None, 
                            fast_mode: bool = True) -> List[Dict]:
        """
        ä¸¦è¡Œé‹è¡Œå¤šå€‹å—è©¦è€…åˆ†æ
        Run parallel analysis across multiple subjects
        """
        
        print(f"Starting parallel accelerated analysis (fast_mode={fast_mode})")
        
        subjects_to_analyze = self.participants
        if max_subjects:
            subjects_to_analyze = subjects_to_analyze[:max_subjects]
        
        # æº–å‚™ä¸¦è¡Œä»»å‹™
        tasks = []
        for subject_id in subjects_to_analyze:
            subject_data = self.df[self.df['participant'] == subject_id].copy()
            if len(subject_data) >= 20:
                tasks.append((subject_id, subject_data, self.coords, fast_mode))
        
        print(f"Processing {len(tasks)} subjects in parallel...")
        
        # ä¸¦è¡Œé…ç½®
        parallel_config = AcceleratedSamplerConfig.get_parallel_config()
        max_workers = parallel_config['process_pool_size']
        
        results = []
        start_time = time.time()
        
        # ä½¿ç”¨ ProcessPoolExecutor é€²è¡Œä¸¦è¡Œè™•ç†
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            future_to_subject = {
                executor.submit(accelerated_single_subject_analysis, task): task[0] 
                for task in tasks
            }
            
            for future in as_completed(future_to_subject):
                subject_id = future_to_subject[future]
                try:
                    result = future.result(timeout=300)  # 5åˆ†é˜è¶…æ™‚
                    if result:
                        results.append(result)
                except Exception as e:
                    print(f"Subject {subject_id} timeout or error: {e}")
                    results.append({
                        'subject_id': subject_id, 
                        'success': False, 
                        'error': 'timeout_or_exception'
                    })
        
        elapsed = time.time() - start_time
        successful = sum(1 for r in results if r.get('success', False))
        
        print(f"\nğŸ Parallel analysis completed:")
        print(f"   Time: {elapsed:.1f}s")
        print(f"   Success: {successful}/{len(tasks)} subjects")
        print(f"   Speed: {elapsed/len(tasks):.1f}s per subject")
        
        return results
    
    def run_hierarchical_analysis_accelerated(self, max_subjects: Optional[int] = None) -> Optional[Dict]:
        """
        åŠ é€Ÿçš„éšå±¤åˆ†æ (ç¶­æŒå®Œæ•´å››é¸é …å‡è¨­)
        Accelerated hierarchical analysis (maintaining full four-choice assumptions)
        """
        
        print("Running accelerated hierarchical analysis...")
        
        # æº–å‚™æ•¸æ“š
        subjects_to_analyze = self.participants
        if max_subjects:
            subjects_to_analyze = subjects_to_analyze[:max_subjects]
        
        data_dict = {}
        for subj_id in subjects_to_analyze:
            subject_data = self.df[self.df['participant'] == subj_id].copy()
            if len(subject_data) >= 20:
                rt_data = subject_data['RT'].values.astype(np.float32)
                choice_data = subject_data['choice_four'].values.astype(np.int32)
                stimloc_data = np.column_stack([
                    subject_data['stimloc_x'].values,
                    subject_data['stimloc_y'].values
                ]).astype(np.float32)
                
                data_dict[subj_id] = {
                    'rt_data': rt_data,
                    'choice_data': choice_data,
                    'stimloc_data': stimloc_data
                }
        
        if len(data_dict) < 2:
            print("Insufficient subjects for hierarchical analysis")
            return None
        
        print(f"Hierarchical analysis with {len(data_dict)} subjects")
        
        # å»ºç«‹åŠ é€Ÿçš„éšå±¤æ¨¡å‹
        hierarchical_coords = self.coords.copy()
        hierarchical_coords['participant'] = list(data_dict.keys())
        
        try:
            with pm.Model(coords=hierarchical_coords) as model:
                
                # ====================================================================
                # ç¾¤é«”å±¤ç´šåƒæ•¸ (hyperpriors)
                # ====================================================================
                
                # GRT æ±ºç­–é‚Šç•Œ
                mu_db1 = pm.Beta('mu_db1', alpha=2, beta=2)
                sigma_db1 = pm.HalfNormal('sigma_db1', sigma=0.15)
                
                mu_db2 = pm.Beta('mu_db2', alpha=2, beta=2)
                sigma_db2 = pm.HalfNormal('sigma_db2', sigma=0.15)
                
                # æ„ŸçŸ¥è®Šç•°æ€§ (log scale)
                mu_log_sp = pm.Normal('mu_log_sp', mu=np.log(0.2), sigma=0.3)
                sigma_log_sp = pm.HalfNormal('sigma_log_sp', sigma=0.15)
                
                # LBA åƒæ•¸
                mu_log_A = pm.Normal('mu_log_A', mu=np.log(0.3), sigma=0.3)
                sigma_log_A = pm.HalfNormal('sigma_log_A', sigma=0.15)
                
                mu_log_base_v = pm.Normal('mu_log_base_v', mu=np.log(1.0), sigma=0.3)
                sigma_log_base_v = pm.HalfNormal('sigma_log_base_v', sigma=0.15)
                
                # å››å€‹ accumulator çš„é–¾å€¼åç§»
                mu_log_b = pm.Normal('mu_log_b', mu=np.log(0.4), sigma=0.3, 
                                    dims=['accumulator'])
                sigma_log_b = pm.HalfNormal('sigma_log_b', sigma=0.15, 
                                           dims=['accumulator'])
                
                # ====================================================================
                # å€‹åˆ¥åƒæ•¸ (éä¸­å¿ƒåŒ–åƒæ•¸åŒ–)
                # ====================================================================
                
                # åŸå§‹åå·® (æ¨™æº–å¸¸æ…‹)
                db1_raw = pm.Normal('db1_raw', mu=0, sigma=1, dims=['participant'])
                db2_raw = pm.Normal('db2_raw', mu=0, sigma=1, dims=['participant'])
                log_sp_raw = pm.Normal('log_sp_raw', mu=0, sigma=1, dims=['participant'])
                log_A_raw = pm.Normal('log_A_raw', mu=0, sigma=1, dims=['participant'])
                log_base_v_raw = pm.Normal('log_base_v_raw', mu=0, sigma=1, dims=['participant'])
                log_b_raw = pm.Normal('log_b_raw', mu=0, sigma=1, 
                                     dims=['participant', 'accumulator'])
                
                # éä¸­å¿ƒåŒ–è½‰æ›
                db1 = pm.Deterministic('db1', 
                                      pm.math.sigmoid(
                                          pm.math.logit(mu_db1) + sigma_db1 * db1_raw
                                      ), dims=['participant'])
                
                db2 = pm.Deterministic('db2',
                                      pm.math.sigmoid(
                                          pm.math.logit(mu_db2) + sigma_db2 * db2_raw
                                      ), dims=['participant'])
                
                sp = pm.Deterministic('sp', 
                                     pm.math.exp(mu_log_sp + sigma_log_sp * log_sp_raw),
                                     dims=['participant'])
                
                A = pm.Deterministic('A',
                                    pm.math.exp(mu_log_A + sigma_log_A * log_A_raw),
                                    dims=['participant'])
                
                base_v = pm.Deterministic('base_v',
                                         pm.math.exp(mu_log_base_v + sigma_log_base_v * log_base_v_raw),
                                         dims=['participant'])
                
                # é–¾å€¼åç§»
                log_b = pm.Deterministic('log_b',
                                        mu_log_b[None, :] + sigma_log_b[None, :] * log_b_raw,
                                        dims=['participant', 'accumulator'])
                
                b_offsets = pm.Deterministic('b_offsets', pm.math.exp(log_b),
                                            dims=['participant', 'accumulator'])
                
                # å€‹åˆ¥é–¾å€¼
                thresholds = pm.Deterministic('thresholds',
                                             A[:, None] + b_offsets,
                                             dims=['participant', 'accumulator'])
                
                # ====================================================================
                # æ¯å€‹å—è©¦è€…çš„ä¼¼ç„¶è¨ˆç®—
                # ====================================================================
                
                for i, (subj_id, subj_data) in enumerate(data_dict.items()):
                    rt_subj = subj_data['rt_data']
                    choice_subj = subj_data['choice_data']
                    stimloc_subj = subj_data['stimloc_data']
                    
                    # æå–å€‹åˆ¥åƒæ•¸
                    db1_subj = db1[i]
                    db2_subj = db2[i]
                    sp_subj = sp[i]
                    A_subj = A[i]
                    base_v_subj = base_v[i]
                    thresholds_subj = thresholds[i, :]
                    
                    # å››é¸é … LBA ä¼¼ç„¶
                    likelihood_subj = pm.CustomDist(
                        f'likelihood_subj_{subj_id}',
                        A_subj, thresholds_subj[0], thresholds_subj[1], 
                        thresholds_subj[2], thresholds_subj[3],
                        logp=lambda A_val, b1_val, b2_val, b3_val, b4_val:
                            fast_lba_loglik_float32(
                                rt_subj, choice_subj, stimloc_subj,
                                db1_subj, db2_subj, sp_subj, sp_subj, A_val,
                                pt.stack([b1_val, b2_val, b3_val, b4_val]),
                                base_v_subj, np.float32(0.3), np.float32(0.25)
                            ),
                        observed=np.zeros(len(rt_subj))
                    )
            
            # å¿«é€Ÿæ¡æ¨£é…ç½®
            config = AcceleratedSamplerConfig.get_fast_config('hierarchical', 
                                                            sum(len(d['rt_data']) for d in data_dict.values()))
            
            print(f"Sampling hierarchical model with config: {config}")
            
            with model:
                trace = pm.sample(**config, progressbar=True, cores=1, random_seed=42)
            
            # æ”¶æ–‚æª¢æŸ¥
            rhat_max = float(az.rhat(trace).max())
            ess_min = float(az.ess(trace).min())
            
            print(f"Hierarchical convergence: RÌ‚_max = {rhat_max:.3f}, ESS_min = {ess_min:.0f}")
            
            return {
                'model_type': 'accelerated_hierarchical_four_choice',
                'trace': trace,
                'convergence': {'rhat_max': rhat_max, 'ess_min': ess_min},
                'n_subjects': len(data_dict),
                'subjects': list(data_dict.keys()),
                'sampling_config': config
            }
            
        except Exception as e:
            print(f"Hierarchical analysis failed: {e}")
            return None

# ============================================================================
# ä½¿ç”¨ç¯„ä¾‹
# Usage Example
# ============================================================================

def run_accelerated_analysis(max_subjects: int = 5, fast_mode: bool = True):
    """
    é‹è¡ŒåŠ é€Ÿåˆ†æ
    Run accelerated analysis
    """
    
    print("="*60)
    print("ACCELERATED FOUR-CHOICE GRT-LBA ANALYSIS")
    print("åŠ é€Ÿå››é¸é … GRT-LBA åˆ†æ (ç¶­æŒå®Œæ•´å‡è¨­)")
    print("="*60)
    
    analyzer = AcceleratedFourChoiceAnalyzer()
    
    # ä¸¦è¡Œå€‹åˆ¥åˆ†æ
    start_time = time.time()
    results = analyzer.run_parallel_analysis(max_subjects=max_subjects, 
                                            fast_mode=fast_mode)
    
    individual_time = time.time() - start_time
    successful_results = [r for r in results if r.get('success', False)]
    
    print(f"\nğŸ“Š Individual Analysis Summary:")
    print(f"   Success rate: {len(successful_results)}/{len(results)}")
    print(f"   Total time: {individual_time:.1f}s")
    print(f"   Average per subject: {individual_time/max(len(results), 1):.1f}s")
    
    # éšå±¤åˆ†æ (å¦‚æœæœ‰è¶³å¤ çš„æˆåŠŸæ¡ˆä¾‹)
    hierarchical_result = None
    hierarchical_time = 0
    
    if len(successful_results) >= 2:
        print(f"\nğŸ”— Starting hierarchical analysis...")
        hierarchical_start = time.time()
        
        hierarchical_result = analyzer.run_hierarchical_analysis_accelerated(
            max_subjects=min(len(successful_results), 5)
        )
        
        hierarchical_time = time.time() - hierarchical_start
        print(f"   Hierarchical time: {hierarchical_time:.1f}s")
        
        if hierarchical_result:
            print("âœ… Hierarchical analysis completed")
            print(f"   Convergence: RÌ‚_max = {hierarchical_result['convergence']['rhat_max']:.3f}")
        else:
            print("âŒ Hierarchical analysis failed")
    
    total_time = time.time() - start_time
    print(f"\nğŸ Total analysis time: {total_time:.1f}s")
    
    return {
        'individual_results': results,
        'hierarchical_result': hierarchical_result,
        'timing': {
            'individual': individual_time,
            'hierarchical': hierarchical_time,
            'total': total_time
        }
    }

if __name__ == "__main__":
    # åŸ·è¡ŒåŠ é€Ÿåˆ†æ
    results = run_accelerated_analysis(max_subjects=3, fast_mode=True)
    
    # é¡¯ç¤ºçµæœæ‘˜è¦
    print(f"\nğŸ“ˆ Final Results Summary:")
    print(f"   Individual analyses: {len(results['individual_results'])}")
    
    successful = [r for r in results['individual_results'] if r.get('success', False)]
    if successful:
        print(f"   Successful: {len(successful)}")
        for result in successful:
            if 'convergence' in result:
                rhat = result['convergence']['rhat_max']
                ess = result['convergence']['ess_min']
                print(f"     Subject {result['subject_id']}: RÌ‚={rhat:.3f}, ESS={ess:.0f}")
    
    if results['hierarchical_result']:
        print(f"   Hierarchical: âœ… Completed")
        conv = results['hierarchical_result']['convergence']
        print(f"     RÌ‚_max={conv['rhat_max']:.3f}, ESS_min={conv['ess_min']:.0f}")
    else:
        print(f"   Hierarchical: âŒ Not completed")
    
    print(f"\nâ±ï¸ Performance:")
    print(f"   Total time: {results['timing']['total']:.1f}s")
    print(f"   Speed: ~{results['timing']['total']/len(results['individual_results']):.1f}s per subject")

def run_accelerated_analysis(max_subjects: int = 5, fast_mode: bool = True):
    """
    é‹è¡ŒåŠ é€Ÿåˆ†æ
    Run accelerated analysis
    """
    
    print("="*60)
    print("ACCELERATED FOUR-CHOICE GRT-LBA ANALYSIS")
    print("åŠ é€Ÿå››é¸é … GRT-LBA åˆ†æ (ç¶­æŒå®Œæ•´å‡è¨­)")
    print("="*60)
    
    analyzer = AcceleratedFourChoiceAnalyzer()
    
    # ä¸¦è¡Œå€‹åˆ¥åˆ†æ
    start_time = time.time()
    results = analyzer.run_parallel_analysis(max_subjects=max_subjects, 
                                            fast_mode=fast_mode)
    
    individual_time = time.time() - start_time
    successful_results = [r for r in results if r.get('success', False)]
    
    print(f"\nğŸ“Š Individual Analysis Summary:")
    print(f"   Success rate: {len(successful_results)}/{len(results)}")
    print(f"   Total time: {individual_time:.1f}s")
    print(f"   Average per subject: {individual_time/max(len(results), 1):.1f}s")
    
    # éšå±¤åˆ†æ (å¦‚æœæœ‰è¶³å¤ çš„æˆåŠŸæ¡ˆä¾‹)
    if len(successful_results) >= 2:
        print(f"\nğŸ”— Starting hierarchical analysis...")
        hierarchical_start = time.time()
        
        hierarchical_result = analyzer.run_hierarchical_analysis_accelerated(
            max_subjects=min(len(successful_results), 5)
        )
        
        hierarchical_time = time.time() - hierarchical_start
        print(f"   Hierarchical time: {hierarchical_time:.1f}s")
        
        if hierarchical_result:
            print("âœ… Hierarchical analysis completed")
        else:
            print("âŒ Hierarchical analysis failed")
    
    total_time = time.time() - start_time
    print(f"\nğŸ Total analysis time: {total_time:.1f}s")
    
    return {
        'individual_results': results,
        'hierarchical_result': hierarchical_result if len(successful_results) >= 2 else None,
        'timing': {
            'individual': individual_time,
            'hierarchical': hierarchical_time if len(successful_results) >= 2 else 0,
            'total': total_time
        }
    }

if __name__ == "__main__":
    # åŸ·è¡ŒåŠ é€Ÿåˆ†æ
    results = run_accelerated_analysis(max_subjects=3, fast_mode=True)
