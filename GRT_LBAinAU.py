# -*- coding: utf-8 -*-
"""
ç·šæ¢å‚¾æ–œåˆ¤æ–·ä»»å‹™ - é›™é€šé“LBAæ¨¡å‹ (Line Tilt Judgment Task - Dual-Channel LBA Model)
Line Tilt Judgment Task - Dual-Channel LBA Model

ç›®çš„ / Purpose:
æ¨¡æ“¬å—è©¦è€…åˆ¤æ–·å·¦å³å…©æ¢ç·šæ¢å‚¾æ–œæ–¹å‘çš„èªçŸ¥éç¨‹
Simulate the cognitive process of judging the tilt direction of left and right lines

å¯¦é©—è¨­è¨ˆ / Experimental Design:
- åˆºæ¿€ï¼šå››å€‹è§’è½å‡ºç¾ä¸åŒç·šæ¢çµ„åˆ (\|, \/, |\, //)
- ä»»å‹™ï¼šåˆ¤æ–·å‰›æ‰çœ‹åˆ°çš„æ˜¯å“ªç¨®çµ„åˆ
- èªçŸ¥æ¨¡å‹ï¼šå…©å€‹å¹³è¡Œçš„LBAé€šé“åˆ†åˆ¥è™•ç†å·¦å³ç·šæ¢å‚¾æ–œ
- Stimuli: Different line combinations appear in four corners (\|, \/, |\, //)
- Task: Judge which combination was just seen
- Cognitive model: Two parallel LBA channels process left and right line tilts
"""

import numpy as np
import pandas as pd
import pymc as pm
import pytensor.tensor as pt
import arviz as az
import time
from typing import Dict, Optional

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•¸æ“šé è™•ç†å’Œåˆºæ¿€ç·¨ç¢¼
# Part 1: Data Preprocessing and Stimulus Encoding
# ============================================================================

def prepare_line_tilt_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    æº–å‚™ç·šæ¢å‚¾æ–œåˆ¤æ–·ä»»å‹™çš„æ•¸æ“š
    Prepare data for line tilt judgment task
    
    ç›®çš„ / Purpose:
    å°‡åŸå§‹åˆºæ¿€ç·¨ç¢¼è½‰æ›ç‚ºå·¦å³ç·šæ¢å‚¾æ–œç‰¹å¾µ
    Convert raw stimulus codes to left and right line tilt features
    
    åƒæ•¸ä¾†æº / Parameter Sources:
    - df: åŸå§‹CSVæ•¸æ“šï¼ŒåŒ…å«Stimulusæ¬„ä½(1,2,3,4)
    - df: Raw CSV data containing Stimulus column (1,2,3,4)
    
    åˆºæ¿€æ˜ å°„ / Stimulus Mapping:
    1 â†’ å·¦ä¸Šè§’ â†’ å·¦ç·šæ¢:\, å³ç·šæ¢:| â†’ (0, 1)
    2 â†’ å·¦ä¸‹è§’ â†’ å·¦ç·šæ¢:\, å³ç·šæ¢:/ â†’ (0, 0) 
    3 â†’ å³ä¸Šè§’ â†’ å·¦ç·šæ¢:|, å³ç·šæ¢:| â†’ (1, 1)
    4 â†’ å³ä¸‹è§’ â†’ å·¦ç·šæ¢:|, å³ç·šæ¢:/ â†’ (1, 0)
    """
    
    print("é–‹å§‹æ•¸æ“šé è™•ç† / Starting data preprocessing...")
    
    # åˆºæ¿€ç·¨ç¢¼æ˜ å°„è¡¨ / Stimulus encoding mapping
    # ç›®çš„ï¼šå°‡1-4çš„åˆºæ¿€ç·¨è™Ÿè½‰æ›ç‚ºå·¦å³ç·šæ¢å‚¾æ–œç‰¹å¾µ
    # Purpose: Convert stimulus numbers 1-4 to left/right line tilt features
    stimulus_mapping = {
        1: {'left_tilt': 0, 'right_tilt': 1, 'description': 'å·¦\\å³|'},  # å·¦æ–œå³ç›´
        2: {'left_tilt': 0, 'right_tilt': 0, 'description': 'å·¦\\å³/'},  # å·¦æ–œå³æ–œ  
        3: {'left_tilt': 1, 'right_tilt': 1, 'description': 'å·¦|å³|'},  # å·¦ç›´å³ç›´
        4: {'left_tilt': 1, 'right_tilt': 0, 'description': 'å·¦|å³/'}   # å·¦ç›´å³æ–œ
    }
    
    # å‰µå»ºæ–°çš„ç‰¹å¾µæ¬„ä½ / Create new feature columns
    # ç›®çš„ï¼šç‚ºLBAæ¨¡å‹æº–å‚™ç¨ç«‹çš„å·¦å³é€šé“è¼¸å…¥
    # Purpose: Prepare independent left/right channel inputs for LBA model
    
    # å·¦ç·šæ¢å‚¾æ–œç‰¹å¾µ / Left line tilt feature
    # ä¾†æºï¼šå¾Stimulusæ¬„ä½æ˜ å°„è€Œä¾†
    # Source: Mapped from Stimulus column
    # 0 = æ–œç·š(\), 1 = ç›´ç·š(|)
    df['left_line_tilt'] = df['Stimulus'].map(
        lambda x: stimulus_mapping.get(x, {'left_tilt': 0})['left_tilt']
    )
    
    # å³ç·šæ¢å‚¾æ–œç‰¹å¾µ / Right line tilt feature  
    # ä¾†æºï¼šå¾Stimulusæ¬„ä½æ˜ å°„è€Œä¾†
    # Source: Mapped from Stimulus column
    # 0 = æ–œç·š(/ or \), 1 = ç›´ç·š(|)
    df['right_line_tilt'] = df['Stimulus'].map(
        lambda x: stimulus_mapping.get(x, {'right_tilt': 0})['right_tilt']
    )
    
    # å››é¸é …çµ„åˆç·¨ç¢¼ / Four-choice combination encoding
    # ç›®çš„ï¼šå°‡Responseæ¬„ä½è½‰æ›ç‚º0-3ç·¨ç¢¼ä¾›LBAä½¿ç”¨
    # Purpose: Convert Response column to 0-3 encoding for LBA use
    # ä¾†æºï¼šCSVæ–‡ä»¶çš„Responseæ¬„ä½
    # Source: Response column from CSV file
    df['choice_response'] = df['Response'].astype(int)
    
    # æ•¸æ“šæ¸…ç† / Data cleaning
    # ç›®çš„ï¼šç§»é™¤ç„¡æ•ˆçš„åæ‡‰æ™‚é–“å’Œé¸æ“‡
    # Purpose: Remove invalid reaction times and choices
    
    # åæ‡‰æ™‚é–“éæ¿¾ / Reaction time filtering
    # ä¾†æºï¼šCSVæ–‡ä»¶çš„RTæ¬„ä½
    # Source: RT column from CSV file
    # ç¯„åœï¼š0.1-10ç§’ï¼Œç§»é™¤éå¿«æˆ–éæ…¢çš„åæ‡‰
    # Range: 0.1-10 seconds, remove too fast or too slow responses
    valid_rt = (df['RT'] >= 0.1) & (df['RT'] <= 10.0)
    
    # é¸æ“‡æœ‰æ•ˆæ€§éæ¿¾ / Choice validity filtering
    # ç›®çš„ï¼šç¢ºä¿é¸æ“‡åœ¨æœ‰æ•ˆç¯„åœå…§
    # Purpose: Ensure choices are within valid range
    valid_choice = df['choice_response'].isin([0, 1, 2, 3])
    
    # æ‡‰ç”¨éæ¿¾æ¢ä»¶ / Apply filtering conditions
    df_clean = df[valid_rt & valid_choice].copy()
    
    # ç§»é™¤ç¼ºå¤±å€¼ / Remove missing values
    # ç›®çš„ï¼šç¢ºä¿æ‰€æœ‰å¿…è¦æ¬„ä½éƒ½æœ‰å€¼
    # Purpose: Ensure all necessary columns have values
    df_clean = df_clean.dropna(subset=['left_line_tilt', 'right_line_tilt', 'choice_response', 'RT'])
    
    print(f"æ•¸æ“šé è™•ç†å®Œæˆ / Data preprocessing completed:")
    print(f"  åŸå§‹æ•¸æ“šé‡ / Original data: {len(df)} trials")
    print(f"  æ¸…ç†å¾Œæ•¸æ“šé‡ / Cleaned data: {len(df_clean)} trials")
    print(f"  åˆºæ¿€åˆ†ä½ˆ / Stimulus distribution:")
    
    # é¡¯ç¤ºåˆºæ¿€åˆ†ä½ˆ / Show stimulus distribution
    for stim, info in stimulus_mapping.items():
        count = len(df_clean[df_clean['Stimulus'] == stim])
        print(f"    åˆºæ¿€{stim} ({info['description']}): {count} trials")
    
    return df_clean

# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šé›™é€šé“LBAä¼¼ç„¶å‡½æ•¸ (ä¿®æ­£ç‰ˆ)
# Part 2: Dual-Channel LBA Likelihood Function (Fixed Version)
# ============================================================================

def compute_dual_lba_likelihood(left_tilt, right_tilt, choice, rt, 
                               left_bias, right_bias, 
                               left_drift, right_drift,
                               noise_left, noise_right):
    """
    è¨ˆç®—é›™é€šé“LBAæ¨¡å‹çš„ä¼¼ç„¶å‡½æ•¸ (å¼µé‡ç‰ˆæœ¬)
    Compute likelihood for dual-channel LBA model (Tensor version)
    
    ç›®çš„ / Purpose:
    æ¨¡æ“¬å…©å€‹ç¨ç«‹çš„LBAé€šé“åˆ†åˆ¥è™•ç†å·¦å³ç·šæ¢å‚¾æ–œåˆ¤æ–·
    Simulate two independent LBA channels processing left/right line tilt judgments
    
    åƒæ•¸èªªæ˜ / Parameter Descriptions:
    
    è¼¸å…¥ç‰¹å¾µ / Input Features:
    - left_tilt: å·¦ç·šæ¢å‚¾æ–œ (0=æ–œç·š, 1=ç›´ç·š) / Left line tilt (0=diagonal, 1=vertical)
    - right_tilt: å³ç·šæ¢å‚¾æ–œ (0=æ–œç·š, 1=ç›´ç·š) / Right line tilt (0=diagonal, 1=vertical)  
    - choice: å—è©¦è€…é¸æ“‡ (0-3) / Subject choice (0-3)
    - rt: åæ‡‰æ™‚é–“ / Reaction time
    
    æ¨¡å‹åƒæ•¸ / Model Parameters:
    - left_bias: å·¦é€šé“åˆ¤æ–·é–¾å€¼ / Left channel judgment threshold
    - right_bias: å³é€šé“åˆ¤æ–·é–¾å€¼ / Right channel judgment threshold
    - left_drift: å·¦é€šé“æ¼‚ç§»ç‡ / Left channel drift rate
    - right_drift: å³é€šé“æ¼‚ç§»ç‡ / Right channel drift rate
    - noise_left: å·¦é€šé“é›œè¨Š / Left channel noise
    - noise_right: å³é€šé“é›œè¨Š / Right channel noise
    """
    
    # LBAæ¨¡å‹å›ºå®šåƒæ•¸ / Fixed LBA parameters
    # ç›®çš„ï¼šè¨­å®šLBAæ¨¡å‹çš„åŸºæœ¬çµæ§‹åƒæ•¸
    # Purpose: Set basic structural parameters for LBA model
    A = 0.4      # èµ·å§‹é»è®Šç•° / Start point variability
    s = 0.3      # æ¼‚ç§»ç‡æ¨™æº–å·® / Drift rate standard deviation
    t0 = 0.2     # éæ±ºç­–æ™‚é–“ / Non-decision time
    b = A + 0.6  # æ±ºç­–é–¾å€¼ / Decision threshold
    
    # è¨ˆç®—æ±ºç­–æ™‚é–“ / Calculate decision time
    # ç›®çš„ï¼šå¾ç¸½åæ‡‰æ™‚é–“ä¸­æ¸›å»éæ±ºç­–æ™‚é–“
    # Purpose: Subtract non-decision time from total reaction time
    # ä¾†æºï¼šrtåƒæ•¸ (å¾CSVçš„RTæ¬„ä½)
    # Source: rt parameter (from RT column in CSV)
    decision_time = pt.maximum(rt - t0, 0.001)
    
    # === å·¦é€šé“LBAè¨ˆç®— / Left Channel LBA Calculation ===
    
    # å·¦é€šé“è­‰æ“šç´¯ç©æ–¹å‘ / Left channel evidence accumulation direction
    # ç›®çš„ï¼šæ ¹æ“šå¯¦éš›åˆºæ¿€å’Œåå¥½è¨ˆç®—è­‰æ“šå¼·åº¦
    # Purpose: Calculate evidence strength based on actual stimulus and bias
    
    # ä½¿ç”¨å¼µé‡æ¢ä»¶åˆ¤æ–·è€ŒéPython if / Use tensor conditional instead of Python if
    left_evidence_direction = pt.where(left_tilt > left_bias, 1.0, -1.0)
    left_evidence_strength = left_drift * left_evidence_direction
    
    # å·¦é€šé“æ¼‚ç§»ç‡è¨ˆç®— / Left channel drift rate calculation
    # ç›®çš„ï¼šçµåˆåˆºæ¿€å¼·åº¦å’Œå€‹é«”åå¥½
    # Purpose: Combine stimulus strength and individual bias
    v_left_correct = pt.maximum(pt.abs(left_evidence_strength) + noise_left, 0.1)
    v_left_incorrect = pt.maximum(0.5 * left_drift + noise_left, 0.1)
    
    # === å³é€šé“LBAè¨ˆç®— / Right Channel LBA Calculation ===
    
    # å³é€šé“è­‰æ“šç´¯ç© / Right channel evidence accumulation
    # ç›®çš„ï¼šç¨ç«‹è™•ç†å³ç·šæ¢å‚¾æ–œåˆ¤æ–·
    # Purpose: Independently process right line tilt judgment
    right_evidence_direction = pt.where(right_tilt > right_bias, 1.0, -1.0)
    right_evidence_strength = right_drift * right_evidence_direction
    
    v_right_correct = pt.maximum(pt.abs(right_evidence_strength) + noise_right, 0.1)
    v_right_incorrect = pt.maximum(0.5 * right_drift + noise_right, 0.1)
    
    # === å››é¸é …çµ„åˆåˆ¤æ–· / Four-choice combination judgment ===
    
    # ç›®çš„ï¼šå°‡å…©å€‹ç¨ç«‹çš„LBAé€šé“çµæœçµ„åˆæˆå››é¸é …åˆ¤æ–·
    # Purpose: Combine two independent LBA channel results into four-choice judgment
    
    # åˆ¤æ–·å·¦å³é€šé“å‚¾å‘ / Determine left/right channel preferences
    # ä½¿ç”¨å¼µé‡æ¢ä»¶åˆ¤æ–· / Use tensor conditional judgments
    left_decision = pt.where(left_tilt > left_bias, 1.0, 0.0)
    right_decision = pt.where(right_tilt > right_bias, 1.0, 0.0)
    
    # çµ„åˆæ±ºç­–æ˜ å°„ / Combined decision mapping
    # ç›®çš„ï¼šå°‡å·¦å³é€šé“æ±ºç­–çµ„åˆç‚ºæœ€çµ‚é¸æ“‡
    # Purpose: Combine left/right channel decisions into final choice
    predicted_choice = left_decision * 2 + right_decision
    
    # === LBAä¼¼ç„¶è¨ˆç®— / LBA Likelihood Calculation ===
    
    # é¸æ“‡ç²å‹è€…å’Œå¤±æ•—è€…çš„æ¼‚ç§»ç‡ / Select winner and loser drift rates
    # ç›®çš„ï¼šæ ¹æ“šå¯¦éš›é¸æ“‡ç¢ºå®šå“ªå€‹ç´¯åŠ å™¨ç²å‹
    # Purpose: Determine which accumulator wins based on actual choice
    
    # ä½¿ç”¨å¼µé‡æ¢ä»¶é¸æ“‡ / Use tensor conditional selection
    choice_correct = pt.eq(choice, predicted_choice)
    
    # æ­£ç¢ºé¸æ“‡çš„æ¼‚ç§»ç‡ / Drift rates for correct choice
    v_winner_correct = (v_left_correct + v_right_correct) / 2
    v_loser1_correct = (v_left_incorrect + v_right_correct) / 2
    v_loser2_correct = (v_left_correct + v_right_incorrect) / 2  
    v_loser3_correct = (v_left_incorrect + v_right_incorrect) / 2
    
    # éŒ¯èª¤é¸æ“‡çš„æ¼‚ç§»ç‡ / Drift rates for incorrect choice
    v_winner_incorrect = (v_left_incorrect + v_right_incorrect) / 2
    v_loser1_incorrect = (v_left_correct + v_right_incorrect) / 2
    v_loser2_incorrect = (v_left_incorrect + v_right_correct) / 2
    v_loser3_incorrect = (v_left_correct + v_right_correct) / 2
    
    # æ ¹æ“šé¸æ“‡æ­£ç¢ºæ€§é¸å–æ¼‚ç§»ç‡ / Select drift rates based on choice correctness
    v_winner = pt.where(choice_correct, v_winner_correct, v_winner_incorrect)
    v_loser1 = pt.where(choice_correct, v_loser1_correct, v_loser1_incorrect)
    v_loser2 = pt.where(choice_correct, v_loser2_correct, v_loser2_incorrect)
    v_loser3 = pt.where(choice_correct, v_loser3_correct, v_loser3_incorrect)
    
    # LBAå¯†åº¦å‡½æ•¸è¨ˆç®— / LBA density function calculation
    # ç›®çš„ï¼šè¨ˆç®—çµ¦å®šåƒæ•¸ä¸‹è§€å¯Ÿåˆ°æ­¤åæ‡‰æ™‚é–“å’Œé¸æ“‡çš„æ©Ÿç‡
    # Purpose: Calculate probability of observing this RT and choice given parameters
    
    sqrt_t = pt.sqrt(decision_time)
    
    # ç²å‹ç´¯åŠ å™¨çš„ä¼¼ç„¶ / Winner accumulator likelihood
    z1_win = pt.clip((v_winner * decision_time - b) / sqrt_t, -6, 6)
    z2_win = pt.clip((v_winner * decision_time - A) / sqrt_t, -6, 6)
    
    # ä½¿ç”¨PyMCçš„å¸¸æ…‹åˆ†ä½ˆå‡½æ•¸ / Use PyMC's normal distribution functions
    from pytensor.tensor import erf
    
    # æ¨™æº–å¸¸æ…‹ç´¯ç©åˆ†ä½ˆå‡½æ•¸ / Standard normal CDF
    def normal_cdf(x):
        return 0.5 * (1 + erf(x / pt.sqrt(2)))
    
    # æ¨™æº–å¸¸æ…‹æ©Ÿç‡å¯†åº¦å‡½æ•¸ / Standard normal PDF
    def normal_pdf(x):
        return pt.exp(-0.5 * x**2) / pt.sqrt(2 * pt.pi)
    
    winner_cdf = normal_cdf(z1_win) - normal_cdf(z2_win)
    winner_pdf = (normal_pdf(z1_win) - normal_pdf(z2_win)) / sqrt_t
    winner_likelihood = pt.maximum((v_winner / A) * winner_cdf + winner_pdf / A, 1e-10)
    
    # å¤±æ•—ç´¯åŠ å™¨çš„ç”Ÿå­˜å‡½æ•¸ / Loser accumulators survival function
    survival_prob = 1.0
    
    for v_loser in [v_loser1, v_loser2, v_loser3]:
        z1_lose = pt.clip((v_loser * decision_time - b) / sqrt_t, -6, 6)
        z2_lose = pt.clip((v_loser * decision_time - A) / sqrt_t, -6, 6)
        
        loser_cdf = normal_cdf(z1_lose) - normal_cdf(z2_lose)
        survival_prob *= pt.maximum(1 - loser_cdf, 1e-6)
    
    # ç¸½ä¼¼ç„¶ / Total likelihood
    # ç›®çš„ï¼šè¨ˆç®—å®Œæ•´çš„LBAæ¨¡å‹ä¼¼ç„¶
    # Purpose: Calculate complete LBA model likelihood
    total_likelihood = winner_likelihood * survival_prob
    
    return pt.log(pt.maximum(total_likelihood, 1e-12))

# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå—è©¦è€…åˆ†æå‡½æ•¸
# Part 3: Subject Analysis Function
# ============================================================================

def analyze_line_tilt_subject(subject_id: int, subject_data: pd.DataFrame) -> Optional[Dict]:
    """
    åˆ†æå–®ä¸€å—è©¦è€…çš„ç·šæ¢å‚¾æ–œåˆ¤æ–·è¡Œç‚º
    Analyze individual subject's line tilt judgment behavior
    
    ç›®çš„ / Purpose:
    ä½¿ç”¨é›™é€šé“LBAæ¨¡å‹ä¼°è¨ˆå—è©¦è€…çš„èªçŸ¥åƒæ•¸
    Use dual-channel LBA model to estimate subject's cognitive parameters
    
    åƒæ•¸ä¾†æº / Parameter Sources:
    - subject_id: å—è©¦è€…ç·¨è™Ÿ (ä¾†è‡ªCSVçš„participantæ¬„ä½)
    - subject_data: è©²å—è©¦è€…çš„æ‰€æœ‰è©¦é©—æ•¸æ“š (å¾ç¸½æ•¸æ“šä¸­éæ¿¾)
    - subject_id: Subject ID (from participant column in CSV)
    - subject_data: All trial data for this subject (filtered from total data)
    """
    
    try:
        print(f"é–‹å§‹åˆ†æå—è©¦è€… {subject_id} / Starting analysis for Subject {subject_id}...")
        
        # === æ•¸æ“šæå–å’Œé©—è­‰ / Data Extraction and Validation ===
        
        # æå–åˆºæ¿€ç‰¹å¾µ / Extract stimulus features
        # ä¾†æºï¼šæ•¸æ“šé è™•ç†éšæ®µå‰µå»ºçš„ç‰¹å¾µ
        # Source: Features created during data preprocessing
        left_tilt_data = subject_data['left_line_tilt'].values    # å·¦ç·šæ¢å‚¾æ–œ
        right_tilt_data = subject_data['right_line_tilt'].values  # å³ç·šæ¢å‚¾æ–œ
        
        # æå–åæ‡‰æ•¸æ“š / Extract response data  
        # ä¾†æºï¼šCSVæ–‡ä»¶çš„åŸå§‹æ¬„ä½
        # Source: Original columns from CSV file
        choice_data = subject_data['choice_response'].values  # é¸æ“‡åæ‡‰ (0-3)
        rt_data = subject_data['RT'].values                   # åæ‡‰æ™‚é–“
        
        # æ•¸æ“šé‡æª¢æŸ¥ / Data quantity check
        # ç›®çš„ï¼šç¢ºä¿æœ‰è¶³å¤ æ•¸æ“šé€²è¡Œå¯é çš„åƒæ•¸ä¼°è¨ˆ
        # Purpose: Ensure sufficient data for reliable parameter estimation
        n_trials = len(rt_data)
        if n_trials < 50:
            print(f"   æ•¸æ“šé‡ä¸è¶³ / Insufficient data: {n_trials} trials (minimum: 50)")
            return None
        
        print(f"   æ•¸æ“šæå–å®Œæˆ / Data extraction completed: {n_trials} trials")
        
        # === PyMCè²è‘‰æ–¯æ¨¡å‹å®šç¾© / PyMC Bayesian Model Definition ===
        
        with pm.Model() as dual_lba_model:
            
            # === å…ˆé©—åˆ†ä½ˆå®šç¾© / Prior Distribution Definition ===
            # ç›®çš„ï¼šè¨­å®šèªçŸ¥åƒæ•¸çš„å…ˆé©—ä¿¡å¿µ
            # Purpose: Set prior beliefs about cognitive parameters
            
            # å·¦é€šé“åˆ¤æ–·åå¥½ / Left channel judgment bias
            # æ„ç¾©ï¼šå—è©¦è€…åˆ¤æ–·å·¦ç·šæ¢ç‚º"ç›´ç·š"çš„å‚¾å‘
            # Meaning: Subject's tendency to judge left line as "vertical"
            # ç¯„åœï¼š0-1ï¼Œ0.5è¡¨ç¤ºç„¡åå¥½
            # Range: 0-1, 0.5 indicates no bias
            left_bias = pm.Beta('left_bias', alpha=2, beta=2)
            
            # å³é€šé“åˆ¤æ–·åå¥½ / Right channel judgment bias  
            # æ„ç¾©ï¼šå—è©¦è€…åˆ¤æ–·å³ç·šæ¢ç‚º"ç›´ç·š"çš„å‚¾å‘
            # Meaning: Subject's tendency to judge right line as "vertical"
            right_bias = pm.Beta('right_bias', alpha=2, beta=2)
            
            # å·¦é€šé“è™•ç†å¼·åº¦ / Left channel processing strength
            # æ„ç¾©ï¼šå·¦é€šé“çš„è­‰æ“šç´¯ç©é€Ÿåº¦
            # Meaning: Evidence accumulation speed for left channel
            # ç¯„åœï¼šæ­£å€¼ï¼Œæ•¸å€¼è¶Šå¤§è™•ç†è¶Šå¿«
            # Range: positive values, higher means faster processing
            left_drift = pm.Gamma('left_drift', alpha=3, beta=1)
            
            # å³é€šé“è™•ç†å¼·åº¦ / Right channel processing strength
            # æ„ç¾©ï¼šå³é€šé“çš„è­‰æ“šç´¯ç©é€Ÿåº¦  
            # Meaning: Evidence accumulation speed for right channel
            right_drift = pm.Gamma('right_drift', alpha=3, beta=1)
            
            # å·¦é€šé“é›œè¨Šæ°´å¹³ / Left channel noise level
            # æ„ç¾©ï¼šå·¦é€šé“è™•ç†çš„è®Šç•°æ€§
            # Meaning: Variability in left channel processing
            noise_left = pm.Gamma('noise_left', alpha=2, beta=4)
            
            # å³é€šé“é›œè¨Šæ°´å¹³ / Right channel noise level
            # æ„ç¾©ï¼šå³é€šé“è™•ç†çš„è®Šç•°æ€§
            # Meaning: Variability in right channel processing  
            noise_right = pm.Gamma('noise_right', alpha=2, beta=4)
            
            # === ä¼¼ç„¶å‡½æ•¸å®šç¾© / Likelihood Function Definition ===
            
            # ç›®çš„ï¼šé€£æ¥è§€å¯Ÿæ•¸æ“šèˆ‡èªçŸ¥æ¨¡å‹
            # Purpose: Connect observed data with cognitive model
            
            # ç‚ºæ¯å€‹è©¦é©—è¨ˆç®—ä¼¼ç„¶ / Calculate likelihood for each trial
            likelihood_values = []
            
            for i in range(n_trials):
                # è¨ˆç®—å–®ä¸€è©¦é©—çš„LBAä¼¼ç„¶ / Calculate LBA likelihood for single trial
                # è¼¸å…¥ï¼šç•¶å‰è©¦é©—çš„åˆºæ¿€å’Œåæ‡‰æ•¸æ“š
                # Input: Current trial's stimulus and response data
                trial_likelihood = compute_dual_lba_likelihood(
                    left_tilt=left_tilt_data[i],     # å·¦ç·šæ¢å‚¾æ–œç‰¹å¾µ
                    right_tilt=right_tilt_data[i],   # å³ç·šæ¢å‚¾æ–œç‰¹å¾µ
                    choice=choice_data[i],           # å—è©¦è€…é¸æ“‡
                    rt=rt_data[i],                   # åæ‡‰æ™‚é–“
                    left_bias=left_bias,             # å·¦é€šé“åå¥½åƒæ•¸
                    right_bias=right_bias,           # å³é€šé“åå¥½åƒæ•¸
                    left_drift=left_drift,           # å·¦é€šé“æ¼‚ç§»ç‡
                    right_drift=right_drift,         # å³é€šé“æ¼‚ç§»ç‡
                    noise_left=noise_left,           # å·¦é€šé“é›œè¨Š
                    noise_right=noise_right          # å³é€šé“é›œè¨Š
                )
                likelihood_values.append(trial_likelihood)
            
            # ç¸½ä¼¼ç„¶ / Total likelihood
            # ç›®çš„ï¼šå°‡æ‰€æœ‰è©¦é©—çš„ä¼¼ç„¶çµ„åˆ
            # Purpose: Combine likelihood across all trials
            total_log_likelihood = pm.math.sum(pt.stack(likelihood_values))
            
            # å°‡ä¼¼ç„¶ç´å…¥æ¨¡å‹ / Include likelihood in model
            pm.Potential('lba_likelihood', total_log_likelihood)
            
        print(f"   è²è‘‰æ–¯æ¨¡å‹å»ºæ§‹å®Œæˆ / Bayesian model construction completed")
        
        # === MCMCæ¡æ¨£ / MCMC Sampling ===
        
        # ç›®çš„ï¼šå¾å¾Œé©—åˆ†ä½ˆä¸­æ¡æ¨£åƒæ•¸
        # Purpose: Sample parameters from posterior distribution
        
        with dual_lba_model:
            trace = pm.sample(
                draws=1000,           # æ¡æ¨£æ•¸é‡ / Number of samples
                tune=1000,            # èª¿æ•´æœŸ / Tuning period  
                chains=4,             # é¦¬å¯å¤«éˆæ•¸é‡ / Number of Markov chains
                target_accept=0.9,    # ç›®æ¨™æ¥å—ç‡ / Target acceptance rate
                cores=1,              # è¨ˆç®—æ ¸å¿ƒæ•¸ / Number of cores
                random_seed=42,       # éš¨æ©Ÿç¨®å­ / Random seed
                progressbar=True,     # é¡¯ç¤ºé€²åº¦æ¢ / Show progress bar
                return_inferencedata=True  # è¿”å›æ¨è«–æ•¸æ“šæ ¼å¼ / Return inference data format
            )
        
        print(f"   MCMCæ¡æ¨£å®Œæˆ / MCMC sampling completed")
        
        # === æ”¶æ–‚æ€§è¨ºæ–· / Convergence Diagnostics ===
        
        # ç›®çš„ï¼šæª¢æŸ¥æ¡æ¨£æ˜¯å¦æ”¶æ–‚åˆ°ç©©å®šåˆ†ä½ˆ
        # Purpose: Check if sampling converged to stable distribution
        
        try:
            summary = az.summary(trace)
            rhat_max = summary['r_hat'].max() if 'r_hat' in summary else 1.0
            ess_min = summary['ess_bulk'].min() if 'ess_bulk' in summary else 100
            
            # æ”¶æ–‚æ€§è­¦å‘Š / Convergence warnings
            convergence_ok = True
            if rhat_max > 1.05:
                print(f"   âš ï¸ æ”¶æ–‚è­¦å‘Š / Convergence warning: R-hat = {rhat_max:.3f}")
                convergence_ok = False
            if ess_min < 100:
                print(f"   âš ï¸ æ¡æ¨£è­¦å‘Š / Sampling warning: ESS = {ess_min:.0f}")
                convergence_ok = False
                
        except Exception as e:
            print(f"   è¨ºæ–·è¨ˆç®—è­¦å‘Š / Diagnostic calculation warning: {e}")
            rhat_max, ess_min = 1.05, 100
            convergence_ok = False
        
        # === çµæœæ•´ç† / Result Organization ===
        
        result = {
            'subject_id': subject_id,           # å—è©¦è€…ç·¨è™Ÿ
            'trace': trace,                     # MCMCæ¡æ¨£è»Œè·¡  
            'convergence': {                    # æ”¶æ–‚æ€§çµ±è¨ˆ
                'rhat_max': float(rhat_max),    # æœ€å¤§R-hatå€¼
                'ess_min': float(ess_min),      # æœ€å°æœ‰æ•ˆæ¨£æœ¬æ•¸
                'converged': convergence_ok     # æ˜¯å¦æ”¶æ–‚
            },
            'data_info': {                      # æ•¸æ“šè³‡è¨Š
                'n_trials': n_trials,          # è©¦é©—ç¸½æ•¸
                'choice_distribution': {        # é¸æ“‡åˆ†ä½ˆ
                    f'choice_{i}': int(np.sum(choice_data == i)) 
                    for i in range(4)
                },
                'mean_rt': float(np.mean(rt_data))  # å¹³å‡åæ‡‰æ™‚é–“
            },
            'model_type': 'dual_channel_lba',   # æ¨¡å‹é¡å‹
            'success': True                     # æˆåŠŸæ¨™è¨˜
        }
        
        status = "âœ… æ”¶æ–‚è‰¯å¥½" if convergence_ok else "âš ï¸ æ”¶æ–‚å•é¡Œ"
        print(f"{status} å—è©¦è€… {subject_id} åˆ†æå®Œæˆ / Subject {subject_id} analysis completed "
              f"(RÌ‚={rhat_max:.3f}, ESS={ess_min:.0f})")
        
        return result
        
    except Exception as e:
        print(f"âŒ å—è©¦è€… {subject_id} åˆ†æå¤±æ•— / Subject {subject_id} analysis failed: {e}")
        import traceback
        traceback.print_exc()
        return {
            'subject_id': subject_id, 
            'success': False, 
            'error': str(e),
            'model_type': 'dual_channel_lba'
        }

# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šä¸»è¦åˆ†æå™¨é¡åˆ¥
# Part 4: Main Analyzer Class  
# ============================================================================

class LineTiltGRTAnalyzer:
    """
    ç·šæ¢å‚¾æ–œåˆ¤æ–·ä»»å‹™çš„GRT-LBAåˆ†æå™¨
    GRT-LBA Analyzer for Line Tilt Judgment Task
    
    ç›®çš„ / Purpose:
    æ•´åˆæ•¸æ“šè¼‰å…¥ã€é è™•ç†ã€æ¨¡å‹åˆ†æå’Œçµæœå„²å­˜åŠŸèƒ½
    Integrate data loading, preprocessing, model analysis, and result saving
    """
    
    def __init__(self, csv_file: str = 'GRT_LBA.csv'):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        Initialize analyzer
        
        åƒæ•¸ / Parameters:
        - csv_file: CSVæ•¸æ“šæ–‡ä»¶è·¯å¾‘ / Path to CSV data file
        """
        
        print("="*60)
        print("ç·šæ¢å‚¾æ–œåˆ¤æ–·ä»»å‹™ - é›™é€šé“LBAåˆ†æå™¨")  
        print("Line Tilt Judgment Task - Dual-Channel LBA Analyzer")
        print("="*60)
        
        # è¼‰å…¥åŸå§‹æ•¸æ“š / Load raw data
        # ä¾†æºï¼šå¯¦é©—ç”¢ç”Ÿçš„CSVæ–‡ä»¶
        # Source: CSV file generated from experiment
        try:
            self.raw_df = pd.read_csv(csv_file)
            print(f"âœ… æ•¸æ“šè¼‰å…¥æˆåŠŸ / Data loaded successfully: {len(self.raw_df)} trials")
        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°æ•¸æ“šæ–‡ä»¶ / Data file not found: {csv_file}")
            raise
        except Exception as e:
            print(f"âŒ æ•¸æ“šè¼‰å…¥å¤±æ•— / Data loading failed: {e}")
            raise
        
        # æ•¸æ“šé è™•ç† / Data preprocessing
        # ç›®çš„ï¼šè½‰æ›åˆºæ¿€ç·¨ç¢¼ç‚ºç·šæ¢å‚¾æ–œç‰¹å¾µ
        # Purpose: Convert stimulus codes to line tilt features
        self.df = prepare_line_tilt_data(self.raw_df)
        
        # å—è©¦è€…åˆ—è¡¨ / Subject list
        # ä¾†æºï¼šparticipantæ¬„ä½çš„å”¯ä¸€å€¼
        # Source: Unique values from participant column
        self.participants = sorted(self.df['participant'].unique())
        print(f"âœ… ç™¼ç¾ {len(self.participants)} ä½å—è©¦è€… / Found {len(self.participants)} subjects")
        
        # é¡¯ç¤ºæ•¸æ“šæ‘˜è¦ / Show data summary
        self._show_data_summary()
    
    def _show_data_summary(self):
        """
        é¡¯ç¤ºæ•¸æ“šæ‘˜è¦çµ±è¨ˆ
        Show data summary statistics
        """
        
        print("\næ•¸æ“šæ‘˜è¦ / Data Summary:")
        print(f"  ç¸½è©¦é©—æ•¸ / Total trials: {len(self.df)}")
        print(f"  å¹³å‡åæ‡‰æ™‚é–“ / Mean RT: {self.df['RT'].mean():.3f}s")
        print(f"  æº–ç¢ºç‡ / Accuracy: {self.df['Correct'].mean():.3f}")
        
        print("\nåˆºæ¿€åˆ†ä½ˆ / Stimulus Distribution:")
        for stim in [1, 2, 3, 4]:
            count = len(self.df[self.df['Stimulus'] == stim])
            pct = count / len(self.df) * 100
            print(f"  åˆºæ¿€{stim}: {count} trials ({pct:.1f}%)")
        
        print("\næ¯ä½å—è©¦è€…æ•¸æ“šé‡ / Trials per subject:")
        for subj in self.participants:
            subj_trials = len(self.df[self.df['participant'] == subj])
            print(f"  å—è©¦è€…{subj}: {subj_trials} trials")
    
    def analyze_all_subjects(self) -> pd.DataFrame:
        """
        åˆ†ææ‰€æœ‰å—è©¦è€…
        Analyze all subjects
        
        è¿”å› / Returns:
        - results_df: åŒ…å«æ‰€æœ‰å—è©¦è€…åƒæ•¸ä¼°è¨ˆçš„DataFrame
        - results_df: DataFrame containing parameter estimates for all subjects
        """
        
        print("\né–‹å§‹æ‰¹é‡åˆ†æ / Starting batch analysis...")
        
        # å„²å­˜çµæœçš„åˆ—è¡¨ / List to store results
        results_list = []
        
        # é€ä¸€åˆ†ææ¯ä½å—è©¦è€… / Analyze each subject individually
        for i, subject_id in enumerate(self.participants, 1):
            print(f"\n[{i}/{len(self.participants)}] è™•ç†å—è©¦è€… {subject_id} / Processing Subject {subject_id}")
            
            # æå–è©²å—è©¦è€…çš„æ•¸æ“š / Extract data for this subject
            subject_data = self.df[self.df['participant'] == subject_id].copy()
            
            # åŸ·è¡Œåˆ†æ / Perform analysis
            result = analyze_line_tilt_subject(subject_id, subject_data)
            
            if result and result.get('success', False):
                # æˆåŠŸæƒ…æ³ï¼šæå–åƒæ•¸ä¼°è¨ˆ / Success case: extract parameter estimates
                trace = result['trace']
                
                # è¨ˆç®—å¾Œé©—çµ±è¨ˆ / Calculate posterior statistics
                posterior_stats = {}
                
                # åƒæ•¸åˆ—è¡¨ / Parameter list
                param_names = ['left_bias', 'right_bias', 'left_drift', 'right_drift', 
                              'noise_left', 'noise_right']
                
                for param in param_names:
                    if param in trace.posterior:
                        samples = trace.posterior[param].values.flatten()
                        posterior_stats[f'{param}_mean'] = float(np.mean(samples))
                        posterior_stats[f'{param}_std'] = float(np.std(samples))
                        posterior_stats[f'{param}_q025'] = float(np.percentile(samples, 2.5))
                        posterior_stats[f'{param}_q975'] = float(np.percentile(samples, 97.5))
                
                # çµ„åˆçµæœ / Combine results
                subject_result = {
                    'subject_id': subject_id,
                    'success': True,
                    'n_trials': result['data_info']['n_trials'],
                    'mean_rt': result['data_info']['mean_rt'],
                    'rhat_max': result['convergence']['rhat_max'],
                    'ess_min': result['convergence']['ess_min'],
                    'converged': result['convergence']['converged'],
                    **posterior_stats,
                    **{f"choice_{i}_count": result['data_info']['choice_distribution'][f'choice_{i}'] 
                       for i in range(4)}
                }
                
            else:
                # å¤±æ•—æƒ…æ³ / Failure case
                subject_result = {
                    'subject_id': subject_id,
                    'success': False,
                    'error': result.get('error', 'Unknown error') if result else 'Analysis failed',
                    'n_trials': len(subject_data),
                    'mean_rt': float(subject_data['RT'].mean()) if len(subject_data) > 0 else np.nan
                }
            
            results_list.append(subject_result)
        
        # è½‰æ›ç‚ºDataFrame / Convert to DataFrame
        results_df = pd.DataFrame(results_list)
        
        # é¡¯ç¤ºåˆ†æçµæœæ‘˜è¦ / Show analysis summary
        self._show_analysis_summary(results_df)
        
        return results_df
    
    def _show_analysis_summary(self, results_df: pd.DataFrame):
        """
        é¡¯ç¤ºåˆ†æçµæœæ‘˜è¦
        Show analysis results summary
        """
        
        print("\n" + "="*60)
        print("åˆ†æçµæœæ‘˜è¦ / Analysis Results Summary")
        print("="*60)
        
        # æˆåŠŸç‡çµ±è¨ˆ / Success rate statistics
        n_total = len(results_df)
        n_success = results_df['success'].sum()
        success_rate = n_success / n_total * 100
        
        print(f"ç¸½å—è©¦è€…æ•¸ / Total subjects: {n_total}")
        print(f"æˆåŠŸåˆ†ææ•¸ / Successful analyses: {n_success}")
        print(f"æˆåŠŸç‡ / Success rate: {success_rate:.1f}%")
        
        if n_success > 0:
            # æ”¶æ–‚æ€§çµ±è¨ˆ / Convergence statistics
            success_df = results_df[results_df['success'] == True]
            n_converged = success_df['converged'].sum() if 'converged' in success_df else 0
            
            print(f"æ”¶æ–‚è‰¯å¥½ / Well converged: {n_converged}/{n_success}")
            
            # åƒæ•¸çµ±è¨ˆ / Parameter statistics
            print("\nåƒæ•¸ä¼°è¨ˆæ‘˜è¦ / Parameter Estimates Summary:")
            
            param_base_names = ['left_bias', 'right_bias', 'left_drift', 'right_drift', 
                               'noise_left', 'noise_right']
            
            for param in param_base_names:
                mean_col = f'{param}_mean'
                if mean_col in success_df.columns:
                    values = success_df[mean_col].dropna()
                    if len(values) > 0:
                        print(f"  {param}: M = {values.mean():.3f}, SD = {values.std():.3f}, "
                              f"Range = [{values.min():.3f}, {values.max():.3f}]")
        
        # å¤±æ•—æ¡ˆä¾‹ / Failed cases
        failed_df = results_df[results_df['success'] == False]
        if len(failed_df) > 0:
            print(f"\nå¤±æ•—æ¡ˆä¾‹ / Failed cases: {list(failed_df['subject_id'].values)}")
    
    def save_results_to_csv(self, results_df: pd.DataFrame, filename: str = None):
        """
        å°‡çµæœå„²å­˜ç‚ºCSVæ–‡ä»¶
        Save results to CSV file
        
        åƒæ•¸ / Parameters:
        - results_df: åˆ†æçµæœDataFrame / Analysis results DataFrame
        - filename: è¼¸å‡ºæ–‡ä»¶å / Output filename
        """
        
        if filename is None:
            # ç”Ÿæˆæ™‚é–“æˆ³æ–‡ä»¶å / Generate timestamped filename
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"line_tilt_lba_results_{timestamp}.csv"
        
        try:
            # å„²å­˜ä¸»è¦çµæœ / Save main results
            results_df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"âœ… çµæœå·²å„²å­˜ / Results saved: {filename}")
            
            # å‰µå»ºæ‘˜è¦çµ±è¨ˆæ–‡ä»¶ / Create summary statistics file
            summary_filename = filename.replace('.csv', '_summary.csv')
            
            if results_df['success'].sum() > 0:
                success_df = results_df[results_df['success'] == True]
                
                # è¨ˆç®—åƒæ•¸æ‘˜è¦çµ±è¨ˆ / Calculate parameter summary statistics
                param_base_names = ['left_bias', 'right_bias', 'left_drift', 'right_drift', 
                                   'noise_left', 'noise_right']
                
                summary_stats = []
                for param in param_base_names:
                    mean_col = f'{param}_mean'
                    std_col = f'{param}_std'
                    
                    if mean_col in success_df.columns:
                        values = success_df[mean_col].dropna()
                        if len(values) > 0:
                            summary_stats.append({
                                'parameter': param,
                                'n_subjects': len(values),
                                'grand_mean': values.mean(),
                                'grand_std': values.std(),
                                'grand_min': values.min(),
                                'grand_max': values.max(),
                                'q25': values.quantile(0.25),
                                'q50': values.quantile(0.50),
                                'q75': values.quantile(0.75)
                            })
                
                if summary_stats:
                    summary_df = pd.DataFrame(summary_stats)
                    summary_df.to_csv(summary_filename, index=False, encoding='utf-8-sig')
                    print(f"âœ… æ‘˜è¦çµ±è¨ˆå·²å„²å­˜ / Summary statistics saved: {summary_filename}")
            
            return filename, summary_filename if 'summary_filename' in locals() else None
            
        except Exception as e:
            print(f"âŒ å„²å­˜å¤±æ•— / Save failed: {e}")
            return None, None

# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šä¸»åŸ·è¡Œç¨‹åº
# Part 5: Main Execution Program
# ============================================================================

def main():
    """
    ä¸»åŸ·è¡Œå‡½æ•¸
    Main execution function
    """
    
    start_time = time.time()
    
    try:
        # å‰µå»ºåˆ†æå™¨å¯¦ä¾‹ / Create analyzer instance
        analyzer = LineTiltGRTAnalyzer('GRT_LBA.csv')
        
        # åŸ·è¡Œæ‰¹é‡åˆ†æ / Perform batch analysis
        results_df = analyzer.analyze_all_subjects()
        
        # å„²å­˜çµæœ / Save results
        main_file, summary_file = analyzer.save_results_to_csv(results_df)
        
        # è¨ˆç®—ç¸½åŸ·è¡Œæ™‚é–“ / Calculate total execution time
        total_time = time.time() - start_time
        
        print("\n" + "="*60)
        print("åˆ†æå®Œæˆ / Analysis Completed")
        print("="*60)
        print(f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“ / Total execution time: {total_time/60:.1f} minutes")
        
        if main_file:
            print(f"ğŸ“Š ä¸»è¦çµæœæ–‡ä»¶ / Main results file: {main_file}")
        if summary_file:
            print(f"ğŸ“ˆ æ‘˜è¦çµ±è¨ˆæ–‡ä»¶ / Summary statistics file: {summary_file}")
        
        print("\nçµæœæ–‡ä»¶åŒ…å«ä»¥ä¸‹æ¬„ä½ / Result files contain the following columns:")
        print("  - subject_id: å—è©¦è€…ç·¨è™Ÿ / Subject ID")
        print("  - success: åˆ†ææ˜¯å¦æˆåŠŸ / Analysis success")
        print("  - n_trials: è©¦é©—æ•¸é‡ / Number of trials")
        print("  - mean_rt: å¹³å‡åæ‡‰æ™‚é–“ / Mean reaction time")
        print("  - rhat_max: æœ€å¤§R-hatå€¼ (æ”¶æ–‚æŒ‡æ¨™) / Max R-hat (convergence indicator)")
        print("  - ess_min: æœ€å°æœ‰æ•ˆæ¨£æœ¬æ•¸ / Minimum effective sample size")
        print("  - converged: æ˜¯å¦æ”¶æ–‚ / Converged")
        print("  - [param]_mean: åƒæ•¸å¾Œé©—å‡å€¼ / Parameter posterior mean")
        print("  - [param]_std: åƒæ•¸å¾Œé©—æ¨™æº–å·® / Parameter posterior standard deviation")
        print("  - [param]_q025/q975: 95%ä¿¡è³´å€é–“ / 95% credible interval")
        print("  - choice_[0-3]_count: å„é¸é …é¸æ“‡æ¬¡æ•¸ / Choice counts")
        
        return results_df
        
    except Exception as e:
        print(f"âŒ ç¨‹åºåŸ·è¡Œå¤±æ•— / Program execution failed: {e}")
        import traceback
        traceback.print_exc()
        return None

# ============================================================================
# ç¨‹åºå…¥å£é» / Program Entry Point
# ============================================================================

if __name__ == "__main__":
    # åŸ·è¡Œä¸»ç¨‹åº / Execute main program
    results = main()
