# -*- coding: utf-8 -*-
"""
ä¿®æ­£ç‰ˆå››é¸é … GRT-LBA åˆ†æç¨‹å¼ç¢¼
Fixed Four-Choice GRT-LBA Analysis Code

ä¸»è¦ä¿®æ­£ / Main Fixes:
1. ä¿®æ­£ PyMC æ¨¡å‹å®šç¾©éŒ¯èª¤ / Fix PyMC model definition errors
2. æ­£ç¢ºçš„ pm.Potential ä½¿ç”¨æ–¹å¼ / Correct pm.Potential usage
3. è©³ç´°çš„ç¨‹å¼ç¢¼è§£é‡‹ / Detailed code explanations
4. è®Šæ•¸ä¾†æºèˆ‡ç”¨é€”èªªæ˜ / Variable source and purpose explanations
"""

import numpy as np
import pandas as pd
import pymc as pm
import pytensor.tensor as pt
import arviz as az
import scipy.stats as stats
import time
import warnings
from pathlib import Path
from typing import Dict, Optional, List
import os

# é—œé–‰ä¸å¿…è¦çš„è­¦å‘Šè¨Šæ¯ / Suppress unnecessary warnings
warnings.filterwarnings('ignore')

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šLBA ä¼¼ç„¶å‡½æ•¸å¯¦ç¾
# Part 1: LBA Likelihood Function Implementation
# ============================================================================

def compute_lba_likelihood(rt_data, choice_data, stimloc_data, params):
    """
    è¨ˆç®— Linear Ballistic Accumulator (LBA) æ¨¡å‹çš„ä¼¼ç„¶å‡½æ•¸
    Compute the likelihood function for Linear Ballistic Accumulator (LBA) model
    
    åƒæ•¸èªªæ˜ / Parameters:
    - rt_data: åæ‡‰æ™‚é–“æ•¸æ“š / Reaction time data (ä¾†è‡ª CSV çš„ RT æ¬„ä½)
    - choice_data: é¸æ“‡åæ‡‰æ•¸æ“š / Choice response data (ä¾†è‡ª CSV çš„ Response æ¬„ä½)
    - stimloc_data: åˆºæ¿€ä½ç½®æ•¸æ“š / Stimulus location data (å¾ Stimulus æ¬„ä½è½‰æ›)
    - params: æ¨¡å‹åƒæ•¸ / Model parameters (db1, db2, sp, base_v)
    
    è¿”å›å€¼ / Returns:
    - å°æ•¸ä¼¼ç„¶å€¼ / Log-likelihood value
    
    è®Šæ•¸ä¾†æº / Variable Sources:
    - rt_data: å¾ self.df['RT'] æå–çš„åæ‡‰æ™‚é–“
    - choice_data: å¾ self.df['Response'] æå–çš„é¸æ“‡åæ‡‰
    - stimloc_data: å¾ self.df['Stimulus'] è½‰æ›çš„äºŒç¶­ä½ç½®åº§æ¨™
    - params: å¾ PyMC æ¨¡å‹æ¡æ¨£çš„åƒæ•¸å€¼
    """
    try:
        # è§£åŒ…æ¨¡å‹åƒæ•¸ / Unpack model parameters
        # db1: X è»¸æ±ºç­–é‚Šç•Œ / X-axis decision boundary (ç¯„åœ 0-1)
        # db2: Y è»¸æ±ºç­–é‚Šç•Œ / Y-axis decision boundary (ç¯„åœ 0-1)  
        # sp: æ„ŸçŸ¥é›œè¨Šåƒæ•¸ / Perceptual noise parameter (æ­£å€¼)
        # base_v: åŸºç¤æ¼‚ç§»ç‡ / Base drift rate (æ­£å€¼)
        db1, db2, sp, base_v = params
        
        # LBA æ¨¡å‹çš„å›ºå®šåƒæ•¸ / Fixed parameters for LBA model
        A = 0.4      # èµ·å§‹é»è®Šç•° / Start point variability
        s = 0.3      # æ¼‚ç§»ç‡è®Šç•° / Drift rate variability  
        t0 = 0.2     # éæ±ºç­–æ™‚é–“ / Non-decision time
        
        # è¨ˆç®—æ±ºç­–é–¾å€¼ / Calculate decision thresholds
        # b: æ±ºç­–é–¾å€¼ / Decision threshold (èµ·å§‹é» + é–¾å€¼åç§»)
        b = A + 0.5  # å›ºå®šé–¾å€¼åç§» / Fixed threshold offset
        thresholds = np.array([b, b, b, b])  # å››å€‹é¸é …çš„ç›¸åŒé–¾å€¼ / Same threshold for all four choices
        
        # åƒæ•¸æœ‰æ•ˆæ€§æª¢æŸ¥ / Parameter validity check
        # ç¢ºä¿æ„ŸçŸ¥é›œè¨Šå’ŒåŸºç¤æ¼‚ç§»ç‡ç‚ºæ­£å€¼ / Ensure perceptual noise and base drift rate are positive
        if sp <= 0 or base_v <= 0:
            return -1000.0  # è¿”å›æ¥µå°å€¼æ‡²ç½°ç„¡æ•ˆåƒæ•¸ / Return very small value to penalize invalid parameters
        
        # è¨ˆç®—æ±ºç­–æ™‚é–“ / Calculate decision time
        # å¾ç¸½åæ‡‰æ™‚é–“ä¸­æ¸›å»éæ±ºç­–æ™‚é–“ï¼Œä½†ä¸è¨­ç½®æœ€å°å€¼é™åˆ¶ / Subtract non-decision time from total reaction time, but no minimum limit
        # ä¿æŒåŸå§‹çš„æ±ºç­–æ™‚é–“è¨ˆç®— / Keep original decision time calculation
        rt_decision = np.maximum(rt_data - t0, 0.001)  # æ¥µå°çš„æœ€å°å€¼åƒ…é¿å…æ•¸å­¸éŒ¯èª¤ / Very small minimum only to avoid mathematical errors
        
        # åˆå§‹åŒ–å°æ•¸ä¼¼ç„¶ç¸½å’Œ / Initialize log-likelihood sum
        loglik_sum = 0.0
        
        # å°æ¯å€‹è©¦é©—è¨ˆç®—ä¼¼ç„¶ / Calculate likelihood for each trial
        for i in range(len(rt_decision)):
            # ç²å–ç•¶å‰è©¦é©—çš„é¸æ“‡ / Get current trial's choice
            choice_idx = int(choice_data[i])  # é¸æ“‡ç´¢å¼• (0,1,2,3) / Choice index (0,1,2,3)
            
            # æª¢æŸ¥é¸æ“‡æœ‰æ•ˆæ€§ / Check choice validity
            if choice_idx < 0 or choice_idx >= 4:
                continue  # è·³éç„¡æ•ˆé¸æ“‡ / Skip invalid choices
                
            # ç²å–ç•¶å‰è©¦é©—çš„æ±ºç­–æ™‚é–“ / Get current trial's decision time
            rt_trial = rt_decision[i]
            if rt_trial <= 0:
                continue  # è·³éç„¡æ•ˆæ™‚é–“ / Skip invalid times
            
            # === GRT (General Recognition Theory) è¨ˆç®—éƒ¨åˆ† ===
            # === GRT (General Recognition Theory) Calculation Section ===
            
            # ç²å–åˆºæ¿€ä½ç½® / Get stimulus location
            # stimloc_data[i, 0]: X è»¸ä½ç½® (0 æˆ– 1) / X-axis position (0 or 1)
            # stimloc_data[i, 1]: Y è»¸ä½ç½® (0 æˆ– 1) / Y-axis position (0 or 1)
            x_pos = stimloc_data[i, 0]  # ä¾†è‡ª Stimulus æ¬„ä½çš„ X åº§æ¨™è½‰æ› / X coordinate converted from Stimulus column
            y_pos = stimloc_data[i, 1]  # ä¾†è‡ª Stimulus æ¬„ä½çš„ Y åº§æ¨™è½‰æ› / Y coordinate converted from Stimulus column
            
            # è¨ˆç®—æ±ºç­–æ©Ÿç‡ / Calculate decision probabilities
            # ä½¿ç”¨ logistic å‡½æ•¸è¨ˆç®—é¸æ“‡å³å´çš„æ©Ÿç‡ / Use logistic function to calculate probability of choosing right
            # db1: X è»¸æ±ºç­–é‚Šç•Œ / X-axis decision boundary
            # db2: Y è»¸æ±ºç­–é‚Šç•Œ / Y-axis decision boundary  
            # sp: æ„ŸçŸ¥é›œè¨Šï¼Œæ§åˆ¶æ±ºç­–çš„ç¢ºå®šæ€§ / Perceptual noise, controls decision certainty
            p_choose_right_x = 1 / (1 + np.exp(-(x_pos - db1) / sp))
            p_choose_right_y = 1 / (1 + np.exp(-(y_pos - db2) / sp))
            
            # è¨ˆç®—å››é¸é …çš„æ©Ÿç‡ / Calculate probabilities for four choices
            # åŸºæ–¼ 2x2 ç©ºé–“çš„ä½ç½®æ©Ÿç‡çµ„åˆ / Based on position probability combinations in 2x2 space
            if choice_idx == 0:      # å·¦ä¸Š (0,0) / Top-left (0,0)
                choice_prob = (1 - p_choose_right_x) * (1 - p_choose_right_y)
            elif choice_idx == 1:    # å·¦ä¸‹ (0,1) / Bottom-left (0,1)
                choice_prob = (1 - p_choose_right_x) * p_choose_right_y
            elif choice_idx == 2:    # å³ä¸Š (1,0) / Top-right (1,0)
                choice_prob = p_choose_right_x * (1 - p_choose_right_y)
            else:                    # å³ä¸‹ (1,1) / Bottom-right (1,1)
                choice_prob = p_choose_right_x * p_choose_right_y
            
            # === LBA æ¨¡å‹è¨ˆç®—éƒ¨åˆ† ===
            # === LBA Model Calculation Section ===
            
            # è¨ˆç®—æ¼‚ç§»ç‡ / Calculate drift rates
            # v_chosen: è¢«é¸æ“‡é¸é …çš„æ¼‚ç§»ç‡ / Drift rate for chosen option
            # v_others: å…¶ä»–é¸é …çš„æ¼‚ç§»ç‡ / Drift rate for other options
            v_chosen = max(choice_prob * base_v, 0.1)  # æœ€å°å€¼ 0.1 é¿å…æ•¸å€¼å•é¡Œ / Minimum 0.1 to avoid numerical issues
            v_others = max((1 - choice_prob) * base_v / 3, 0.1)  # å¹³å‡åˆ†é…çµ¦å…¶ä»–ä¸‰å€‹é¸é … / Evenly distributed to other three options
            
            # LBA æ¨¡å‹çš„æ ¸å¿ƒè¨ˆç®— / Core calculation of LBA model
            sqrt_rt = np.sqrt(rt_trial)  # æ™‚é–“çš„å¹³æ–¹æ ¹ï¼Œç”¨æ–¼æ­£æ…‹åˆ†ä½ˆè¨ˆç®— / Square root of time for normal distribution calculation
            
            # è¨ˆç®—ç²å‹ç´¯åŠ å™¨çš„ä¼¼ç„¶ / Calculate likelihood for winning accumulator
            b_win = thresholds[choice_idx]  # ç²å‹é¸é …çš„é–¾å€¼ / Threshold for winning option
            
            # æ¨™æº–åŒ–è®Šæ•¸ / Standardized variables
            # z1, z2: ç”¨æ–¼è¨ˆç®—ç´¯ç©åˆ†ä½ˆå‡½æ•¸çš„æ¨™æº–åŒ–å€¼ / Standardized values for CDF calculation
            z1 = (v_chosen * rt_trial - b_win) / sqrt_rt
            z2 = (v_chosen * rt_trial - A) / sqrt_rt
            
            # é™åˆ¶æ•¸å€¼ç¯„åœé¿å…æº¢å‡º / Limit numerical range to avoid overflow
            z1 = np.clip(z1, -6, 6)
            z2 = np.clip(z2, -6, 6)
            
            try:
                # è¨ˆç®—ç²å‹è€…çš„ CDF å’Œ PDF / Calculate winner's CDF and PDF
                winner_cdf = stats.norm.cdf(z1) - stats.norm.cdf(z2)
                winner_pdf = (stats.norm.pdf(z1) - stats.norm.pdf(z2)) / sqrt_rt
                winner_lik = max((v_chosen / A) * winner_cdf + winner_pdf / A, 1e-10)
            except:
                winner_lik = 1e-10  # æ•¸å€¼è¨ˆç®—å¤±æ•—æ™‚çš„å‚™ç”¨å€¼ / Fallback value when numerical calculation fails
            
            # è¨ˆç®—å¤±æ•—ç´¯åŠ å™¨çš„ç”Ÿå­˜å‡½æ•¸ / Calculate survival function for losing accumulators
            loser_survival = 1.0
            for j in range(3):  # å…¶ä»–ä¸‰å€‹é¸é … / Other three options
                b_lose = thresholds[(choice_idx + j + 1) % 4]  # å¤±æ•—é¸é …çš„é–¾å€¼ / Threshold for losing option
                z1_lose = (v_others * rt_trial - b_lose) / sqrt_rt
                z2_lose = (v_others * rt_trial - A) / sqrt_rt
                
                # é™åˆ¶æ•¸å€¼ç¯„åœ / Limit numerical range
                z1_lose = np.clip(z1_lose, -6, 6)
                z2_lose = np.clip(z2_lose, -6, 6)
                
                try:
                    loser_cdf = stats.norm.cdf(z1_lose) - stats.norm.cdf(z2_lose)
                    loser_survival *= max(1 - loser_cdf, 1e-6)  # ç”Ÿå­˜æ©Ÿç‡ / Survival probability
                except:
                    loser_survival *= 0.5  # å‚™ç”¨å€¼ / Fallback value
            
            # è¨ˆç®—è©¦é©—çš„ç¸½ä¼¼ç„¶ / Calculate total likelihood for this trial
            trial_lik = winner_lik * loser_survival
            trial_loglik = np.log(max(trial_lik, 1e-12))  # è½‰æ›ç‚ºå°æ•¸ä¼¼ç„¶ / Convert to log-likelihood
            
            # ç´¯åŠ åˆ°ç¸½ä¼¼ç„¶ / Add to total likelihood
            if np.isfinite(trial_loglik):
                loglik_sum += trial_loglik
            else:
                loglik_sum += -10.0  # ç„¡æ•ˆå€¼çš„æ‡²ç½° / Penalty for invalid values
        
        # è¿”å›ç¸½å°æ•¸ä¼¼ç„¶ / Return total log-likelihood
        return loglik_sum if np.isfinite(loglik_sum) else -1000.0
        
    except Exception as e:
        print(f"ä¼¼ç„¶è¨ˆç®—éŒ¯èª¤ / Likelihood calculation error: {e}")
        return -1000.0

# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šPyTensor åŒ…è£å‡½æ•¸
# Part 2: PyTensor Wrapper Function  
# ============================================================================

def create_lba_logp_tensor(rt_data, choice_data, stimloc_data):
    """
    å‰µå»º PyTensor å…¼å®¹çš„ä¼¼ç„¶å‡½æ•¸
    Create PyTensor-compatible likelihood function
    
    åƒæ•¸èªªæ˜ / Parameters:
    - rt_data: åæ‡‰æ™‚é–“æ•¸æ“šé™£åˆ— / Reaction time data array
    - choice_data: é¸æ“‡æ•¸æ“šé™£åˆ— / Choice data array  
    - stimloc_data: åˆºæ¿€ä½ç½®æ•¸æ“šé™£åˆ— / Stimulus location data array
    
    è¿”å›å€¼ / Returns:
    - PyTensor æ“ä½œå‡½æ•¸ / PyTensor operation function
    
    ç”¨é€” / Purpose:
    - å°‡ç´” Python å‡½æ•¸åŒ…è£æˆ PyTensor å¯ç”¨çš„æ“ä½œ
    - Wrap pure Python function into PyTensor-compatible operation
    """
    
    # å®šç¾© PyTensor æ“ä½œ / Define PyTensor operation
    def lba_logp_op(params_tensor):
        """
        PyTensor æ“ä½œå‡½æ•¸ / PyTensor operation function
        
        åƒæ•¸ / Parameters:
        - params_tensor: åŒ…å«æ¨¡å‹åƒæ•¸çš„å¼µé‡ / Tensor containing model parameters
        
        è®Šæ•¸ä¾†æº / Variable Sources:
        - params_tensor: ä¾†è‡ª PyMC æ¨¡å‹çš„åƒæ•¸å¼µé‡ [db1, db2, sp, base_v]
        """
        
        # å°‡ PyTensor å¼µé‡è½‰æ›ç‚º NumPy æ•¸çµ„ / Convert PyTensor tensor to NumPy array
        params_np = params_tensor.eval() if hasattr(params_tensor, 'eval') else params_tensor
        
        # èª¿ç”¨ä¼¼ç„¶è¨ˆç®—å‡½æ•¸ / Call likelihood calculation function
        loglik = compute_lba_likelihood(rt_data, choice_data, stimloc_data, params_np)
        
        # è¿”å› PyTensor æ¨™é‡ / Return PyTensor scalar
        return pt.as_tensor_variable(loglik)
    
    return lba_logp_op

# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¿®æ­£çš„å—è©¦è€…åˆ†æå‡½æ•¸
# Part 3: Fixed Subject Analysis Function
# ============================================================================

def fixed_subject_analysis(subject_id: int, subject_data: pd.DataFrame) -> Optional[Dict]:
    """
    ä¿®æ­£ç‰ˆå—è©¦è€…åˆ†æå‡½æ•¸
    Fixed subject analysis function
    
    åƒæ•¸èªªæ˜ / Parameters:
    - subject_id: å—è©¦è€…ç·¨è™Ÿ / Subject ID (ä¾†è‡ª CSV çš„ participant æ¬„ä½)
    - subject_data: å—è©¦è€…æ•¸æ“š / Subject data (å¾ç¸½æ•¸æ“šä¸­éæ¿¾çš„ç‰¹å®šå—è©¦è€…æ•¸æ“š)
    
    è¿”å›å€¼ / Returns:
    - åˆ†æçµæœå­—å…¸æˆ– None / Analysis result dictionary or None
    
    è®Šæ•¸ä¾†æºèˆ‡ç”¨é€” / Variable Sources and Purposes:
    - subject_id: å¾ self.participants åˆ—è¡¨ä¸­ç²å–çš„å—è©¦è€…ç·¨è™Ÿ
    - subject_data: é€šé self.df[self.df['participant'] == subject_id] éæ¿¾çš„æ•¸æ“š
    """
    
    try:
        print(f"è™•ç†å—è©¦è€… {subject_id} / Processing Subject {subject_id}...")
        
        # === æ•¸æ“šæº–å‚™éšæ®µ / Data Preparation Phase ===
        
        # æå–åæ‡‰æ™‚é–“æ•¸æ“š / Extract reaction time data
        # ä¾†æºï¼šCSV æ–‡ä»¶çš„ RT æ¬„ä½ / Source: RT column from CSV file
        rt_data = subject_data['RT'].values
        
        # æå–é¸æ“‡åæ‡‰æ•¸æ“š / Extract choice response data  
        # ä¾†æºï¼šCSV æ–‡ä»¶çš„ Response æ¬„ä½ / Source: Response column from CSV file
        choice_data = subject_data['choice_four'].values
        
        # æå–åˆºæ¿€ä½ç½®æ•¸æ“š / Extract stimulus location data
        # ä¾†æºï¼šå¾ Stimulus æ¬„ä½è½‰æ›çš„ X, Y åº§æ¨™ / Source: X, Y coordinates converted from Stimulus column
        stimloc_data = np.column_stack([
            subject_data['stimloc_x'].values,  # X è»¸ä½ç½® / X-axis position
            subject_data['stimloc_y'].values   # Y è»¸ä½ç½® / Y-axis position
        ])
        
        # æª¢æŸ¥æ•¸æ“šé‡æ˜¯å¦è¶³å¤  / Check if data amount is sufficient
        if len(rt_data) < 50:
            print(f"   æ•¸æ“šä¸è¶³ / Insufficient data: {len(rt_data)} trials")
            return None
        
        # æ•¸æ“šæ¸…ç† / Data cleaning
        # ä¸å°åæ‡‰æ™‚é–“é€²è¡Œé™åˆ¶ï¼Œä¿æŒåŸå§‹æ•¸æ“š / Do not limit reaction time, keep original data
        # rt_data ä¿æŒåŸå§‹å€¼ / rt_data keeps original values
        choice_data = np.clip(choice_data, 0, 3)    # é¸æ“‡ç¯„åœ 0-3 / Choice range 0-3
        
        print(f"   æ•¸æ“šæº–å‚™å®Œæˆ / Data ready: {len(rt_data)} trials")
        
        # === PyMC æ¨¡å‹å®šç¾©éšæ®µ / PyMC Model Definition Phase ===
        
        with pm.Model() as model:
            
            # === å…ˆé©—åˆ†ä½ˆå®šç¾© / Prior Distribution Definition ===
            
            # GRT åƒæ•¸ï¼šæ±ºç­–é‚Šç•Œ / GRT parameters: Decision boundaries
            # db1: X è»¸æ±ºç­–é‚Šç•Œï¼Œç¯„åœ 0.2-0.8 / X-axis decision boundary, range 0.2-0.8
            # ç”¨é€”ï¼šæ±ºå®šåœ¨ X è»¸ä¸Šçš„åˆ†é¡é‚Šç•Œä½ç½® / Purpose: Determine classification boundary position on X-axis
            db1 = pm.Uniform('db1', lower=0.2, upper=0.8)
            
            # db2: Y è»¸æ±ºç­–é‚Šç•Œï¼Œç¯„åœ 0.2-0.8 / Y-axis decision boundary, range 0.2-0.8  
            # ç”¨é€”ï¼šæ±ºå®šåœ¨ Y è»¸ä¸Šçš„åˆ†é¡é‚Šç•Œä½ç½® / Purpose: Determine classification boundary position on Y-axis
            db2 = pm.Uniform('db2', lower=0.2, upper=0.8)
            
            # æ„ŸçŸ¥é›œè¨Šåƒæ•¸ (å°æ•¸å°ºåº¦) / Perceptual noise parameter (log scale)
            # ç”¨é€”ï¼šæ§åˆ¶æ±ºç­–çš„ç¢ºå®šæ€§ï¼Œå€¼è¶Šå°æ±ºç­–è¶Šç¢ºå®š / Purpose: Control decision certainty, smaller values mean more certain decisions
            log_sp = pm.Normal('log_sp', mu=np.log(0.3), sigma=0.5)
            sp = pm.Deterministic('sp', pt.exp(log_sp))  # è½‰æ›ç‚ºæ­£å€¼ / Transform to positive value
            
            # åŸºç¤æ¼‚ç§»ç‡åƒæ•¸ (å°æ•¸å°ºåº¦) / Base drift rate parameter (log scale)
            # ç”¨é€”ï¼šæ§åˆ¶åæ‡‰é€Ÿåº¦ï¼Œå€¼è¶Šå¤§åæ‡‰è¶Šå¿« / Purpose: Control response speed, larger values mean faster responses
            log_base_v = pm.Normal('log_base_v', mu=np.log(1.0), sigma=0.5)
            base_v = pm.Deterministic('base_v', pt.exp(log_base_v))  # è½‰æ›ç‚ºæ­£å€¼ / Transform to positive value
            
            # === è‡ªå®šç¾©ä¼¼ç„¶å‡½æ•¸å®šç¾© / Custom Likelihood Function Definition ===
            
            # çµ„åˆæ‰€æœ‰åƒæ•¸ / Combine all parameters
            # é€™äº›åƒæ•¸å°‡å‚³éçµ¦ä¼¼ç„¶å‡½æ•¸ / These parameters will be passed to likelihood function
            params = pt.stack([db1, db2, sp, base_v])
            
            # â­ é—œéµä¿®æ­£ï¼šæ­£ç¢ºä½¿ç”¨ pm.Potential â­
            # â­ Key Fix: Correct usage of pm.Potential â­
            
            # å®šç¾©ä¼¼ç„¶å‡½æ•¸ / Define likelihood function
            def logp_func(params_val):
                """
                è‡ªå®šç¾©å°æ•¸ä¼¼ç„¶å‡½æ•¸ / Custom log-likelihood function
                
                åƒæ•¸ / Parameters:
                - params_val: åƒæ•¸å€¼ [db1, db2, sp, base_v]
                
                è¿”å›å€¼ / Returns:
                - å°æ•¸ä¼¼ç„¶å€¼ / Log-likelihood value
                """
                return compute_lba_likelihood(rt_data, choice_data, stimloc_data, params_val)
            
            # ğŸ”§ ä¿®æ­£å‰çš„éŒ¯èª¤ç”¨æ³• / Previous incorrect usage:
            # likelihood = pm.Potential('likelihood', logp_func(params))  # âŒ é€™æœƒå°è‡´ 'float' object has no attribute 'name' éŒ¯èª¤
            
            # âœ… ä¿®æ­£å¾Œçš„æ­£ç¢ºç”¨æ³• / Corrected usage:
            # ä½¿ç”¨ pm.CustomDist ä¾†å®šç¾©è‡ªå®šç¾©åˆ†ä½ˆ / Use pm.CustomDist to define custom distribution
            likelihood = pm.CustomDist(
                'likelihood',
                params,  # åƒæ•¸å¼µé‡ / Parameter tensor
                logp=lambda value, params: compute_lba_likelihood(rt_data, choice_data, stimloc_data, params),
                observed=np.zeros(1)  # è§€æ¸¬å€¼ä½”ä½ç¬¦ / Observed value placeholder
            )
            
            # ğŸ’¡ æ›¿ä»£æ–¹æ¡ˆï¼šä½¿ç”¨ pm.DensityDist (å¦‚æœ CustomDist ä¸å¯ç”¨)
            # Alternative: Use pm.DensityDist (if CustomDist is not available)
            # likelihood = pm.DensityDist(
            #     'likelihood',
            #     lambda params: compute_lba_likelihood(rt_data, choice_data, stimloc_data, params),
            #     observed={'params': params}
            # )
        
        print(f"   æ¨¡å‹å»ºç«‹å®Œæˆï¼Œé–‹å§‹æ¡æ¨£ / Model built, starting sampling...")
        
        # === MCMC æ¡æ¨£éšæ®µ / MCMC Sampling Phase ===
        
        with model:
            # æ¡æ¨£è¨­å®š / Sampling configuration
            # draws: æ¡æ¨£æ•¸é‡ / Number of draws
            # tune: èª¿æ•´æ­¥æ•¸ / Number of tuning steps  
            # chains: éˆæ•¸é‡ / Number of chains
            # target_accept: ç›®æ¨™æ¥å—ç‡ / Target acceptance rate
            trace = pm.sample(
                draws=200,          # æ¡æ¨£æ•¸é‡ / Number of samples
                tune=200,           # èª¿æ•´æ­¥æ•¸ / Number of tuning steps
                chains=2,           # éˆæ•¸é‡ / Number of chains
                target_accept=0.8,  # ç›®æ¨™æ¥å—ç‡ / Target acceptance rate
                progressbar=True,   # é¡¯ç¤ºé€²åº¦æ¢ / Show progress bar
                return_inferencedata=True,  # è¿”å›æ¨è«–æ•¸æ“š / Return inference data
                cores=1,            # ä½¿ç”¨æ ¸å¿ƒæ•¸ / Number of cores to use
                random_seed=42      # éš¨æ©Ÿç¨®å­ / Random seed
            )
        
        print(f"   æ¡æ¨£å®Œæˆ / Sampling completed")
        
        # === æ”¶æ–‚æ€§è¨ºæ–·éšæ®µ / Convergence Diagnosis Phase ===
        
        try:
            # è¨ˆç®—æ”¶æ–‚æ€§çµ±è¨ˆé‡ / Calculate convergence statistics
            summary = az.summary(trace)
            
            # R-hat çµ±è¨ˆé‡ï¼šæ‡‰è©²æ¥è¿‘ 1.0ï¼Œè¡¨ç¤ºæ”¶æ–‚è‰¯å¥½ / R-hat statistic: should be close to 1.0 for good convergence
            rhat_max = summary['r_hat'].max() if 'r_hat' in summary else 1.0
            
            # æœ‰æ•ˆæ¨£æœ¬æ•¸ï¼šæ‡‰è©²è¶³å¤ å¤§ / Effective sample size: should be large enough
            ess_min = summary['ess_bulk'].min() if 'ess_bulk' in summary else 50
            
        except Exception as e:
            print(f"   æ”¶æ–‚æ€§è¨ºæ–·è­¦å‘Š / Convergence diagnosis warning: {e}")
            rhat_max, ess_min = 1.05, 50
        
        # === çµæœæ•´ç†éšæ®µ / Result Organization Phase ===
        
        result = {
            'subject_id': subject_id,                    # å—è©¦è€…ç·¨è™Ÿ / Subject ID
            'trace': trace,                              # MCMC æ¡æ¨£çµæœ / MCMC sampling results
            'convergence': {                             # æ”¶æ–‚æ€§çµ±è¨ˆ / Convergence statistics
                'rhat_max': float(rhat_max),             # æœ€å¤§ R-hat å€¼ / Maximum R-hat value
                'ess_min': float(ess_min)                # æœ€å°æœ‰æ•ˆæ¨£æœ¬æ•¸ / Minimum effective sample size
            },
            'n_trials': len(rt_data),                    # è©¦é©—æ•¸é‡ / Number of trials
            'success': True                              # æˆåŠŸæ¨™è¨˜ / Success flag
        }
        
        print(f"âœ… å—è©¦è€… {subject_id} å®Œæˆ / Subject {subject_id} completed "
              f"(RÌ‚={rhat_max:.3f}, ESS={ess_min:.0f})")
        
        return result
        
    except Exception as e:
        print(f"âŒ å—è©¦è€… {subject_id} å¤±æ•— / Subject {subject_id} failed: {e}")
        import traceback
        traceback.print_exc()
        return {'subject_id': subject_id, 'success': False, 'error': str(e)}

# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šä¸»è¦åˆ†æå™¨é¡åˆ¥
# Part 4: Main Analyzer Class
# ============================================================================

class FixedGRTAnalyzer:
    """
    ä¿®æ­£ç‰ˆ GRT åˆ†æå™¨
    Fixed GRT Analyzer
    
    ç”¨é€” / Purpose:
    - è¼‰å…¥å’Œé è™•ç† GRT å¯¦é©—æ•¸æ“š / Load and preprocess GRT experiment data
    - åŸ·è¡Œ GRT-LBA æ¨¡å‹åˆ†æ / Execute GRT-LBA model analysis
    - ç®¡ç†å¤šå€‹å—è©¦è€…çš„åˆ†ææµç¨‹ / Manage analysis workflow for multiple subjects
    """
    
    def __init__(self, csv_file: str = 'GRT_LBA.csv'):
        """
        åˆå§‹åŒ–åˆ†æå™¨ / Initialize analyzer
        
        åƒæ•¸ / Parameters:
        - csv_file: CSV æ•¸æ“šæ–‡ä»¶è·¯å¾‘ / CSV data file path
        """
        
        print("è¼‰å…¥æ•¸æ“š / Loading data...")
        
        # === æ•¸æ“šè¼‰å…¥éšæ®µ / Data Loading Phase ===
        
        # è®€å– CSV æ–‡ä»¶ / Read CSV file
        # ä¾†æºï¼šå¯¦é©—æ•¸æ“šæ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰å—è©¦è€…çš„è©¦é©—æ•¸æ“š / Source: Experiment data file containing all subjects' trial data
        self.df = pd.read_csv(csv_file)
        
        print(f"åŸå§‹æ•¸æ“š / Raw data: {len(self.df)} rows, {len(self.df.columns)} columns")
        print(f"æ¬„ä½åç¨± / Column names: {list(self.df.columns)}")
        
        # === æ•¸æ“šæ¸…ç†éšæ®µ / Data Cleaning Phase ===
        
        # ä¸å°åæ‡‰æ™‚é–“é€²è¡Œéæ¿¾ï¼Œä¿ç•™æ‰€æœ‰ RT æ•¸æ“š / Do not filter reaction times, keep all RT data
        # RT: åæ‡‰æ™‚é–“ï¼Œä¾†è‡ª CSV çš„ RT æ¬„ä½ï¼Œä¿æŒåŸå§‹æ•¸æ“š / Reaction time from RT column in CSV, keep original data
        print(f"RT ç¯„åœ / RT range: {self.df['RT'].min():.3f} - {self.df['RT'].max():.3f}s")
        
        # éæ¿¾ç„¡æ•ˆçš„åæ‡‰é¸æ“‡ / Filter invalid response choices
        # Response: é¸æ“‡åæ‡‰ï¼Œä¾†è‡ª CSV çš„ Response æ¬„ä½ / Choice response from Response column in CSV
        # æœ‰æ•ˆå€¼ï¼š0, 1, 2, 3 (å››å€‹é¸é …) / Valid values: 0, 1, 2, 3 (four choices)
        self.df = self.df[self.df['Response'].isin([0, 1, 2, 3])]
        print(f"éæ¿¾åæ‡‰é¸æ“‡å¾Œ / After response filtering: {len(self.df)} rows")
        
        # ===
