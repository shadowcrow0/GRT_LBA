# drift_rate_evidence_integration.py - ä¿®æ­£éŒ¯èª¤ç‰ˆæœ¬

import numpy as np
import pandas as pd
import pymc as pm
import arviz as az
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple

def diagnose_sampling_issues(trace, verbose=True):
    """è¨ºæ–·æ¡æ¨£å•é¡Œ"""
    
    issues = []
    
    try:
        # æª¢æŸ¥ç™¼æ•£æ¨£æœ¬
        if hasattr(trace, 'sample_stats'):
            divergences = trace.sample_stats.diverging.sum().values
            if divergences > 0:
                issues.append(f"ç™¼æ•£æ¨£æœ¬: {divergences}")
        
        # æª¢æŸ¥ R-hat
        try:
            rhat = az.rhat(trace)
            max_rhat = float(rhat.to_array().max())
            if max_rhat > 1.1:
                issues.append(f"R-hat éé«˜: {max_rhat:.3f}")
        except:
            issues.append("R-hat è¨ˆç®—å¤±æ•—")
        
        # æª¢æŸ¥æœ‰æ•ˆæ¨£æœ¬æ•¸
        try:
            ess = az.ess(trace)
            min_ess = float(ess.to_array().min())
            if min_ess < 100:
                issues.append(f"ESS éä½: {min_ess:.0f}")
        except:
            issues.append("ESS è¨ˆç®—å¤±æ•—")
        
        if verbose:
            if issues:
                print("âš ï¸ ç™¼ç¾æ¡æ¨£å•é¡Œ:")
                for issue in issues:
                    print(f"   - {issue}")
            else:
                print("âœ… æ¡æ¨£è¨ºæ–·é€šé")
        
        return issues
        
    except Exception as e:
        if verbose:
            print(f"âŒ è¨ºæ–·å¤±æ•—: {e}")
        return [f"è¨ºæ–·å¤±æ•—: {e}"]

class EvidenceIntegrationComparison:
    """åŸºæ–¼Single LBAçš„è­‰æ“šæ•´åˆæ¨¡å‹æ¯”è¼ƒå™¨ - ä¿®æ­£ç‰ˆæœ¬"""
    
    def __init__(self, mcmc_config=None):
        """åˆå§‹åŒ–è­‰æ“šæ•´åˆæ¯”è¼ƒå™¨"""
        
        self.mcmc_config = self._setup_mcmc_config(mcmc_config)
        
        # å›ºå®šçš„å…±äº«åƒæ•¸
        self.FIXED_PARAMS = {
            'shared_start_var': 0.35,
            'shared_threshold': 0.60,
            'shared_ndt': 0.22,
            'shared_noise': 0.25
        }
        
        print("âœ… åˆå§‹åŒ–è­‰æ“šæ•´åˆæ¨¡å‹æ¯”è¼ƒå™¨ (ä¿®æ­£ç‰ˆ)")
        print("   å›ºå®šåƒæ•¸:")
        for param, value in self.FIXED_PARAMS.items():
            print(f"     {param}: {value}")
    
    def _setup_mcmc_config(self, user_config):
        """è¨­å®šæ”¹é€²çš„MCMCé…ç½®"""
        
        # æ›´ä¿å®ˆçš„MCMCè¨­å®šä»¥é¿å…ç™¼æ•£
        default_config = {
            'draws': 600,               # å¢åŠ draws
            'tune': 800,                # å¢åŠ tune
            'chains': 4,                # å¢åŠ chains
            'cores': 1,                 # åºåˆ—æ¡æ¨£é¿å…ä¸¦è¡Œå•é¡Œ
            'target_accept': 0.95,      # æé«˜target_accept
            'max_treedepth': 12,        # å¢åŠ max_treedepth
            'init': 'adapt_diag',       # æ›´å¥½çš„åˆå§‹åŒ–
            'random_seed': [42, 43, 44, 45],  # æ¯æ¢éˆä¸åŒç¨®å­
            'progressbar': True,
            'return_inferencedata': True
        }
        
        if user_config:
            default_config.update(user_config)
        
        return default_config
    
    def prepare_subject_data(self, df, subject_id):
        """æº–å‚™å—è©¦è€…æ•¸æ“š"""
        
        # éæ¿¾å—è©¦è€…è³‡æ–™
        subject_df = df[df['participant'] == subject_id].copy()
        
        if len(subject_df) == 0:
            raise ValueError(f"æ‰¾ä¸åˆ°å—è©¦è€… {subject_id} çš„è³‡æ–™")
        
        # åˆºæ¿€æ˜ å°„
        stimulus_mapping = {
            0: {'left': 1, 'right': 0},  # å·¦å°è§’ï¼Œå³å‚ç›´
            1: {'left': 1, 'right': 1},  # å·¦å°è§’ï¼Œå³å°è§’
            2: {'left': 0, 'right': 0},  # å·¦å‚ç›´ï¼Œå³å‚ç›´
            3: {'left': 0, 'right': 1}   # å·¦å‚ç›´ï¼Œå³å°è§’
        }
        
        # é¸æ“‡æ˜ å°„
        choice_mapping = {
            0: {'left': 1, 'right': 0},  # é¸æ“‡ \|
            1: {'left': 1, 'right': 1},  # é¸æ“‡ \/
            2: {'left': 0, 'right': 0},  # é¸æ“‡ ||
            3: {'left': 0, 'right': 1}   # é¸æ“‡ |/
        }
        
        # åˆ†è§£åˆºæ¿€å’Œé¸æ“‡
        left_stimuli = []
        right_stimuli = []
        left_choices = []
        right_choices = []
        
        for _, row in subject_df.iterrows():
            stimulus = int(row['Stimulus'])
            choice = int(row['Response'])
            
            left_stimuli.append(stimulus_mapping[stimulus]['left'])
            right_stimuli.append(stimulus_mapping[stimulus]['right'])
            left_choices.append(choice_mapping[choice]['left'])
            right_choices.append(choice_mapping[choice]['right'])
        
        # è¨ˆç®—æº–ç¢ºç‡
        left_correct = np.array(left_choices) == np.array(left_stimuli)
        right_correct = np.array(right_choices) == np.array(right_stimuli)
        both_correct = left_correct & right_correct
        
        return {
            'subject_id': subject_id,
            'n_trials': len(subject_df),
            'choices': subject_df['Response'].values,
            'rt': subject_df['RT'].values,
            'stimuli': subject_df['Stimulus'].values,
            'left_stimuli': np.array(left_stimuli),
            'right_stimuli': np.array(right_stimuli),
            'left_choices': np.array(left_choices),
            'right_choices': np.array(right_choices),
            'left_correct': left_correct,
            'right_correct': right_correct,
            'accuracy': np.mean(both_correct),
            'left_accuracy': np.mean(left_correct),
            'right_accuracy': np.mean(right_correct)
        }
    
    def step1_estimate_single_lba(self, subject_data):
        """æ­¥é©Ÿ1: ä½¿ç”¨Single LBAä¼°è¨ˆå·¦å³é€šé“çš„ match/mismatch drift rates"""
        
        print("\nğŸ“ æ­¥é©Ÿ1: Single LBAä¼°è¨ˆå·¦å³é€šé“ match/mismatch drift rates")
        print("-" * 60)
        print("   ä½¿ç”¨å›ºå®šåƒæ•¸:")
        for param, value in self.FIXED_PARAMS.items():
            print(f"     {param}: {value}")
        
        with pm.Model() as single_lba_model:
            
            # === å›ºå®šçš„å…±äº«åƒæ•¸ ===
            shared_threshold = self.FIXED_PARAMS['shared_threshold']
            shared_start_var = self.FIXED_PARAMS['shared_start_var']
            shared_ndt = self.FIXED_PARAMS['shared_ndt']
            shared_noise = self.FIXED_PARAMS['shared_noise']
            
            # === æ”¹é€²çš„å…ˆé©—è¨­å®š ===
            # æ·»åŠ RTç¯„åœç´„æŸ
            min_drift = shared_threshold / (1.5 - shared_ndt)  # â‰ˆ 0.47
            max_drift = shared_threshold / (0.3 - shared_ndt)  # â‰ˆ 7.5
            
            # å·¦é€šé“çš„ match/mismatch drift rates
            left_drift_match = pm.TruncatedNormal(
                'left_drift_match', 
                mu=2.0, sigma=0.8, 
                lower=min_drift, upper=max_drift
            )
            left_drift_mismatch = pm.TruncatedNormal(
                'left_drift_mismatch', 
                mu=0.4, sigma=0.3, 
                lower=0.05, upper=2.0
            )
            
            # å³é€šé“çš„ match/mismatch drift rates
            right_drift_match = pm.TruncatedNormal(
                'right_drift_match', 
                mu=2.0, sigma=0.8, 
                lower=min_drift, upper=max_drift
            )
            right_drift_mismatch = pm.TruncatedNormal(
                'right_drift_mismatch', 
                mu=0.4, sigma=0.3, 
                lower=0.05, upper=2.0
            )
            
            # === è»Ÿç´„æŸ ===
            # åå¥½ match > mismatch
            pm.Potential(
                'left_match_advantage', 
                pm.math.log(1 + pm.math.exp(left_drift_match - left_drift_mismatch - 0.1))
            )
            pm.Potential(
                'right_match_advantage', 
                pm.math.log(1 + pm.math.exp(right_drift_match - right_drift_mismatch - 0.1))
            )
            
            # === æ•¸æ“šæº–å‚™ ===
            left_stimuli = subject_data['left_stimuli']
            left_choices = subject_data['left_choices']
            right_stimuli = subject_data['right_stimuli']
            right_choices = subject_data['right_choices']
            rt = subject_data['rt']
            
            # === è¨ˆç®—likelihood ===
            left_likelihood = self._compute_side_likelihood_match_mismatch(
                left_choices, left_stimuli, rt,
                left_drift_match, left_drift_mismatch, 
                shared_threshold, shared_start_var, shared_ndt, shared_noise, 'left'
            )
            
            right_likelihood = self._compute_side_likelihood_match_mismatch(
                right_choices, right_stimuli, rt,
                right_drift_match, right_drift_mismatch,
                shared_threshold, shared_start_var, shared_ndt, shared_noise, 'right'
            )
            
            # === æ·»åŠ åˆ°æ¨¡å‹ ===
            pm.Potential('left_likelihood', left_likelihood)
            pm.Potential('right_likelihood', right_likelihood)
        
        # åŸ·è¡ŒMCMCæ¡æ¨£
        print("   ğŸ² åŸ·è¡ŒSingle LBAæ¡æ¨£...")
        print("   ä½¿ç”¨æ”¹é€²çš„MCMCè¨­å®šï¼šé«˜target_accept, æ›´å¤šchains, æ›´å¤šiterations")
        
        with single_lba_model:
            single_trace = pm.sample(**self.mcmc_config)
        
        # æª¢æŸ¥æ”¶æ–‚
        issues = diagnose_sampling_issues(single_trace)
        if issues:
            print(f"   âš ï¸ Single LBAæ¡æ¨£æœ‰å•é¡Œ: {issues}")
            print("   ğŸ’¡ å»ºè­°ï¼šæª¢æŸ¥æ•¸æ“šå“è³ªæˆ–é€²ä¸€æ­¥èª¿æ•´MCMCè¨­å®š")
        else:
            print("   âœ… Single LBAæ¡æ¨£æˆåŠŸ")
        
        # æå–drift rateå¾Œé©—åˆ†å¸ƒ
        drift_estimates = self._extract_drift_estimates(single_trace)
        
        return single_lba_model, single_trace, drift_estimates
    
    def _compute_side_likelihood_match_mismatch(self, decisions, stimuli, rt, 
                                              drift_match, drift_mismatch, 
                                              threshold, start_var, ndt, noise, side_name):
        """è¨ˆç®—å–®é‚ŠLBA likelihood - ä½¿ç”¨ match/mismatch è¨­è¨ˆ"""
        
        from pytensor.tensor import erf
        
        # æ›´åš´æ ¼çš„åƒæ•¸ç´„æŸ
        drift_match = pm.math.maximum(drift_match, 0.1)
        drift_mismatch = pm.math.maximum(drift_mismatch, 0.05)
        # ç¢ºä¿ match > mismatch
        drift_match = pm.math.maximum(drift_match, drift_mismatch + 0.05)
        
        # è¨ˆç®—æ±ºç­–æ™‚é–“
        decision_time = pm.math.maximum(rt - ndt, 0.01)
        
        # åˆ¤æ–·åŒ¹é…æ€§
        stimulus_match = pm.math.eq(decisions, stimuli)
        
        # è¨­å®šdrift rates
        v_chosen = pm.math.where(stimulus_match, drift_match, drift_mismatch)
        v_unchosen = pm.math.where(stimulus_match, drift_mismatch, drift_match)
        
        # ä½¿ç”¨æ›´ç©©å®šçš„LBAè¨ˆç®—
        sqrt_t = pm.math.sqrt(decision_time)
        
        # Chosenç´¯ç©å™¨çš„z-scores (æ›´ä¿å®ˆçš„è£å‰ª)
        z1_chosen = pm.math.clip(
            (v_chosen * decision_time - threshold) / (noise * sqrt_t), 
            -4.0, 4.0  # æ›´ä¿å®ˆçš„è£å‰ªç¯„åœ
        )
        z2_chosen = pm.math.clip(
            (v_chosen * decision_time - start_var) / (noise * sqrt_t), 
            -4.0, 4.0
        )
        
        # Unchosenç´¯ç©å™¨çš„z-score
        z1_unchosen = pm.math.clip(
            (v_unchosen * decision_time - threshold) / (noise * sqrt_t), 
            -4.0, 4.0
        )
        
        def safe_normal_cdf(x):
            """å®‰å…¨çš„æ­£æ…‹CDFå‡½æ•¸"""
            x_safe = pm.math.clip(x, -4.0, 4.0)
            return 0.5 * (1 + erf(x_safe / pm.math.sqrt(2)))
        
        def safe_normal_pdf(x):
            """å®‰å…¨çš„æ­£æ…‹PDFå‡½æ•¸"""
            x_safe = pm.math.clip(x, -4.0, 4.0)
            return pm.math.exp(-0.5 * x_safe**2) / pm.math.sqrt(2 * np.pi)
        
        # Chosençš„ä¼¼ç„¶è¨ˆç®—
        chosen_cdf_term = safe_normal_cdf(z1_chosen) - safe_normal_cdf(z2_chosen)
        chosen_pdf_term = (safe_normal_pdf(z1_chosen) - safe_normal_pdf(z2_chosen)) / (noise * sqrt_t)
        
        # ç¢ºä¿CDFé …ç‚ºæ­£
        chosen_cdf_term = pm.math.maximum(chosen_cdf_term, 1e-10)
        
        # å®Œæ•´çš„chosenä¼¼ç„¶
        chosen_likelihood = pm.math.maximum(
            (v_chosen / start_var) * chosen_cdf_term + chosen_pdf_term / start_var,
            1e-10
        )
        
        # Unchosençš„å­˜æ´»æ©Ÿç‡
        unchosen_survival = pm.math.maximum(1 - safe_normal_cdf(z1_unchosen), 1e-10)
        
        # è¯åˆä¼¼ç„¶
        joint_likelihood = chosen_likelihood * unchosen_survival
        joint_likelihood = pm.math.maximum(joint_likelihood, 1e-12)
        
        # è½‰ç‚ºå°æ•¸ä¼¼ç„¶
        log_likelihood = pm.math.log(joint_likelihood)
        
        # è™•ç†ç„¡æ•ˆå€¼ - æ›´ä¿å®ˆçš„è£å‰ª
        log_likelihood_safe = pm.math.clip(log_likelihood, -50.0, 10.0)
        
        return pm.math.sum(log_likelihood_safe)
    
    def _extract_drift_estimates(self, trace):
        """æå–drift rateçš„å¾Œé©—ä¼°è¨ˆ - ä¿®æ­£ç‰ˆ"""
        
        summary = az.summary(trace)
        
        estimates = {
            # åŸå§‹drift rates
            'left_drift_match': summary.loc['left_drift_match', 'mean'],
            'left_drift_mismatch': summary.loc['left_drift_mismatch', 'mean'],
            'right_drift_match': summary.loc['right_drift_match', 'mean'],
            'right_drift_mismatch': summary.loc['right_drift_mismatch', 'mean'],
            
            # æœ‰æ„ç¾©çš„æŒ‡æ¨™
            'left_processing_speed': summary.loc['left_drift_match', 'mean'],
            'left_noise_level': summary.loc['left_drift_mismatch', 'mean'],
            'left_discrimination': (summary.loc['left_drift_match', 'mean'] - 
                                   summary.loc['left_drift_mismatch', 'mean']),
            'left_efficiency_ratio': (summary.loc['left_drift_match', 'mean'] / 
                                     summary.loc['left_drift_mismatch', 'mean']),
            
            'right_processing_speed': summary.loc['right_drift_match', 'mean'],
            'right_noise_level': summary.loc['right_drift_mismatch', 'mean'], 
            'right_discrimination': (summary.loc['right_drift_match', 'mean'] - 
                                    summary.loc['right_drift_mismatch', 'mean']),
            'right_efficiency_ratio': (summary.loc['right_drift_match', 'mean'] / 
                                      summary.loc['right_drift_mismatch', 'mean']),
            
            # è·¨é€šé“æ¯”è¼ƒ
            'discrimination_asymmetry': abs(
                (summary.loc['left_drift_match', 'mean'] - summary.loc['left_drift_mismatch', 'mean']) -
                (summary.loc['right_drift_match', 'mean'] - summary.loc['right_drift_mismatch', 'mean'])
            ),
            'processing_asymmetry': abs(
                summary.loc['left_drift_match', 'mean'] - summary.loc['right_drift_match', 'mean']
            )
        }
        
        # æ·»åŠ å›ºå®šåƒæ•¸
        estimates.update(self.FIXED_PARAMS)
        
        print(f"   ğŸ“Š ä¼°è¨ˆçµæœ:")
        print(f"     å·¦é€šé“ - è™•ç†é€Ÿåº¦: {estimates['left_processing_speed']:.3f}, å™ªéŸ³: {estimates['left_noise_level']:.3f}")
        print(f"     å³é€šé“ - è™•ç†é€Ÿåº¦: {estimates['right_processing_speed']:.3f}, å™ªéŸ³: {estimates['right_noise_level']:.3f}")
        print(f"     å·¦é€šé“è¾¨åˆ¥èƒ½åŠ›: {estimates['left_discrimination']:.3f} (æ•ˆç‡æ¯”: {estimates['left_efficiency_ratio']:.2f})")
        print(f"     å³é€šé“è¾¨åˆ¥èƒ½åŠ›: {estimates['right_discrimination']:.3f} (æ•ˆç‡æ¯”: {estimates['right_efficiency_ratio']:.2f})")
        print(f"     é€šé“ä¸å°ç¨±æ€§: è¾¨åˆ¥={estimates['discrimination_asymmetry']:.3f}, è™•ç†={estimates['processing_asymmetry']:.3f}")
        
        # å°ç¨±æ€§æª¢æŸ¥
        self._check_symmetry_assumption(estimates)
        
        return estimates
    
    def _check_symmetry_assumption(self, estimates):
        """æª¢æŸ¥å°ç¨±æ€§å‡è¨­"""
        
        print(f"\n   ğŸ” å°ç¨±æ€§å‡è¨­æª¢æŸ¥:")
        
        # è™•ç†é€Ÿåº¦å·®ç•°
        processing_diff = abs(estimates['left_processing_speed'] - estimates['right_processing_speed'])
        discrimination_diff = estimates['discrimination_asymmetry']
        
        print(f"     è™•ç†é€Ÿåº¦å·®ç•°: {processing_diff:.3f}")
        print(f"     è¾¨åˆ¥èƒ½åŠ›å·®ç•°: {discrimination_diff:.3f}")
        
        # åˆ¤æ–·å°ç¨±æ€§
        if processing_diff < 0.2 and discrimination_diff < 0.3:
            print(f"     âœ… æ”¯æŒå°ç¨±æ€§å‡è¨­ (å·®ç•°å°)")
            symmetry_support = True
        elif processing_diff < 0.5 and discrimination_diff < 0.6:
            print(f"     âš ï¸ å¼±æ”¯æŒå°ç¨±æ€§å‡è¨­ (å·®ç•°ä¸­ç­‰)")
            symmetry_support = False
        else:
            print(f"     âŒ ä¸æ”¯æŒå°ç¨±æ€§å‡è¨­ (å·®ç•°å¤§)")
            symmetry_support = False
        
        estimates['symmetry_supported'] = symmetry_support
        
        return symmetry_support

def quick_symmetry_validation(csv_file='GRT_LBA.csv', n_subjects=5):
    """å¿«é€Ÿå°ç¨±æ€§é©—è­‰"""
    
    print("ğŸ” å¿«é€Ÿå°ç¨±æ€§é©—è­‰")
    print("=" * 30)
    
    # ç°¡åŒ–çš„MCMCè¨­å®šç”¨æ–¼å¿«é€Ÿé©—è­‰
    quick_mcmc = {
        'draws': 300,
        'tune': 400,
        'chains': 4,
        'target_accept': 0.90,
        'progressbar': True
    }
    
    analyzer = EvidenceIntegrationComparison(mcmc_config=quick_mcmc)
    
    try:
        # è¼‰å…¥è³‡æ–™
        df = pd.read_csv(csv_file)
        df = df.dropna(subset=['Response', 'RT', 'Stimulus', 'participant'])
        df = df[(df['RT'] >= 0.1) & (df['RT'] <= 3.0)]
        
        # é¸æ“‡å—è©¦è€…
        subjects = df['participant'].unique()[:n_subjects]
        
        symmetry_results = []
        
        for subject_id in subjects:
            print(f"\nğŸ“ é©—è­‰å—è©¦è€… {subject_id}")
            
            try:
                subject_data = analyzer.prepare_subject_data(df, subject_id)
                
                if subject_data['accuracy'] < 0.5:
                    print(f"   âš ï¸ è·³éï¼šæº–ç¢ºç‡éä½ ({subject_data['accuracy']:.1%})")
                    continue
                
                # åªé€²è¡Œæ­¥é©Ÿ1çš„ä¼°è¨ˆ
                _, _, drift_estimates = analyzer.step1_estimate_single_lba(subject_data)
                
                symmetry_results.append({
                    'subject_id': subject_id,
                    'processing_diff': estimates['processing_asymmetry'],
                    'discrimination_diff': estimates['discrimination_asymmetry'],
                    'symmetry_supported': estimates['symmetry_supported']
                })
                
            except Exception as e:
                print(f"   âŒ å—è©¦è€… {subject_id} åˆ†æå¤±æ•—: {e}")
                continue
        
        # ç¸½çµå°ç¨±æ€§çµæœ
        if symmetry_results:
            support_count = sum(1 for r in symmetry_results if r['symmetry_supported'])
            print(f"\nğŸ¯ å°ç¨±æ€§é©—è­‰çµæœ:")
            print(f"   æˆåŠŸåˆ†æ: {len(symmetry_results)}/{len(subjects)} å—è©¦è€…")
            print(f"   æ”¯æŒå°ç¨±æ€§: {support_count}/{len(symmetry_results)} å—è©¦è€…")
            print(f"   æ”¯æŒæ¯”ä¾‹: {support_count/len(symmetry_results)*100:.1f}%")
            
            if support_count / len(symmetry_results) >= 0.6:
                print(f"   âœ… æ•´é«”æ”¯æŒå°ç¨±æ€§å‡è¨­ï¼Œå¯è€ƒæ…®ä½¿ç”¨å…±äº«å…ˆé©—")
            else:
                print(f"   âŒ æ•´é«”ä¸æ”¯æŒå°ç¨±æ€§å‡è¨­ï¼Œå»ºè­°ä¿æŒç¨ç«‹åƒæ•¸")
        
        return symmetry_results
        
    except Exception as e:
        print(f"âŒ å°ç¨±æ€§é©—è­‰å¤±æ•—: {e}")
        return None

def run_improved_single_lba_only(csv_file='GRT_LBA.csv', subject_id=None):
    """é‹è¡Œæ”¹é€²çš„å–®æ­¥é©ŸLBAä¼°è¨ˆ (ä¸é€²è¡Œè­‰æ“šæ•´åˆ)"""
    
    print("ğŸš€ æ”¹é€²çš„Single LBAä¼°è¨ˆ")
    print("=" * 40)
    
    try:
        # è¼‰å…¥è³‡æ–™
        df = pd.read_csv(csv_file)
        df = df.dropna(subset=['Response', 'RT', 'Stimulus', 'participant'])
        df = df[(df['RT'] >= 0.1) & (df['RT'] <= 3.0)]
        
        analyzer = EvidenceIntegrationComparison()
        
        # é¸æ“‡å—è©¦è€…
        if subject_id is None:
            # è‡ªå‹•é¸æ“‡æº–ç¢ºç‡æœ€é«˜çš„å—è©¦è€…
            best_subject = None
            best_accuracy = 0
            
            for sid in df['participant'].unique()[:10]:  # æª¢æŸ¥å‰10å€‹
                temp_data = analyzer.prepare_subject_data(df, sid)
                if temp_data['accuracy'] > best_accuracy and temp_data['n_trials'] >= 50:
                    best_accuracy = temp_data['accuracy']
                    best_subject = sid
            
            if best_subject is None:
                print("âŒ æ‰¾ä¸åˆ°åˆé©çš„å—è©¦è€…")
                return None
            
            subject_id = best_subject
            print(f"âœ… è‡ªå‹•é¸æ“‡å—è©¦è€… {subject_id} (æº–ç¢ºç‡: {best_accuracy:.1%})")
        
        # æº–å‚™æ•¸æ“šä¸¦åŸ·è¡Œä¼°è¨ˆ
        subject_data = analyzer.prepare_subject_data(df, subject_id)
        print(f"ğŸ“Š å—è©¦è€…è³‡æ–™: {subject_data['n_trials']} trials, æº–ç¢ºç‡ {subject_data['accuracy']:.1%}")
        
        # åŸ·è¡ŒSingle LBAä¼°è¨ˆ
        model, trace, estimates = analyzer.step1_estimate_single_lba(subject_data)
        
        print(f"\nâœ… Single LBAä¼°è¨ˆå®Œæˆ!")
        return {
            'success': True,
            'subject_id': subject_id,
            'estimates': estimates,
            'trace': trace,
            'model': model
        }
        
    except Exception as e:
        print(f"âŒ ä¼°è¨ˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}

if __name__ == "__main__":
    print("ğŸ¯ ä¿®æ­£ç‰ˆæœ¬é¸é …:")
    print("1. æ¸¬è©¦æ”¹é€²çš„Single LBAä¼°è¨ˆ")
    print("2. å¿«é€Ÿå°ç¨±æ€§é©—è­‰ (5å€‹å—è©¦è€…)")
    print("3. å–®ä¸€å—è©¦è€…å®Œæ•´ä¼°è¨ˆ")
    
    try:
        choice = input("\nè«‹é¸æ“‡ (1-3): ").strip()
        
        if choice == '1':
            print("\nğŸ§ª æ¸¬è©¦æ”¹é€²çš„Single LBAä¼°è¨ˆ...")
            result = run_improved_single_lba_only()
            
        elif choice == '2':
            print("\nğŸ” åŸ·è¡Œå¿«é€Ÿå°ç¨±æ€§é©—è­‰...")
            results = quick_symmetry_validation()
            
        elif choice == '3':
            subject_id = input("è«‹è¼¸å…¥å—è©¦è€…ID (æˆ–æŒ‰Enterè‡ªå‹•é¸æ“‡): ").strip()
            if not subject_id:
                subject_id = None
            else:
                subject_id = int(subject_id)
            
            print(f"\nğŸš€ é–‹å§‹å—è©¦è€…åˆ†æ...")
            result = run_improved_single_lba_only(subject_id=subject_id)
            
        else:
            print("ç„¡æ•ˆé¸æ“‡")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ åˆ†æè¢«ä¸­æ–·")
    except Exception as e:
        print(f"\nğŸ’¥ éŒ¯èª¤: {e}")
