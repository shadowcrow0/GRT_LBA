def diagnose_sampling_issues(trace, verbose=True):
    """
    è¨ºæ–·æ¡æ¨£å•é¡Œ (å…§å»ºç‰ˆæœ¬ï¼Œä¸ä¾è³´å¤–éƒ¨æ¨¡çµ„)
    """
    
    issues = []
    
    try:
        # æª¢æŸ¥ç™¼æ•£æ¨£æœ¬
        if hasattr(trace, 'sample_stats'):
            divergences = trace.sample_stats.diverging.sum().values
            if divergences > 0:
                issues.append(f"ç™¼æ•£æ¨£æœ¬: {divergences}")
        
        # æª¢æŸ¥ R-hat
        try:
            rhat = az.rhat(trace)
            max_rhat = float(rhat.to_array().max())
            if max_rhat > 1.1:
                issues.append(f"R-hat éé«˜: {max_rhat:.3f}")
        except:
            issues.append("R-hat è¨ˆç®—å¤±æ•—")
        
        # æª¢æŸ¥æœ‰æ•ˆæ¨£æœ¬æ•¸
        try:
            ess = az.ess(trace)
            min_ess = float(ess.to_array().min())
            if min_ess < 100:
                issues.append(f"ESS éä½: {min_ess:.0f}")
        except:
            issues.append("ESS è¨ˆç®—å¤±æ•—")
        
        if verbose:
            if issues:
                print("âš ï¸ ç™¼ç¾æ¡æ¨£å•é¡Œ:")
                for issue in issues:
                    print(f"   - {issue}")
            else:
                print("âœ… æ¡æ¨£è¨ºæ–·é€šé")
        
        return issues
        
    except Exception as e:
        if verbose:
            print(f"âŒ è¨ºæ–·å¤±æ•—: {e}")
        return [f"è¨ºæ–·å¤±æ•—: {e}"]# drift_rate_evidence_integration.py - åŸºæ–¼Single LBAçš„è­‰æ“šæ•´åˆæ¨¡å‹æ¯”è¼ƒ
# ä½¿ç”¨Bayes Factoræ¯”è¼ƒCoactive vs Parallel ANDå‡è¨­

import numpy as np
import pandas as pd
import pymc as pm
import arviz as az
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple

class EvidenceIntegrationComparison:
    """åŸºæ–¼Single LBAçš„è­‰æ“šæ•´åˆæ¨¡å‹æ¯”è¼ƒå™¨"""
    
    def __init__(self, mcmc_config=None):
        """
        åˆå§‹åŒ–è­‰æ“šæ•´åˆæ¯”è¼ƒå™¨
        
        Args:
            mcmc_config: MCMCé…ç½®å­—å…¸
        """
        
        self.mcmc_config = self._setup_mcmc_config(mcmc_config)
        
        print("âœ… åˆå§‹åŒ–è­‰æ“šæ•´åˆæ¨¡å‹æ¯”è¼ƒå™¨")
        print("   åˆ†ææµç¨‹:")
        print("     1. Single LBA: ä¼°è¨ˆå·¦å³é€šé“å„è‡ªçš„drift rate")
        print("     2. Coactive: è­‰æ“šç›¸åŠ  (drift_left + drift_right)")
        print("     3. Parallel AND: è­‰æ“šå–æœ€å¤§å€¼ (max(drift_left, drift_right))")
        print("     4. Bayes Factor: æ¯”è¼ƒå…©ç¨®æ•´åˆå‡è¨­")
    
    def _setup_mcmc_config(self, user_config):
        """è¨­å®šMCMCé…ç½®"""
        
        default_config = {
            'draws': 200,           # æ¸›å°‘æ¡æ¨£æ•¸
            'tune': 300,            # æ¸›å°‘èª¿æ•´æœŸ
            'chains': 2,
            'cores': 1,
            'target_accept': 0.95,
            'max_treedepth': 12,    # å¢åŠ æ¨¹æ·±åº¦
            'init': 'jitter+adapt_diag',
            'random_seed': 42,
            'progressbar': True,
            'return_inferencedata': True
        }
        
        if user_config:
            default_config.update(user_config)
        
        return default_config
    
    def prepare_subject_data(self, df, subject_id):
        """
        æº–å‚™å—è©¦è€…æ•¸æ“š (æ ¹æ“šä½ çš„GRT_LBA.csvæ¬„ä½çµæ§‹)
        
        ä½ çš„æ•¸æ“šæ¬„ä½:
        - Response: æœ€çµ‚é¸æ“‡ (0-3)
        - RT: åæ‡‰æ™‚é–“
        - participant: å—è©¦è€…ID  
        - Stimulus: åˆºæ¿€é¡å‹ (0-3)
        - Chanel1, Chanel2: å·¦å³é€šé“ä¿¡æ¯
        """
        
        # éæ¿¾å—è©¦è€…è³‡æ–™
        subject_df = df[df['participant'] == subject_id].copy()
        
        if len(subject_df) == 0:
            raise ValueError(f"æ‰¾ä¸åˆ°å—è©¦è€… {subject_id} çš„è³‡æ–™")
        
        # åˆºæ¿€æ˜ å°„ï¼šå°‡åˆºæ¿€ç·¨è™Ÿè½‰ç‚ºå·¦å³é€šé“çš„ç·šæ¢é¡å‹
        stimulus_mapping = {
            0: {'left': 1, 'right': 0},  # å·¦å°è§’ï¼Œå³å‚ç›´
            1: {'left': 1, 'right': 1},  # å·¦å°è§’ï¼Œå³å°è§’
            2: {'left': 0, 'right': 0},  # å·¦å‚ç›´ï¼Œå³å‚ç›´
            3: {'left': 0, 'right': 1}   # å·¦å‚ç›´ï¼Œå³å°è§’
        }
        
        # é¸æ“‡æ˜ å°„ï¼šå°‡é¸æ“‡ç·¨è™Ÿè½‰ç‚ºå·¦å³é€šé“çš„åˆ¤æ–·
        choice_mapping = {
            0: {'left': 1, 'right': 0},  # é¸æ“‡ \|
            1: {'left': 1, 'right': 1},  # é¸æ“‡ \/
            2: {'left': 0, 'right': 0},  # é¸æ“‡ ||
            3: {'left': 0, 'right': 1}   # é¸æ“‡ |/
        }
        
        # åˆ†è§£åˆºæ¿€å’Œé¸æ“‡
        left_stimuli = []
        right_stimuli = []
        left_choices = []
        right_choices = []
        
        for _, row in subject_df.iterrows():
            stimulus = int(row['Stimulus'])
            choice = int(row['Response'])
            
            # åˆ†è§£åˆºæ¿€
            left_stimuli.append(stimulus_mapping[stimulus]['left'])
            right_stimuli.append(stimulus_mapping[stimulus]['right'])
            
            # åˆ†è§£é¸æ“‡
            left_choices.append(choice_mapping[choice]['left'])
            right_choices.append(choice_mapping[choice]['right'])
        
        # è¨ˆç®—æº–ç¢ºç‡
        left_correct = np.array(left_choices) == np.array(left_stimuli)
        right_correct = np.array(right_choices) == np.array(right_stimuli)
        both_correct = left_correct & right_correct
        
        return {
            'subject_id': subject_id,
            'n_trials': len(subject_df),
            'choices': subject_df['Response'].values,
            'rt': subject_df['RT'].values,
            'stimuli': subject_df['Stimulus'].values,
            'left_stimuli': np.array(left_stimuli),
            'right_stimuli': np.array(right_stimuli),
            'left_choices': np.array(left_choices),
            'right_choices': np.array(right_choices),
            'left_correct': left_correct,
            'right_correct': right_correct,
            'accuracy': np.mean(both_correct),
            'left_accuracy': np.mean(left_correct),
            'right_accuracy': np.mean(right_correct)
        }
    
    def step1_estimate_single_lba(self, subject_data):
        """
        æ­¥é©Ÿ1: ä½¿ç”¨Single LBAä¼°è¨ˆå·¦å³é€šé“çš„drift rate
        å…±äº« threshold, start_var, ndt, noise åƒæ•¸
        """
        
        print("\nğŸ“ æ­¥é©Ÿ1: Single LBAä¼°è¨ˆå·¦å³é€šé“drift rate")
        print("-" * 50)
        
        with pm.Model() as single_lba_model:
            
            # === å…±äº«åƒæ•¸ (å·¦å³é€šé“å…±ç”¨) ===
            shared_threshold = pm.Gamma('shared_threshold', alpha=3.0, beta=3.5)
            shared_start_var = pm.Uniform('shared_start_var', lower=0.1, upper=0.7)
            shared_ndt = pm.Uniform('shared_ndt', lower=0.05, upper=0.6)
            shared_noise = pm.Gamma('shared_noise', alpha=2.5, beta=8.0)
            
            # === å·¦é€šé“ç¨ç«‹çš„drift rateåƒæ•¸ ===
            left_drift_correct = pm.Gamma('left_drift_correct', alpha=2.5, beta=1.2)
            left_drift_incorrect = pm.Gamma('left_drift_incorrect', alpha=2.0, beta=3.0)
            
            # === å³é€šé“ç¨ç«‹çš„drift rateåƒæ•¸ ===
            right_drift_correct = pm.Gamma('right_drift_correct', alpha=2.5, beta=1.2)
            right_drift_incorrect = pm.Gamma('right_drift_incorrect', alpha=2.0, beta=3.0)
            
            # === æ•¸æ“šæº–å‚™ ===
            left_stimuli = subject_data['left_stimuli']
            left_choices = subject_data['left_choices']
            right_stimuli = subject_data['right_stimuli']
            right_choices = subject_data['right_choices']
            rt = subject_data['rt']
            
            # === è¨ˆç®—lefté€šé“likelihood (ä½¿ç”¨å…±äº«åƒæ•¸) ===
            left_likelihood = self._compute_side_likelihood(
                left_choices, left_stimuli, rt,
                left_drift_correct, left_drift_incorrect, shared_threshold,
                shared_start_var, shared_ndt, shared_noise, 'left'
            )
            
            # === è¨ˆç®—righté€šé“likelihood (ä½¿ç”¨å…±äº«åƒæ•¸) ===
            right_likelihood = self._compute_side_likelihood(
                right_choices, right_stimuli, rt,
                right_drift_correct, right_drift_incorrect, shared_threshold,
                shared_start_var, shared_ndt, shared_noise, 'right'
            )
            
            # === æ·»åŠ åˆ°æ¨¡å‹ ===
            pm.Potential('left_likelihood', left_likelihood)
            pm.Potential('right_likelihood', right_likelihood)
        
        # åŸ·è¡ŒMCMCæ¡æ¨£
        print("   ğŸ² åŸ·è¡ŒSingle LBAæ¡æ¨£...")
        with single_lba_model:
            single_trace = pm.sample(**self.mcmc_config)
        
        # æª¢æŸ¥æ”¶æ–‚
        issues = diagnose_sampling_issues(single_trace)
        if issues:
            print(f"   âš ï¸ Single LBAæ¡æ¨£æœ‰å•é¡Œ: {issues}")
        else:
            print("   âœ… Single LBAæ¡æ¨£æˆåŠŸ")
        
        # æå–drift rateå¾Œé©—åˆ†å¸ƒ
        drift_estimates = self._extract_drift_estimates(single_trace)
        
        return single_lba_model, single_trace, drift_estimates
    
    def step2_test_evidence_integration(self, subject_data, drift_estimates):
        """
        æ­¥é©Ÿ2: ä½¿ç”¨ä¼°è¨ˆçš„drift rateæ¸¬è©¦å…©ç¨®è­‰æ“šæ•´åˆå‡è¨­
        """
        
        print("\nğŸ“ æ­¥é©Ÿ2: æ¸¬è©¦è­‰æ“šæ•´åˆå‡è¨­")
        print("-" * 50)
        
        # é¦–å…ˆå¾æ•¸æ“šä¼°è¨ˆè¦–è¦ºç‰¹å¾µçš„è™•ç†é›£åº¦ä¿‚æ•¸
        difficulty_coefficients = self._estimate_visual_difficulty_coefficients(subject_data, drift_estimates)
        
        # 2A. Coactiveæ¨¡å‹
        print("   ğŸ”¬ æ¸¬è©¦ Coactive å‡è¨­ (è­‰æ“šç›¸åŠ )...")
        coactive_model, coactive_trace = self._create_coactive_integration_model(
            subject_data, drift_estimates, difficulty_coefficients)
        
        # 2B. Parallel ANDæ¨¡å‹  
        print("   ğŸ”¬ æ¸¬è©¦ Parallel AND å‡è¨­ (è­‰æ“šå–æœ€å¤§å€¼)...")
        parallel_and_model, parallel_and_trace = self._create_parallel_and_integration_model(
            subject_data, drift_estimates, difficulty_coefficients)
        
        return {
            'coactive_model': coactive_model,
            'coactive_trace': coactive_trace,
            'parallel_and_model': parallel_and_model,
            'parallel_and_trace': parallel_and_trace,
            'difficulty_coefficients': difficulty_coefficients
        }
    
    def _create_coactive_integration_model(self, subject_data, drift_estimates, difficulty_coefficients):
        """å‰µå»ºCoactiveè­‰æ“šæ•´åˆæ¨¡å‹"""
        
        with pm.Model() as coactive_model:
            
            # === ä½¿ç”¨ä¼°è¨ˆçš„å·¦å³é€šé“drift rate (å›ºå®šå€¼) ===
            left_drift = drift_estimates['left_drift_mean']   # å·¦é€šé“çš„drift rate
            right_drift = drift_estimates['right_drift_mean'] # å³é€šé“çš„drift rate
            
            print(f"     ä½¿ç”¨ä¼°è¨ˆçš„drift rate: å·¦={left_drift:.3f}, å³={right_drift:.3f}")
            
            # === å›ºå®šçš„å››é¸ä¸€æ±ºç­–åƒæ•¸ ===
            choice_threshold = 1.0    # å›ºå®šé–¾å€¼
            choice_ndt = 0.2          # å›ºå®šéæ±ºç­–æ™‚é–“
            choice_noise = 0.3        # å›ºå®šå™ªéŸ³
            
            # === è¨ˆç®—Coactiveå‡è¨­ä¸‹çš„å››é¸ä¸€drift rates ===
            choices = subject_data['choices']
            rt = subject_data['rt']
            
            # æ ¹æ“šCoactiveå‡è¨­å’Œæ•¸æ“šé©…å‹•çš„é›£åº¦ä¿‚æ•¸è¨ˆç®—æ¯å€‹é¸é …çš„drift rate
            coactive_drift_rates = self._compute_coactive_choice_drifts(
                left_drift, right_drift, difficulty_coefficients)
            
            print(f"     Coactive drift rates: {coactive_drift_rates}")
            
            # === è¨ˆç®—å››é¸ä¸€é¸æ“‡çš„likelihood ===
            coactive_likelihood = self._compute_four_choice_likelihood(
                choices, rt, coactive_drift_rates, choice_threshold, choice_ndt, choice_noise
            )
            
            pm.Potential('coactive_likelihood', coactive_likelihood)
            
            # æ·»åŠ è§€å¯Ÿæ¨¡å‹ä»¥ä¾¿è¨ˆç®—WAIC
            trial_drift_rates = coactive_drift_rates[choices]  # æ¯å€‹trialå°æ‡‰çš„drift rate
            predicted_rt = choice_ndt + choice_threshold / trial_drift_rates
            pm.Normal('coactive_obs_rt', mu=predicted_rt, sigma=choice_noise, observed=rt)
        
        # åŸ·è¡Œæ¡æ¨£
        with coactive_model:
            coactive_trace = pm.sample(**self.mcmc_config)
        
        issues = diagnose_sampling_issues(coactive_trace)
        if not issues:
            print("     âœ… Coactiveæ¨¡å‹æ¡æ¨£æˆåŠŸ")
        else:
            print(f"     âš ï¸ Coactiveæ¨¡å‹æ¡æ¨£å•é¡Œ: {issues}")
        
        return coactive_model, coactive_trace
    
    def _create_parallel_and_integration_model(self, subject_data, drift_estimates, difficulty_coefficients):
        """å‰µå»ºParallel ANDè­‰æ“šæ•´åˆæ¨¡å‹"""
        
        with pm.Model() as parallel_and_model:
            
            # === ä½¿ç”¨ä¼°è¨ˆçš„å·¦å³é€šé“drift rate (å›ºå®šå€¼) ===
            left_drift = drift_estimates['left_drift_mean']   # å·¦é€šé“çš„drift rate
            right_drift = drift_estimates['right_drift_mean'] # å³é€šé“çš„drift rate
            
            print(f"     ä½¿ç”¨ä¼°è¨ˆçš„drift rate: å·¦={left_drift:.3f}, å³={right_drift:.3f}")
            
            # === å›ºå®šçš„å››é¸ä¸€æ±ºç­–åƒæ•¸ ===
            choice_threshold = 1.0    # å›ºå®šé–¾å€¼
            choice_ndt = 0.2          # å›ºå®šéæ±ºç­–æ™‚é–“
            choice_noise = 0.3        # å›ºå®šå™ªéŸ³
            
            # === è¨ˆç®—Parallel ANDå‡è¨­ä¸‹çš„å››é¸ä¸€drift rates ===
            choices = subject_data['choices']
            rt = subject_data['rt']
            
            # æ ¹æ“šParallel ANDå‡è¨­å’Œæ•¸æ“šé©…å‹•çš„é›£åº¦ä¿‚æ•¸è¨ˆç®—æ¯å€‹é¸é …çš„drift rate
            parallel_drift_rates = self._compute_parallel_and_choice_drifts(
                left_drift, right_drift, difficulty_coefficients)
            
            print(f"     Parallel AND drift rates: {parallel_drift_rates}")
            
            # === è¨ˆç®—å››é¸ä¸€é¸æ“‡çš„likelihood ===
            parallel_likelihood = self._compute_four_choice_likelihood(
                choices, rt, parallel_drift_rates, choice_threshold, choice_ndt, choice_noise
            )
            
            pm.Potential('parallel_likelihood', parallel_likelihood)
            
            # æ·»åŠ è§€å¯Ÿæ¨¡å‹ä»¥ä¾¿è¨ˆç®—WAIC
            trial_drift_rates = parallel_drift_rates[choices]  # æ¯å€‹trialå°æ‡‰çš„drift rate
            predicted_rt = choice_ndt + choice_threshold / trial_drift_rates
            pm.Normal('parallel_obs_rt', mu=predicted_rt, sigma=choice_noise, observed=rt)
        
        # åŸ·è¡Œæ¡æ¨£
        with parallel_and_model:
            parallel_trace = pm.sample(**self.mcmc_config)
        
        issues = diagnose_sampling_issues(parallel_trace)
        if not issues:
            print("     âœ… Parallel ANDæ¨¡å‹æ¡æ¨£æˆåŠŸ")
        else:
            print(f"     âš ï¸ Parallel ANDæ¨¡å‹æ¡æ¨£å•é¡Œ: {issues}")
        
        return parallel_and_model, parallel_trace
    
    def _compute_coactive_choice_drifts(self, left_drift, right_drift, difficulty_coefficients):
        """
        è¨ˆç®—Coactiveå‡è¨­ä¸‹å››å€‹é¸é …çš„drift rates
        ä½¿ç”¨å¾æ•¸æ“šä¼°è¨ˆçš„è¦–è¦ºç‰¹å¾µè™•ç†é›£åº¦ä¿‚æ•¸
        
        é¸é …å°æ‡‰:
        0: å·¦\å³| (å·¦å°è§’ + å³å‚ç›´)
        1: å·¦\å³/ (å·¦å°è§’ + å³å°è§’)  
        2: å·¦|å³| (å·¦å‚ç›´ + å³å‚ç›´)
        3: å·¦|å³/ (å·¦å‚ç›´ + å³å°è§’)
        
        Coactiveå‡è¨­: å…©å€‹é€šé“çš„è™•ç†èƒ½åŠ›ç›¸åŠ 
        """
        
        # æå–æ•¸æ“šé©…å‹•çš„é›£åº¦ä¿‚æ•¸
        left_vertical_coeff = difficulty_coefficients['left_vertical']
        left_diagonal_coeff = difficulty_coefficients['left_diagonal']
        right_vertical_coeff = difficulty_coefficients['right_vertical']
        right_diagonal_coeff = difficulty_coefficients['right_diagonal']
        
        # è¨ˆç®—æ¯å€‹é€šé“å°ä¸åŒè¦–è¦ºç‰¹å¾µçš„æœ‰æ•ˆè™•ç†èƒ½åŠ›
        left_vertical_strength = left_drift * left_vertical_coeff
        left_diagonal_strength = left_drift * left_diagonal_coeff
        right_vertical_strength = right_drift * right_vertical_coeff
        right_diagonal_strength = right_drift * right_diagonal_coeff
        
        # Coactive: ç›¸åŠ  (å…©å€‹é€šé“å”åŒå·¥ä½œ)
        drift_rates = np.array([
            left_diagonal_strength + right_vertical_strength,   # é¸é …0: å·¦\å³|
            left_diagonal_strength + right_diagonal_strength,   # é¸é …1: å·¦\å³/
            left_vertical_strength + right_vertical_strength,   # é¸é …2: å·¦|å³|
            left_vertical_strength + right_diagonal_strength    # é¸é …3: å·¦|å³/
        ])
        
        return drift_rates
    
    def _compute_parallel_and_choice_drifts(self, left_drift, right_drift, difficulty_coefficients):
        """
        è¨ˆç®—Parallel ANDå‡è¨­ä¸‹å››å€‹é¸é …çš„drift rates
        ä½¿ç”¨å¾æ•¸æ“šä¼°è¨ˆçš„è¦–è¦ºç‰¹å¾µè™•ç†é›£åº¦ä¿‚æ•¸
        
        Parallel ANDå‡è¨­: å–æœ€å¤§å€¼ (è¼ƒå¿«çš„é€šé“æ±ºå®šæ•´é«”é€Ÿåº¦)
        """
        
        # æå–æ•¸æ“šé©…å‹•çš„é›£åº¦ä¿‚æ•¸
        left_vertical_coeff = difficulty_coefficients['left_vertical']
        left_diagonal_coeff = difficulty_coefficients['left_diagonal']
        right_vertical_coeff = difficulty_coefficients['right_vertical']
        right_diagonal_coeff = difficulty_coefficients['right_diagonal']
        
        # è¨ˆç®—æ¯å€‹é€šé“å°ä¸åŒè¦–è¦ºç‰¹å¾µçš„æœ‰æ•ˆè™•ç†èƒ½åŠ›
        left_vertical_strength = left_drift * left_vertical_coeff
        left_diagonal_strength = left_drift * left_diagonal_coeff
        right_vertical_strength = right_drift * right_vertical_coeff
        right_diagonal_strength = right_drift * right_diagonal_coeff
        
        # Parallel AND: å–æœ€å¤§å€¼ (æœ€å¿«çš„é€šé“æ±ºå®š)
        drift_rates = np.array([
            max(left_diagonal_strength, right_vertical_strength),   # é¸é …0: å·¦\å³|
            max(left_diagonal_strength, right_diagonal_strength),   # é¸é …1: å·¦\å³/
            max(left_vertical_strength, right_vertical_strength),   # é¸é …2: å·¦|å³|
            max(left_vertical_strength, right_diagonal_strength)    # é¸é …3: å·¦|å³/
        ])
        
        return drift_rates
    
    def _compute_four_choice_likelihood(self, choices, rt, drift_rates, threshold, ndt, noise):
        """
        è¨ˆç®—å››é¸ä¸€é¸æ“‡çš„likelihood
        
        Args:
            choices: é¸æ“‡é™£åˆ— (0-3)
            rt: åæ‡‰æ™‚é–“é™£åˆ—
            drift_rates: å››å€‹é¸é …çš„drift rates [drift_0, drift_1, drift_2, drift_3]
            threshold, ndt, noise: LBAåƒæ•¸ (å›ºå®šå€¼)
        """
        
        # ç‚ºæ¯å€‹trialåˆ†é…å°æ‡‰çš„drift rate
        trial_drift_rates = drift_rates[choices]
        
        # è¨ˆç®—æ±ºç­–æ™‚é–“
        decision_time = np.maximum(rt - ndt, 0.01)
        
        # ç°¡åŒ–çš„LBA likelihoodè¨ˆç®—
        # å‡è¨­æ¯å€‹é¸é …éƒ½æœ‰ç›¸åŒçš„åƒæ•¸ï¼Œåªæœ‰drift rateä¸åŒ
        predicted_rt = ndt + threshold / trial_drift_rates
        
        # è¨ˆç®—RT likelihood (ä½¿ç”¨æ­£æ…‹åˆ†ä½ˆè¿‘ä¼¼)
        rt_likelihood = np.sum(
            -0.5 * ((rt - predicted_rt) / noise) ** 2 - np.log(noise * np.sqrt(2 * np.pi))
        )
        
        return rt_likelihood
    
    def _extract_drift_estimates(self, trace):
        """æå–drift rateçš„å¾Œé©—ä¼°è¨ˆ (é…åˆå…±äº«åƒæ•¸è¨­è¨ˆ)"""
        
        summary = az.summary(trace)
        
        # æå–åŸºæœ¬drift rateåƒæ•¸
        left_correct = summary.loc['left_drift_correct', 'mean']
        left_incorrect = summary.loc['left_drift_incorrect', 'mean']
        right_correct = summary.loc['right_drift_correct', 'mean']
        right_incorrect = summary.loc['right_drift_incorrect', 'mean']
        
        return {
            'left_drift_mean': (left_correct + left_incorrect) / 2,
            'right_drift_mean': (right_correct + right_incorrect) / 2,
            'left_drift_correct': left_correct,
            'left_drift_incorrect': left_incorrect,
            'right_drift_correct': right_correct,
            'right_drift_incorrect': right_incorrect,
            # æ·»åŠ å…±äº«åƒæ•¸
            'shared_threshold': summary.loc['shared_threshold', 'mean'],
            'shared_start_var': summary.loc['shared_start_var', 'mean'],
            'shared_ndt': summary.loc['shared_ndt', 'mean'],
            'shared_noise': summary.loc['shared_noise', 'mean']
        }
    
    def _estimate_visual_difficulty_coefficients(self, subject_data, drift_estimates):
        """
        å¾æ•¸æ“šä¼°è¨ˆå‚ç›´ç·š vs å°è§’ç·šçš„ç›¸å°è™•ç†é›£åº¦ä¿‚æ•¸
        
        æ–¹æ³•: åˆ†æå–®é€šé“å°ä¸åŒè¦–è¦ºç‰¹å¾µçš„è¡¨ç¾å·®ç•°
        """
        
        print("     ğŸ” å¾æ•¸æ“šä¼°è¨ˆè¦–è¦ºç‰¹å¾µè™•ç†é›£åº¦...")
        
        # åˆ†æå·¦é€šé“å°å‚ç›´ç·š vs å°è§’ç·šçš„è¡¨ç¾
        left_stimuli = subject_data['left_stimuli']
        left_choices = subject_data['left_choices']
        left_correct = subject_data['left_correct']
        
        # å·¦é€šé“ï¼šå‚ç›´ç·š (0) vs å°è§’ç·š (1) çš„æº–ç¢ºç‡
        left_vertical_trials = left_stimuli == 0
        left_diagonal_trials = left_stimuli == 1
        
        if np.sum(left_vertical_trials) > 0:
            left_vertical_acc = np.mean(left_correct[left_vertical_trials])
        else:
            left_vertical_acc = 0.5
            
        if np.sum(left_diagonal_trials) > 0:
            left_diagonal_acc = np.mean(left_correct[left_diagonal_trials])
        else:
            left_diagonal_acc = 0.5
        
        # åˆ†æå³é€šé“å°å‚ç›´ç·š vs å°è§’ç·šçš„è¡¨ç¾
        right_stimuli = subject_data['right_stimuli']
        right_choices = subject_data['right_choices']
        right_correct = subject_data['right_correct']
        
        right_vertical_trials = right_stimuli == 0
        right_diagonal_trials = right_stimuli == 1
        
        if np.sum(right_vertical_trials) > 0:
            right_vertical_acc = np.mean(right_correct[right_vertical_trials])
        else:
            right_vertical_acc = 0.5
            
        if np.sum(right_diagonal_trials) > 0:
            right_diagonal_acc = np.mean(right_correct[right_diagonal_trials])
        else:
            right_diagonal_acc = 0.5
        
        # è¨ˆç®—ç›¸å°é›£åº¦ä¿‚æ•¸ (ä»¥å‚ç›´ç·šç‚ºåŸºæº– = 1.0)
        # ä¿‚æ•¸ = è©²ç‰¹å¾µæº–ç¢ºç‡ / å‚ç›´ç·šæº–ç¢ºç‡
        
        # å·¦é€šé“ä¿‚æ•¸
        if left_vertical_acc > 0:
            left_vertical_coeff = 1.0  # åŸºæº–
            left_diagonal_coeff = left_diagonal_acc / left_vertical_acc
        else:
            left_vertical_coeff = 1.0
            left_diagonal_coeff = 1.0
        
        # å³é€šé“ä¿‚æ•¸
        if right_vertical_acc > 0:
            right_vertical_coeff = 1.0  # åŸºæº–
            right_diagonal_coeff = right_diagonal_acc / right_vertical_acc
        else:
            right_vertical_coeff = 1.0
            right_diagonal_coeff = 1.0
        
        # é™åˆ¶ä¿‚æ•¸ç¯„åœï¼Œé¿å…æ¥µç«¯å€¼
        left_diagonal_coeff = np.clip(left_diagonal_coeff, 0.5, 1.5)
        right_diagonal_coeff = np.clip(right_diagonal_coeff, 0.5, 1.5)
        
        coefficients = {
            'left_vertical': left_vertical_coeff,
            'left_diagonal': left_diagonal_coeff,
            'right_vertical': right_vertical_coeff,
            'right_diagonal': right_diagonal_coeff
        }
        
        print(f"     ğŸ“Š ä¼°è¨ˆçš„é›£åº¦ä¿‚æ•¸:")
        print(f"       å·¦é€šé“ - å‚ç›´ç·š: {left_vertical_coeff:.3f}, å°è§’ç·š: {left_diagonal_coeff:.3f}")
        print(f"       å³é€šé“ - å‚ç›´ç·š: {right_vertical_coeff:.3f}, å°è§’ç·š: {right_diagonal_coeff:.3f}")
        print(f"       å·¦é€šé“æº–ç¢ºç‡ - å‚ç›´ç·š: {left_vertical_acc:.1%}, å°è§’ç·š: {left_diagonal_acc:.1%}")
        print(f"       å³é€šé“æº–ç¢ºç‡ - å‚ç›´ç·š: {right_vertical_acc:.1%}, å°è§’ç·š: {right_diagonal_acc:.1%}")
        
        return coefficients
    
    def _compute_side_likelihood(self, decisions, stimuli, rt, drift_correct, drift_incorrect, 
                               threshold, start_var, ndt, noise, side_name):
        """
        è¨ˆç®—å–®é‚ŠLBA likelihood (ä½¿ç”¨å®Œæ•´çš„LBAå…¬å¼)
        é€™å€‹å‡½æ•¸è¨ˆç®—2é¸æ“‡LBAçš„å°æ•¸ä¼¼ç„¶ï¼Œç”¨æ–¼å·¦å³é€šé“å„è‡ªçš„å‚ç›´ç·švså°è§’ç·šåˆ¤æ–·
        
        Args:
            decisions: æ±ºç­–é™£åˆ— (0=å‚ç›´, 1=å°è§’)  
            stimuli: åˆºæ¿€é™£åˆ— (0=å‚ç›´, 1=å°è§’)
            rt: åæ‡‰æ™‚é–“é™£åˆ—
            drift_correct, drift_incorrect: æ­£ç¢ºå’ŒéŒ¯èª¤çš„drift rates
            threshold, start_var, ndt, noise: LBAåƒæ•¸
            side_name: é€šé“åç¨± (ç”¨æ–¼èª¿è©¦)
        """
        
        from pytensor.tensor import erf
        
        # åƒæ•¸ç´„æŸ
        drift_correct = pm.math.maximum(drift_correct, 0.1)
        drift_incorrect = pm.math.maximum(drift_incorrect, 0.05)
        drift_correct = pm.math.maximum(drift_correct, drift_incorrect + 0.05)
        threshold = pm.math.maximum(threshold, 0.1)
        start_var = pm.math.maximum(start_var, 0.05)
        ndt = pm.math.maximum(ndt, 0.05)
        noise = pm.math.maximum(noise, 0.1)
        
        # è¨ˆç®—æ±ºç­–æ™‚é–“
        decision_time = pm.math.maximum(rt - ndt, 0.01)
        
        # åˆ¤æ–·æ­£ç¢ºæ€§
        stimulus_correct = pm.math.eq(decisions, stimuli)
        
        # è¨­å®šwinnerå’Œloserçš„æ¼‚ç§»ç‡
        v_winner = pm.math.where(stimulus_correct, drift_correct, drift_incorrect)
        v_loser = pm.math.where(stimulus_correct, drift_incorrect, drift_correct)
        
        # ç¢ºä¿drift ratesæœ‰æœ€å°å€¼
        v_winner = pm.math.maximum(v_winner, 0.1)
        v_loser = pm.math.maximum(v_loser, 0.05)
        
        # ä½¿ç”¨å®Œæ•´çš„LBAå…¬å¼è¨ˆç®—2é¸æ“‡ä¼¼ç„¶
        sqrt_t = pm.math.sqrt(decision_time)
        
        # Winnerç´¯ç©å™¨çš„z-scores
        z1_winner = pm.math.clip(
            (v_winner * decision_time - threshold) / (noise * sqrt_t), 
            -4.5, 4.5
        )
        z2_winner = pm.math.clip(
            (v_winner * decision_time - start_var) / (noise * sqrt_t), 
            -4.5, 4.5
        )
        
        # Loserç´¯ç©å™¨çš„z-score
        z1_loser = pm.math.clip(
            (v_loser * decision_time - threshold) / (noise * sqrt_t), 
            -4.5, 4.5
        )
        
        def safe_normal_cdf(x):
            """å®‰å…¨çš„æ­£æ…‹CDFå‡½æ•¸"""
            x_safe = pm.math.clip(x, -4.5, 4.5)
            return 0.5 * (1 + erf(x_safe / pm.math.sqrt(2)))
        
        def safe_normal_pdf(x):
            """å®‰å…¨çš„æ­£æ…‹PDFå‡½æ•¸"""
            x_safe = pm.math.clip(x, -4.5, 4.5)
            return pm.math.exp(-0.5 * x_safe**2) / pm.math.sqrt(2 * np.pi)
        
        # Winnerçš„ä¼¼ç„¶è¨ˆç®—
        winner_cdf_term = safe_normal_cdf(z1_winner) - safe_normal_cdf(z2_winner)
        winner_pdf_term = (safe_normal_pdf(z1_winner) - safe_normal_pdf(z2_winner)) / (noise * sqrt_t)
        
        # ç¢ºä¿CDFé …ç‚ºæ­£
        winner_cdf_term = pm.math.maximum(winner_cdf_term, 1e-10)
        
        # å®Œæ•´çš„winnerä¼¼ç„¶
        winner_likelihood = pm.math.maximum(
            (v_winner / start_var) * winner_cdf_term + winner_pdf_term / start_var,
            1e-10
        )
        
        # Loserçš„å­˜æ´»æ©Ÿç‡
        loser_survival = pm.math.maximum(1 - safe_normal_cdf(z1_loser), 1e-10)
        
        # è¯åˆä¼¼ç„¶ï¼šwinnerçš„PDF Ã— loserçš„survival
        joint_likelihood = winner_likelihood * loser_survival
        joint_likelihood = pm.math.maximum(joint_likelihood, 1e-12)
        
        # è½‰ç‚ºå°æ•¸ä¼¼ç„¶
        log_likelihood = pm.math.log(joint_likelihood)
        
        # è™•ç†ç„¡æ•ˆå€¼ - ç›´æ¥è£å‰ªæ¥µç«¯å€¼
        log_likelihood_safe = pm.math.clip(log_likelihood, -100.0, 10.0)
        
        # è£å‰ªæ¥µç«¯å€¼ä¸¦æ±‚å’Œ
        return pm.math.sum(pm.math.clip(log_likelihood_safe, -100.0, 10.0))
    
    def step3_compute_bayes_factors(self, integration_results):
        """
        æ­¥é©Ÿ3: è¨ˆç®—Bayes Factoré€²è¡Œæ¨¡å‹æ¯”è¼ƒ
        """
        
        print("\nğŸ“ æ­¥é©Ÿ3: Bayes Factoræ¨¡å‹æ¯”è¼ƒ")
        print("-" * 50)
        
        coactive_trace = integration_results['coactive_trace']
        parallel_trace = integration_results['parallel_and_trace']
        
        try:
            # æ–¹æ³•1: å˜—è©¦è¨ˆç®—WAIC
            try:
                coactive_waic = az.waic(coactive_trace)
                parallel_waic = az.waic(parallel_trace)
                
                waic_diff = coactive_waic.waic - parallel_waic.waic
                waic_available = True
                
            except Exception as e:
                print(f"   âš ï¸ WAICè¨ˆç®—å¤±æ•—: {e}")
                waic_available = False
            
            # æ–¹æ³•2: å˜—è©¦è¨ˆç®—LOO
            try:
                coactive_loo = az.loo(coactive_trace)
                parallel_loo = az.loo(parallel_trace)
                
                loo_diff = coactive_loo.loo - parallel_loo.loo
                loo_available = True
                
            except Exception as e:
                print(f"   âš ï¸ LOOè¨ˆç®—å¤±æ•—: {e}")
                loo_available = False
            
            # æ–¹æ³•3: å‚™ç”¨è¨ˆç®— - ä½¿ç”¨ä¼¼ç„¶ä¼°è¨ˆ
            if not waic_available and not loo_available:
                print("   ğŸ”„ ä½¿ç”¨å‚™ç”¨æ–¹æ³•è¨ˆç®—æ¨¡å‹æ¯”è¼ƒ...")
                
                # è¨ˆç®—å¹³å‡å°æ•¸ä¼¼ç„¶
                try:
                    coactive_logp = coactive_trace.log_likelihood['coactive_obs_rt'].mean()
                    parallel_logp = parallel_trace.log_likelihood['parallel_obs_rt'].mean()
                    
                    likelihood_diff = float(coactive_logp.sum() - parallel_logp.sum())
                    
                    if likelihood_diff < -10:
                        conclusion = "å¼·çƒˆæ”¯æŒ Coactive å‡è¨­"
                    elif likelihood_diff < 0:
                        conclusion = "å‚¾å‘æ”¯æŒ Coactive å‡è¨­"
                    elif likelihood_diff > 10:
                        conclusion = "å¼·çƒˆæ”¯æŒ Parallel AND å‡è¨­"
                    else:
                        conclusion = "å‚¾å‘æ”¯æŒ Parallel AND å‡è¨­"
                    
                    comparison_results = {
                        'method': 'likelihood_comparison',
                        'likelihood_diff': likelihood_diff,
                        'conclusion': conclusion
                    }
                    
                except Exception as e:
                    print(f"   âŒ å‚™ç”¨æ–¹æ³•ä¹Ÿå¤±æ•—: {e}")
                    return None
            
            else:
                # ä½¿ç”¨WAICæˆ–LOOçµæœ
                if waic_available:
                    if waic_diff < -2:
                        waic_conclusion = "å¼·çƒˆæ”¯æŒ Coactive å‡è¨­"
                    elif waic_diff < 0:
                        waic_conclusion = "å‚¾å‘æ”¯æŒ Coactive å‡è¨­"
                    elif waic_diff > 2:
                        waic_conclusion = "å¼·çƒˆæ”¯æŒ Parallel AND å‡è¨­"
                    else:
                        waic_conclusion = "å‚¾å‘æ”¯æŒ Parallel AND å‡è¨­"
                else:
                    waic_diff = None
                    waic_conclusion = "WAICç„¡æ³•è¨ˆç®—"
                
                if loo_available:
                    if loo_diff < -2:
                        loo_conclusion = "å¼·çƒˆæ”¯æŒ Coactive å‡è¨­"
                    elif loo_diff < 0:
                        loo_conclusion = "å‚¾å‘æ”¯æŒ Coactive å‡è¨­"
                    elif loo_diff > 2:
                        loo_conclusion = "å¼·çƒˆæ”¯æŒ Parallel AND å‡è¨­"
                    else:
                        loo_conclusion = "å‚¾å‘æ”¯æŒ Parallel AND å‡è¨­"
                else:
                    loo_diff = None
                    loo_conclusion = "LOOç„¡æ³•è¨ˆç®—"
                
                comparison_results = {
                    'method': 'information_criteria',
                    'coactive_waic': coactive_waic.waic if waic_available else None,
                    'parallel_waic': parallel_waic.waic if waic_available else None,
                    'waic_diff': waic_diff,
                    'waic_conclusion': waic_conclusion,
                    'coactive_loo': coactive_loo.loo if loo_available else None,
                    'parallel_loo': parallel_loo.loo if loo_available else None,
                    'loo_diff': loo_diff,
                    'loo_conclusion': loo_conclusion
                }
            
            print(f"   ğŸ“Š æ¨¡å‹æ¯”è¼ƒçµæœ:")
            if comparison_results['method'] == 'information_criteria':
                if waic_available:
                    print(f"      Coactive WAIC:    {coactive_waic.waic:.2f}")
                    print(f"      Parallel WAIC:    {parallel_waic.waic:.2f}")
                    print(f"      WAIC å·®ç•°:        {waic_diff:.2f}")
                    print(f"      WAIC çµè«–:        {waic_conclusion}")
                if loo_available:
                    print(f"      LOO å·®ç•°:         {loo_diff:.2f}")
                    print(f"      LOO çµè«–:         {loo_conclusion}")
            else:
                print(f"      ä¼¼ç„¶å·®ç•°:         {comparison_results['likelihood_diff']:.2f}")
                print(f"      çµè«–:            {comparison_results['conclusion']}")
            
            return comparison_results
            
        except Exception as e:
            print(f"   âŒ Bayes Factorè¨ˆç®—å¤±æ•—: {e}")
            return None

def _get_final_conclusion(bayes_results):
    """ç²å–æœ€çµ‚çµè«–"""
    if not bayes_results:
        return "ç„¡æ³•åˆ¤æ–·"
    
    if bayes_results['method'] == 'information_criteria':
        if 'waic_conclusion' in bayes_results and bayes_results['waic_conclusion'] != "WAICç„¡æ³•è¨ˆç®—":
            return bayes_results['waic_conclusion']
        elif 'loo_conclusion' in bayes_results and bayes_results['loo_conclusion'] != "LOOç„¡æ³•è¨ˆç®—":
            return bayes_results['loo_conclusion']
        else:
            return "ç„¡æ³•åˆ¤æ–·"
    else:
        return bayes_results.get('conclusion', 'ç„¡æ³•åˆ¤æ–·')

def run_evidence_integration_analysis(csv_file='GRT_LBA.csv', subject_id=None, min_accuracy=0.5):
    """
    åŸ·è¡Œå®Œæ•´çš„è­‰æ“šæ•´åˆåˆ†æ
    
    Args:
        csv_file: æ•¸æ“šæª”æ¡ˆè·¯å¾‘
        subject_id: å—è©¦è€…IDï¼Œå¦‚æœNoneå‰‡è‡ªå‹•é¸æ“‡ç¬¦åˆæ¢ä»¶çš„å—è©¦è€…
        min_accuracy: æœ€ä½æº–ç¢ºç‡è¦æ±‚ (é è¨­50%)
    """
    
    print("ğŸš€ è­‰æ“šæ•´åˆå‡è¨­æª¢é©—åˆ†æ")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        # è¼‰å…¥è³‡æ–™ - ç›´æ¥ä½¿ç”¨pandasï¼Œä¸ä¾è³´DataProcessor
        print("ğŸ“‚ è¼‰å…¥è³‡æ–™...")
        df = pd.read_csv(csv_file)
        print(f"âœ… è¼‰å…¥ {len(df)} å€‹è©¦é©—")
        
        # åŸºæœ¬è³‡æ–™æ¸…ç†
        df = df.dropna(subset=['Response', 'RT', 'Stimulus', 'participant'])
        df = df[(df['RT'] >= 0.1) & (df['RT'] <= 3.0)]
        df = df[df['Response'].isin([0, 1, 2, 3])]
        df = df[df['Stimulus'].isin([0, 1, 2, 3])]
        
        print(f"âœ… æ¸…ç†å¾Œ: {len(df)} å€‹è©¦é©—")
        print(f"   å—è©¦è€…æ•¸: {df['participant'].nunique()}")
        
        # å‰µå»ºåˆ†æå™¨
        analyzer = EvidenceIntegrationComparison()
        
        # é¸æ“‡å—è©¦è€…ä¸¦æª¢æŸ¥æº–ç¢ºç‡
        if subject_id is None:
            # è‡ªå‹•é¸æ“‡ç¬¦åˆæ¢ä»¶çš„å—è©¦è€…
            suitable_subjects = []
            
            print(f"\nğŸ” å°‹æ‰¾æº–ç¢ºç‡ â‰¥ {min_accuracy:.0%} çš„å—è©¦è€…...")
            
            for sid in df['participant'].unique():
                temp_data = analyzer.prepare_subject_data(df, sid)
                if temp_data['accuracy'] >= min_accuracy and temp_data['n_trials'] >= 50:
                    suitable_subjects.append({
                        'id': sid,
                        'accuracy': temp_data['accuracy'],
                        'n_trials': temp_data['n_trials']
                    })
            
            if not suitable_subjects:
                print(f"âŒ æ‰¾ä¸åˆ°æº–ç¢ºç‡ â‰¥ {min_accuracy:.0%} ä¸”è©¦é©—æ•¸ â‰¥ 50 çš„å—è©¦è€…")
                print("   å»ºè­°é™ä½æº–ç¢ºç‡è¦æ±‚æˆ–æª¢æŸ¥æ•¸æ“šå“è³ª")
                return {
                    'success': False,
                    'error': f'No subjects with accuracy >= {min_accuracy:.0%}',
                    'total_time': time.time() - start_time
                }
            
            # é¸æ“‡æº–ç¢ºç‡æœ€é«˜çš„å—è©¦è€…
            best_subject = max(suitable_subjects, key=lambda x: x['accuracy'])
            subject_id = best_subject['id']
            
            print(f"âœ… æ‰¾åˆ° {len(suitable_subjects)} ä½ç¬¦åˆæ¢ä»¶çš„å—è©¦è€…")
            print(f"   è‡ªå‹•é¸æ“‡å—è©¦è€… {subject_id} (æº–ç¢ºç‡: {best_subject['accuracy']:.1%}, è©¦é©—æ•¸: {best_subject['n_trials']})")
            
        else:
            # æª¢æŸ¥æŒ‡å®šå—è©¦è€…æ˜¯å¦ç¬¦åˆæ¢ä»¶
            temp_data = analyzer.prepare_subject_data(df, subject_id)
            if temp_data['accuracy'] < min_accuracy:
                print(f"âŒ å—è©¦è€… {subject_id} æº–ç¢ºç‡ {temp_data['accuracy']:.1%} < {min_accuracy:.0%}")
                print("   è·³éåˆ†æï¼Œå»ºè­°é¸æ“‡å…¶ä»–å—è©¦è€…")
                return {
                    'success': False,
                    'error': f'Subject {subject_id} accuracy {temp_data["accuracy"]:.1%} below threshold',
                    'total_time': time.time() - start_time
                }
        
        # æº–å‚™æ•¸æ“š
        subject_data = analyzer.prepare_subject_data(df, subject_id)
        print(f"\nğŸ“Š å—è©¦è€… {subject_id} æ•¸æ“šåˆ†æ:")
        print(f"   è©¦é©—æ•¸: {subject_data['n_trials']}")
        print(f"   æ•´é«”æº–ç¢ºç‡: {subject_data['accuracy']:.1%}")
        print(f"   å·¦é€šé“æº–ç¢ºç‡: {subject_data['left_accuracy']:.1%}")
        print(f"   å³é€šé“æº–ç¢ºç‡: {subject_data['right_accuracy']:.1%}")
        
        # æª¢æŸ¥æ•¸æ“šåˆ†å¸ƒ
        print(f"   åˆºæ¿€åˆ†å¸ƒ: {np.bincount(subject_data['stimuli'])}")
        print(f"   é¸æ“‡åˆ†å¸ƒ: {np.bincount(subject_data['choices'])}")
        
        # æ­¥é©Ÿ1: Single LBAä¼°è¨ˆ
        print(f"\nğŸ“ é–‹å§‹ä¸‰æ­¥é©Ÿåˆ†æ...")
        single_model, single_trace, drift_estimates = analyzer.step1_estimate_single_lba(subject_data)
        
        # æ­¥é©Ÿ2: è­‰æ“šæ•´åˆæ¸¬è©¦
        integration_results = analyzer.step2_test_evidence_integration(subject_data, drift_estimates)
        
        # æ­¥é©Ÿ3: Bayes Factoræ¯”è¼ƒ
        bayes_results = analyzer.step3_compute_bayes_factors(integration_results)
        
        total_time = time.time() - start_time
        
        print(f"\n{'='*60}")
        print("ğŸ‰ è­‰æ“šæ•´åˆåˆ†æå®Œæˆ!")
        print(f"â±ï¸ ç¸½æ™‚é–“: {total_time/60:.1f} åˆ†é˜")
        print(f"ğŸ† æœ€çµ‚çµè«–: {_get_final_conclusion(bayes_results)}")
        print("="*60)
        
        return {
            'subject_id': subject_id,
            'subject_accuracy': subject_data['accuracy'],
            'n_trials': subject_data['n_trials'],
            'drift_estimates': drift_estimates,
            'integration_results': integration_results,
            'bayes_results': bayes_results,
            'total_time': total_time,
            'success': True
        }
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        
        return {
            'success': False,
            'error': str(e),
            'total_time': time.time() - start_time
        }

def run_batch_analysis(csv_file='GRT_LBA.csv', max_subjects=5, min_accuracy=0.5):
    """
    æ‰¹æ¬¡åˆ†æå¤šå€‹å—è©¦è€…
    
    Args:
        csv_file: æ•¸æ“šæª”æ¡ˆè·¯å¾‘
        max_subjects: æœ€å¤§åˆ†æå—è©¦è€…æ•¸
        min_accuracy: æœ€ä½æº–ç¢ºç‡è¦æ±‚
    """
    
    print("ğŸš€ æ‰¹æ¬¡è­‰æ“šæ•´åˆåˆ†æ")
    print("=" * 60)
    
    start_time = time.time()
    results = []
    
    try:
        # è¼‰å…¥è³‡æ–™
        df = pd.read_csv(csv_file)
        df = df.dropna(subset=['Response', 'RT', 'Stimulus', 'participant'])
        df = df[(df['RT'] >= 0.1) & (df['RT'] <= 3.0)]
        df = df[df['Response'].isin([0, 1, 2, 3])]
        df = df[df['Stimulus'].isin([0, 1, 2, 3])]
        
        # ç¯©é¸ç¬¦åˆæ¢ä»¶çš„å—è©¦è€…
        analyzer = EvidenceIntegrationComparison()
        suitable_subjects = []
        
        for sid in df['participant'].unique():
            temp_data = analyzer.prepare_subject_data(df, sid)
            if temp_data['accuracy'] >= min_accuracy and temp_data['n_trials'] >= 50:
                suitable_subjects.append({
                    'id': sid,
                    'accuracy': temp_data['accuracy'],
                    'n_trials': temp_data['n_trials']
                })
        
        # æŒ‰æº–ç¢ºç‡æ’åºï¼Œé¸æ“‡æœ€å¥½çš„
        suitable_subjects.sort(key=lambda x: x['accuracy'], reverse=True)
        selected_subjects = suitable_subjects[:max_subjects]
        
        print(f"ğŸ“Š ç¬¦åˆæ¢ä»¶çš„å—è©¦è€…: {len(suitable_subjects)}")
        print(f"   é¸æ“‡åˆ†æ: {len(selected_subjects)} ä½")
        
        # é€ä¸€åˆ†æ
        for i, subject_info in enumerate(selected_subjects, 1):
            print(f"\n{'='*40}")
            print(f"ğŸ“ åˆ†æ {i}/{len(selected_subjects)}: å—è©¦è€… {subject_info['id']}")
            print(f"   é æœŸæº–ç¢ºç‡: {subject_info['accuracy']:.1%}")
            
            result = run_evidence_integration_analysis(
                csv_file, 
                subject_id=subject_info['id'], 
                min_accuracy=min_accuracy
            )
            
            results.append(result)
            
            if result['success']:
                print(f"   âœ… å®Œæˆ: {result['bayes_results']['waic_conclusion'] if result['bayes_results'] else 'ç„¡æ³•åˆ¤æ–·'}")
            else:
                print(f"   âŒ å¤±æ•—: {result['error']}")
        
        # çµ±è¨ˆçµæœ
        successful_results = [r for r in results if r['success']]
        total_time = time.time() - start_time
        
        print(f"\n{'='*60}")
        print("ğŸ‰ æ‰¹æ¬¡åˆ†æå®Œæˆ!")
        print(f"â±ï¸ ç¸½æ™‚é–“: {total_time/60:.1f} åˆ†é˜")
        print(f"âœ… æˆåŠŸç‡: {len(successful_results)}/{len(results)} ({len(successful_results)/len(results)*100:.1f}%)")
        
        if successful_results:
            # çµ±è¨ˆçµè«–
            coactive_count = sum(1 for r in successful_results 
                               if r['bayes_results'] and 'Coactive' in r['bayes_results']['waic_conclusion'])
            parallel_count = sum(1 for r in successful_results 
                               if r['bayes_results'] and 'Parallel' in r['bayes_results']['waic_conclusion'])
            
            print(f"ğŸ† çµè«–çµ±è¨ˆ:")
            print(f"   æ”¯æŒ Coactive: {coactive_count} ä½")
            print(f"   æ”¯æŒ Parallel AND: {parallel_count} ä½")
            
        print("="*60)
        
        return {
            'results': results,
            'successful_count': len(successful_results),
            'total_time': total_time,
            'success': True
        }
        
    except Exception as e:
        print(f"\nâŒ æ‰¹æ¬¡åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        
        return {
            'success': False,
            'error': str(e),
            'total_time': time.time() - start_time
        }

if __name__ == "__main__":
    print("ğŸ¯ è­‰æ“šæ•´åˆå‡è¨­æª¢é©—:")
    print("=" * 40)
    print("é€™å€‹åˆ†æå°‡æœƒ:")
    print("1. ç”¨Single LBAä¼°è¨ˆå·¦å³é€šé“drift rate")
    print("2. æ¸¬è©¦Coactive (ç›¸åŠ ) vs Parallel AND (æœ€å¤§å€¼) å‡è¨­")
    print("3. ç”¨Bayes Factoråˆ¤æ–·å“ªå€‹å‡è¨­æ›´ç¬¦åˆæ•¸æ“š")
    print()
    print("é¸é …:")
    print("1. å–®ä¸€å—è©¦è€…åˆ†æ (è‡ªå‹•é¸æ“‡)")
    print("2. æŒ‡å®šå—è©¦è€…åˆ†æ")
    print("3. æ‰¹æ¬¡åˆ†æ (å¤šå€‹å—è©¦è€…)")
    
    try:
        choice = input("\nè«‹é¸æ“‡ (1-3): ").strip()
        
        if choice == '1':
            print("\nğŸš€ é–‹å§‹å–®ä¸€å—è©¦è€…åˆ†æ (è‡ªå‹•é¸æ“‡æº–ç¢ºç‡æœ€é«˜è€…)...")
            result = run_evidence_integration_analysis()
            
            if result['success']:
                print("\nâœ… åˆ†ææˆåŠŸå®Œæˆ!")
            else:
                print("\nâŒ åˆ†æå¤±æ•—")
                
        elif choice == '2':
            subject_id = int(input("è«‹è¼¸å…¥å—è©¦è€…ID: "))
            print(f"\nğŸš€ é–‹å§‹å—è©¦è€… {subject_id} åˆ†æ...")
            result = run_evidence_integration_analysis(subject_id=subject_id)
            
            if result['success']:
                print("\nâœ… åˆ†ææˆåŠŸå®Œæˆ!")
            else:
                print("\nâŒ åˆ†æå¤±æ•—")
                
        elif choice == '3':
            max_subjects = int(input("è«‹è¼¸å…¥æœ€å¤§åˆ†æå—è©¦è€…æ•¸ (å»ºè­°3-5): ") or "3")
            print(f"\nğŸš€ é–‹å§‹æ‰¹æ¬¡åˆ†æ (æœ€å¤š{max_subjects}ä½å—è©¦è€…)...")
            result = run_batch_analysis(max_subjects=max_subjects)
            
            if result['success']:
                print("\nâœ… æ‰¹æ¬¡åˆ†ææˆåŠŸå®Œæˆ!")
            else:
                print("\nâŒ æ‰¹æ¬¡åˆ†æå¤±æ•—")
        else:
            print("ç„¡æ•ˆé¸æ“‡ï¼Œåˆ†æå–æ¶ˆ")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ åˆ†æè¢«ä½¿ç”¨è€…ä¸­æ–·")
    except Exception as e:
        print(f"\nğŸ’¥ æœªé æœŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
