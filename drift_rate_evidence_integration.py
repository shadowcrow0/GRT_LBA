# drift_rate_evidence_integration.py - åŸºæ–¼Single LBAçš„è­‰æ“šæ•´åˆæ¨¡å‹æ¯”è¼ƒ
# ä½¿ç”¨Bayes Factoræ¯”è¼ƒCoactive vs Parallel ANDå‡è¨­

import numpy as np
import pandas as pd
import pymc as pm
import arviz as az
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from data_utils import DataProcessor
from grt_model_comparison import get_robust_mcmc_config, diagnose_sampling_issues

class EvidenceIntegrationComparison:
    """åŸºæ–¼Single LBAçš„è­‰æ“šæ•´åˆæ¨¡å‹æ¯”è¼ƒå™¨"""
    
    def __init__(self, mcmc_config=None):
        """
        åˆå§‹åŒ–è­‰æ“šæ•´åˆæ¯”è¼ƒå™¨
        
        Args:
            mcmc_config: MCMCé…ç½®å­—å…¸
        """
        
        self.mcmc_config = self._setup_mcmc_config(mcmc_config)
        
        print("âœ… åˆå§‹åŒ–è­‰æ“šæ•´åˆæ¨¡å‹æ¯”è¼ƒå™¨")
        print("   åˆ†ææµç¨‹:")
        print("     1. Single LBA: ä¼°è¨ˆå·¦å³é€šé“å„è‡ªçš„drift rate")
        print("     2. Coactive: è­‰æ“šç›¸åŠ  (drift_left + drift_right)")
        print("     3. Parallel AND: è­‰æ“šå–æœ€å¤§å€¼ (max(drift_left, drift_right))")
        print("     4. Bayes Factor: æ¯”è¼ƒå…©ç¨®æ•´åˆå‡è¨­")
    
    def _setup_mcmc_config(self, user_config):
        """è¨­å®šMCMCé…ç½®"""
        
        default_config = {
            'draws': 500,
            'tune': 500,
            'chains': 2,
            'cores': 1,
            'target_accept': 0.90,
            'max_treedepth': 10,
            'init': 'jitter+adapt_diag',
            'random_seed': 42,
            'progressbar': True,
            'return_inferencedata': True
        }
        
        if user_config:
            default_config.update(user_config)
        
        return default_config
    
    def prepare_subject_data(self, df, subject_id):
        """
        æº–å‚™å—è©¦è€…æ•¸æ“š (æ ¹æ“šä½ çš„GRT_LBA.csvæ¬„ä½çµæ§‹)
        
        ä½ çš„æ•¸æ“šæ¬„ä½:
        - Response: æœ€çµ‚é¸æ“‡ (0-3)
        - RT: åæ‡‰æ™‚é–“
        - participant: å—è©¦è€…ID  
        - Stimulus: åˆºæ¿€é¡å‹ (0-3)
        - Chanel1, Chanel2: å·¦å³é€šé“ä¿¡æ¯
        """
        
        # éæ¿¾å—è©¦è€…è³‡æ–™
        subject_df = df[df['participant'] == subject_id].copy()
        
        if len(subject_df) == 0:
            raise ValueError(f"æ‰¾ä¸åˆ°å—è©¦è€… {subject_id} çš„è³‡æ–™")
        
        # åˆºæ¿€æ˜ å°„ï¼šå°‡åˆºæ¿€ç·¨è™Ÿè½‰ç‚ºå·¦å³é€šé“çš„ç·šæ¢é¡å‹
        stimulus_mapping = {
            0: {'left': 1, 'right': 0},  # å·¦å°è§’ï¼Œå³å‚ç›´
            1: {'left': 1, 'right': 1},  # å·¦å°è§’ï¼Œå³å°è§’
            2: {'left': 0, 'right': 0},  # å·¦å‚ç›´ï¼Œå³å‚ç›´
            3: {'left': 0, 'right': 1}   # å·¦å‚ç›´ï¼Œå³å°è§’
        }
        
        # é¸æ“‡æ˜ å°„ï¼šå°‡é¸æ“‡ç·¨è™Ÿè½‰ç‚ºå·¦å³é€šé“çš„åˆ¤æ–·
        choice_mapping = {
            0: {'left': 1, 'right': 0},  # é¸æ“‡ \|
            1: {'left': 1, 'right': 1},  # é¸æ“‡ \/
            2: {'left': 0, 'right': 0},  # é¸æ“‡ ||
            3: {'left': 0, 'right': 1}   # é¸æ“‡ |/
        }
        
        # åˆ†è§£åˆºæ¿€å’Œé¸æ“‡
        left_stimuli = []
        right_stimuli = []
        left_choices = []
        right_choices = []
        
        for _, row in subject_df.iterrows():
            stimulus = int(row['Stimulus'])
            choice = int(row['Response'])
            
            # åˆ†è§£åˆºæ¿€
            left_stimuli.append(stimulus_mapping[stimulus]['left'])
            right_stimuli.append(stimulus_mapping[stimulus]['right'])
            
            # åˆ†è§£é¸æ“‡
            left_choices.append(choice_mapping[choice]['left'])
            right_choices.append(choice_mapping[choice]['right'])
        
        # è¨ˆç®—æº–ç¢ºç‡
        left_correct = np.array(left_choices) == np.array(left_stimuli)
        right_correct = np.array(right_choices) == np.array(right_stimuli)
        both_correct = left_correct & right_correct
        
        return {
            'subject_id': subject_id,
            'n_trials': len(subject_df),
            'choices': subject_df['Response'].values,
            'rt': subject_df['RT'].values,
            'stimuli': subject_df['Stimulus'].values,
            'left_stimuli': np.array(left_stimuli),
            'right_stimuli': np.array(right_stimuli),
            'left_choices': np.array(left_choices),
            'right_choices': np.array(right_choices),
            'left_correct': left_correct,
            'right_correct': right_correct,
            'accuracy': np.mean(both_correct),
            'left_accuracy': np.mean(left_correct),
            'right_accuracy': np.mean(right_correct)
        }
    
    def step1_estimate_single_lba(self, subject_data):
        """
        æ­¥é©Ÿ1: ä½¿ç”¨Single LBAä¼°è¨ˆå·¦å³é€šé“çš„drift rate
        """
        
        print("\nğŸ“ æ­¥é©Ÿ1: Single LBAä¼°è¨ˆå·¦å³é€šé“drift rate")
        print("-" * 50)
        
        with pm.Model() as single_lba_model:
            
            # === å·¦é€šé“LBAæ¨¡å‹ ===
            left_drift_correct = pm.Gamma('left_drift_correct', alpha=2.5, beta=1.2)
            left_drift_incorrect = pm.Gamma('left_drift_incorrect', alpha=2.0, beta=3.0)
            left_threshold = pm.Gamma('left_threshold', alpha=3.0, beta=3.5)
            left_start_var = pm.Uniform('left_start_var', lower=0.1, upper=0.7)
            left_ndt = pm.Uniform('left_ndt', lower=0.05, upper=0.6)
            left_noise = pm.Gamma('left_noise', alpha=2.5, beta=8.0)
            
            # === å³é€šé“LBAæ¨¡å‹ ===
            right_drift_correct = pm.Gamma('right_drift_correct', alpha=2.5, beta=1.2)
            right_drift_incorrect = pm.Gamma('right_drift_incorrect', alpha=2.0, beta=3.0)
            right_threshold = pm.Gamma('right_threshold', alpha=3.0, beta=3.5)
            right_start_var = pm.Uniform('right_start_var', lower=0.1, upper=0.7)
            right_ndt = pm.Uniform('right_ndt', lower=0.05, upper=0.6)
            right_noise = pm.Gamma('right_noise', alpha=2.5, beta=8.0)
            
            # === æ•¸æ“šæº–å‚™ ===
            left_stimuli = subject_data['left_stimuli']
            left_choices = subject_data['left_choices']
            right_stimuli = subject_data['right_stimuli']
            right_choices = subject_data['right_choices']
            rt = subject_data['rt']
            
            # === è¨ˆç®—lefté€šé“likelihood ===
            left_likelihood = self._compute_side_likelihood(
                left_choices, left_stimuli, rt,
                left_drift_correct, left_drift_incorrect, left_threshold,
                left_start_var, left_ndt, left_noise, 'left'
            )
            
            # === è¨ˆç®—righté€šé“likelihood ===
            right_likelihood = self._compute_side_likelihood(
                right_choices, right_stimuli, rt,
                right_drift_correct, right_drift_incorrect, right_threshold,
                right_start_var, right_ndt, right_noise, 'right'
            )
            
            # === æ·»åŠ åˆ°æ¨¡å‹ ===
            pm.Potential('left_likelihood', left_likelihood)
            pm.Potential('right_likelihood', right_likelihood)
        
        # åŸ·è¡ŒMCMCæ¡æ¨£
        print("   ğŸ² åŸ·è¡ŒSingle LBAæ¡æ¨£...")
        with single_lba_model:
            single_trace = pm.sample(**self.mcmc_config)
        
        # æª¢æŸ¥æ”¶æ–‚
        issues = diagnose_sampling_issues(single_trace)
        if issues:
            print(f"   âš ï¸ Single LBAæ¡æ¨£æœ‰å•é¡Œ: {issues}")
        else:
            print("   âœ… Single LBAæ¡æ¨£æˆåŠŸ")
        
        # æå–drift rateå¾Œé©—åˆ†å¸ƒ
        drift_estimates = self._extract_drift_estimates(single_trace)
        
        return single_lba_model, single_trace, drift_estimates
    
    def step2_test_evidence_integration(self, subject_data, drift_estimates):
        """
        æ­¥é©Ÿ2: ä½¿ç”¨ä¼°è¨ˆçš„drift rateæ¸¬è©¦å…©ç¨®è­‰æ“šæ•´åˆå‡è¨­
        """
        
        print("\nğŸ“ æ­¥é©Ÿ2: æ¸¬è©¦è­‰æ“šæ•´åˆå‡è¨­")
        print("-" * 50)
        
        # 2A. Coactiveæ¨¡å‹
        print("   ğŸ”¬ æ¸¬è©¦ Coactive å‡è¨­ (è­‰æ“šç›¸åŠ )...")
        coactive_model, coactive_trace = self._create_coactive_integration_model(subject_data, drift_estimates)
        
        # 2B. Parallel ANDæ¨¡å‹  
        print("   ğŸ”¬ æ¸¬è©¦ Parallel AND å‡è¨­ (è­‰æ“šå–æœ€å¤§å€¼)...")
        parallel_and_model, parallel_and_trace = self._create_parallel_and_integration_model(subject_data, drift_estimates)
        
        return {
            'coactive_model': coactive_model,
            'coactive_trace': coactive_trace,
            'parallel_and_model': parallel_and_model,
            'parallel_and_trace': parallel_and_trace
        }
    
    def _create_coactive_integration_model(self, subject_data, drift_estimates):
        """å‰µå»ºCoactiveè­‰æ“šæ•´åˆæ¨¡å‹"""
        
        with pm.Model() as coactive_model:
            
            # === ä½¿ç”¨ä¼°è¨ˆçš„drift rateä½œç‚ºå›ºå®šå€¼æˆ–å…ˆé©— ===
            # é€™è£¡æˆ‘å€‘ä½¿ç”¨ä¼°è¨ˆçš„å‡å€¼ä½œç‚ºå…ˆé©—çš„ä¸­å¿ƒ
            left_drift_mean = drift_estimates['left_drift_mean']
            right_drift_mean = drift_estimates['right_drift_mean']
            
            # å››é¸ä¸€æ±ºç­–çš„åƒæ•¸
            choice_threshold = pm.Gamma('coactive_choice_threshold', alpha=2.0, beta=2.0)
            choice_ndt = pm.Uniform('coactive_choice_ndt', lower=0.05, upper=0.3)
            choice_noise = pm.Gamma('coactive_choice_noise', alpha=2.0, beta=5.0)
            
            # === æ ¹æ“šå››é¸ä¸€é¸æ“‡è¨ˆç®—çµ„åˆdrift rate ===
            choices = subject_data['choices']
            rt = subject_data['rt']
            
            # å°æ¯å€‹é¸æ“‡ï¼Œè¨ˆç®—å°æ‡‰çš„Coactive drift rate
            # é¸æ“‡ 0: å·¦å°è§’å³å‚ç›´ -> å·¦diagonal + å³vertical
            # é¸æ“‡ 1: å·¦å°è§’å³å°è§’ -> å·¦diagonal + å³diagonal  
            # é¸æ“‡ 2: å·¦å‚ç›´å³å‚ç›´ -> å·¦vertical + å³vertical
            # é¸æ“‡ 3: å·¦å‚ç›´å³å°è§’ -> å·¦vertical + å³diagonal
            
            coactive_drifts = self._compute_coactive_drift_rates(
                choices, left_drift_mean, right_drift_mean
            )
            
            # === è¨ˆç®—å››é¸ä¸€LBA likelihood ===
            coactive_likelihood = self._compute_choice_likelihood(
                choices, rt, coactive_drifts, choice_threshold, choice_ndt, choice_noise
            )
            
            pm.Potential('coactive_likelihood', coactive_likelihood)
        
        # åŸ·è¡Œæ¡æ¨£
        with coactive_model:
            coactive_trace = pm.sample(**self.mcmc_config)
        
        issues = diagnose_sampling_issues(coactive_trace)
        if not issues:
            print("     âœ… Coactiveæ¨¡å‹æ¡æ¨£æˆåŠŸ")
        else:
            print(f"     âš ï¸ Coactiveæ¨¡å‹æ¡æ¨£å•é¡Œ: {issues}")
        
        return coactive_model, coactive_trace
    
    def _create_parallel_and_integration_model(self, subject_data, drift_estimates):
        """å‰µå»ºParallel ANDè­‰æ“šæ•´åˆæ¨¡å‹"""
        
        with pm.Model() as parallel_and_model:
            
            # === ä½¿ç”¨ä¼°è¨ˆçš„drift rate ===
            left_drift_mean = drift_estimates['left_drift_mean']
            right_drift_mean = drift_estimates['right_drift_mean']
            
            # å››é¸ä¸€æ±ºç­–çš„åƒæ•¸
            choice_threshold = pm.Gamma('parallel_choice_threshold', alpha=2.0, beta=2.0)
            choice_ndt = pm.Uniform('parallel_choice_ndt', lower=0.05, upper=0.3)
            choice_noise = pm.Gamma('parallel_choice_noise', alpha=2.0, beta=5.0)
            
            # === æ ¹æ“šå››é¸ä¸€é¸æ“‡è¨ˆç®—çµ„åˆdrift rate ===
            choices = subject_data['choices']
            rt = subject_data['rt']
            
            # Parallel AND: å–æœ€å¤§å€¼
            parallel_drifts = self._compute_parallel_and_drift_rates(
                choices, left_drift_mean, right_drift_mean
            )
            
            # === è¨ˆç®—å››é¸ä¸€LBA likelihood ===
            parallel_likelihood = self._compute_choice_likelihood(
                choices, rt, parallel_drifts, choice_threshold, choice_ndt, choice_noise
            )
            
            pm.Potential('parallel_likelihood', parallel_likelihood)
        
        # åŸ·è¡Œæ¡æ¨£
        with parallel_and_model:
            parallel_trace = pm.sample(**self.mcmc_config)
        
        issues = diagnose_sampling_issues(parallel_trace)
        if not issues:
            print("     âœ… Parallel ANDæ¨¡å‹æ¡æ¨£æˆåŠŸ")
        else:
            print(f"     âš ï¸ Parallel ANDæ¨¡å‹æ¡æ¨£å•é¡Œ: {issues}")
        
        return parallel_and_model, parallel_trace
    
    def _compute_coactive_drift_rates(self, choices, left_drift_mean, right_drift_mean):
        """
        è¨ˆç®—Coactiveå‡è¨­ä¸‹çš„drift rate (ç›¸åŠ )
        """
        
        # ç°¡åŒ–å‡è¨­ï¼šæ¯å€‹é€šé“å°å‚ç›´ç·šå’Œå°è§’ç·šæœ‰ä¸åŒçš„æ•æ„Ÿåº¦
        left_vertical_strength = left_drift_mean * 0.8    # å·¦é€šé“å°å‚ç›´ç·šçš„å¼·åº¦
        left_diagonal_strength = left_drift_mean * 1.2    # å·¦é€šé“å°å°è§’ç·šçš„å¼·åº¦
        right_vertical_strength = right_drift_mean * 0.8  # å³é€šé“å°å‚ç›´ç·šçš„å¼·åº¦
        right_diagonal_strength = right_drift_mean * 1.2  # å³é€šé“å°å°è§’ç·šçš„å¼·åº¦
        
        # ç‚ºæ¯å€‹é¸æ“‡è¨ˆç®—Coactive drift rate
        drift_choice_0 = left_diagonal_strength + right_vertical_strength    # å·¦\å³|
        drift_choice_1 = left_diagonal_strength + right_diagonal_strength   # å·¦\å³/
        drift_choice_2 = left_vertical_strength + right_vertical_strength   # å·¦|å³|
        drift_choice_3 = left_vertical_strength + right_diagonal_strength   # å·¦|å³/
        
        # æ ¹æ“šå¯¦éš›é¸æ“‡åˆ†é…drift rate
        coactive_drifts = pm.math.switch(
            pm.math.eq(choices, 0), drift_choice_0,
            pm.math.switch(
                pm.math.eq(choices, 1), drift_choice_1,
                pm.math.switch(
                    pm.math.eq(choices, 2), drift_choice_2,
                    drift_choice_3
                )
            )
        )
        
        return pm.math.maximum(coactive_drifts, 0.1)  # ç¢ºä¿æ­£å€¼
    
    def _compute_parallel_and_drift_rates(self, choices, left_drift_mean, right_drift_mean):
        """
        è¨ˆç®—Parallel ANDå‡è¨­ä¸‹çš„drift rate (å–æœ€å¤§å€¼)
        """
        
        # æ¯å€‹é€šé“çš„å¼·åº¦
        left_vertical_strength = left_drift_mean * 0.8
        left_diagonal_strength = left_drift_mean * 1.2
        right_vertical_strength = right_drift_mean * 0.8
        right_diagonal_strength = right_drift_mean * 1.2
        
        # ç‚ºæ¯å€‹é¸æ“‡è¨ˆç®—Parallel AND drift rate (å–æœ€å¤§å€¼)
        drift_choice_0 = pm.math.maximum(left_diagonal_strength, right_vertical_strength)    # å·¦\å³|
        drift_choice_1 = pm.math.maximum(left_diagonal_strength, right_diagonal_strength)   # å·¦\å³/
        drift_choice_2 = pm.math.maximum(left_vertical_strength, right_vertical_strength)   # å·¦|å³|
        drift_choice_3 = pm.math.maximum(left_vertical_strength, right_diagonal_strength)   # å·¦|å³/
        
        # æ ¹æ“šå¯¦éš›é¸æ“‡åˆ†é…drift rate
        parallel_drifts = pm.math.switch(
            pm.math.eq(choices, 0), drift_choice_0,
            pm.math.switch(
                pm.math.eq(choices, 1), drift_choice_1,
                pm.math.switch(
                    pm.math.eq(choices, 2), drift_choice_2,
                    drift_choice_3
                )
            )
        )
        
        return pm.math.maximum(parallel_drifts, 0.1)  # ç¢ºä¿æ­£å€¼
    
    def _compute_choice_likelihood(self, choices, rt, drift_rates, threshold, ndt, noise):
        """
        è¨ˆç®—å››é¸ä¸€é¸æ“‡çš„likelihood
        """
        
        # æ‡‰ç”¨åƒæ•¸ç´„æŸ
        drift_rates = pm.math.maximum(drift_rates, 0.1)
        threshold = pm.math.maximum(threshold, 0.1)
        ndt = pm.math.maximum(ndt, 0.05)
        noise = pm.math.maximum(noise, 0.1)
        
        # è¨ˆç®—æ±ºç­–æ™‚é–“
        decision_time = pm.math.maximum(rt - ndt, 0.01)
        
        # ç°¡åŒ–çš„LBA likelihoodè¨ˆç®—
        # é€™è£¡ä½¿ç”¨ç°¡åŒ–çš„æ­£æ…‹åˆ†ä½ˆè¿‘ä¼¼
        predicted_rt = threshold / drift_rates + ndt
        
        # RT likelihood
        rt_likelihood = pm.math.sum(
            -0.5 * ((rt - predicted_rt) / noise) ** 2 - pm.math.log(noise * pm.math.sqrt(2 * np.pi))
        )
        
        return rt_likelihood
    
    def _extract_drift_estimates(self, trace):
        """æå–drift rateçš„å¾Œé©—ä¼°è¨ˆ"""
        
        summary = az.summary(trace)
        
        return {
            'left_drift_mean': (summary.loc['left_drift_correct', 'mean'] + 
                              summary.loc['left_drift_incorrect', 'mean']) / 2,
            'right_drift_mean': (summary.loc['right_drift_correct', 'mean'] + 
                               summary.loc['right_drift_incorrect', 'mean']) / 2,
            'left_drift_correct': summary.loc['left_drift_correct', 'mean'],
            'left_drift_incorrect': summary.loc['left_drift_incorrect', 'mean'],
            'right_drift_correct': summary.loc['right_drift_correct', 'mean'],
            'right_drift_incorrect': summary.loc['right_drift_incorrect', 'mean']
        }
    
    def _compute_side_likelihood(self, decisions, stimuli, rt, drift_correct, drift_incorrect, 
                               threshold, start_var, ndt, noise, side_name):
        """
        è¨ˆç®—å–®é‚ŠLBA likelihood (ä½¿ç”¨å®Œæ•´çš„LBAå…¬å¼)
        é€™å€‹å‡½æ•¸è¨ˆç®—2é¸æ“‡LBAçš„å°æ•¸ä¼¼ç„¶ï¼Œç”¨æ–¼å·¦å³é€šé“å„è‡ªçš„å‚ç›´ç·švså°è§’ç·šåˆ¤æ–·
        
        Args:
            decisions: æ±ºç­–é™£åˆ— (0=å‚ç›´, 1=å°è§’)  
            stimuli: åˆºæ¿€é™£åˆ— (0=å‚ç›´, 1=å°è§’)
            rt: åæ‡‰æ™‚é–“é™£åˆ—
            drift_correct, drift_incorrect: æ­£ç¢ºå’ŒéŒ¯èª¤çš„drift rates
            threshold, start_var, ndt, noise: LBAåƒæ•¸
            side_name: é€šé“åç¨± (ç”¨æ–¼èª¿è©¦)
        """
        
        from pytensor.tensor import erf
        
        # åƒæ•¸ç´„æŸ
        drift_correct = pm.math.maximum(drift_correct, 0.1)
        drift_incorrect = pm.math.maximum(drift_incorrect, 0.05)
        drift_correct = pm.math.maximum(drift_correct, drift_incorrect + 0.05)
        threshold = pm.math.maximum(threshold, 0.1)
        start_var = pm.math.maximum(start_var, 0.05)
        ndt = pm.math.maximum(ndt, 0.05)
        noise = pm.math.maximum(noise, 0.1)
        
        # è¨ˆç®—æ±ºç­–æ™‚é–“
        decision_time = pm.math.maximum(rt - ndt, 0.01)
        
        # åˆ¤æ–·æ­£ç¢ºæ€§
        stimulus_correct = pm.math.eq(decisions, stimuli)
        
        # è¨­å®šwinnerå’Œloserçš„æ¼‚ç§»ç‡
        v_winner = pm.math.where(stimulus_correct, drift_correct, drift_incorrect)
        v_loser = pm.math.where(stimulus_correct, drift_incorrect, drift_correct)
        
        # ä½¿ç”¨å®Œæ•´çš„LBAå…¬å¼è¨ˆç®—2é¸æ“‡ä¼¼ç„¶
        sqrt_t = pm.math.sqrt(decision_time)
        
        # Winnerç´¯ç©å™¨çš„z-scores
        z1_winner = pm.math.clip(
            (v_winner * decision_time - threshold) / (noise * sqrt_t), 
            -4.5, 4.5
        )
        z2_winner = pm.math.clip(
            (v_winner * decision_time - start_var) / (noise * sqrt_t), 
            -4.5, 4.5
        )
        
        # Loserç´¯ç©å™¨çš„z-score
        z1_loser = pm.math.clip(
            (v_loser * decision_time - threshold) / (noise * sqrt_t), 
            -4.5, 4.5
        )
        
        def safe_normal_cdf(x):
            """å®‰å…¨çš„æ­£æ…‹CDFå‡½æ•¸"""
            x_safe = pm.math.clip(x, -4.5, 4.5)
            return 0.5 * (1 + erf(x_safe / pm.math.sqrt(2)))
        
        def safe_normal_pdf(x):
            """å®‰å…¨çš„æ­£æ…‹PDFå‡½æ•¸"""
            x_safe = pm.math.clip(x, -4.5, 4.5)
            return pm.math.exp(-0.5 * x_safe**2) / pm.math.sqrt(2 * np.pi)
        
        # Winnerçš„ä¼¼ç„¶è¨ˆç®—
        winner_cdf_term = safe_normal_cdf(z1_winner) - safe_normal_cdf(z2_winner)
        winner_pdf_term = (safe_normal_pdf(z1_winner) - safe_normal_pdf(z2_winner)) / (noise * sqrt_t)
        
        # ç¢ºä¿CDFé …ç‚ºæ­£
        winner_cdf_term = pm.math.maximum(winner_cdf_term, 1e-10)
        
        # å®Œæ•´çš„winnerä¼¼ç„¶
        winner_likelihood = pm.math.maximum(
            (v_winner / start_var) * winner_cdf_term + winner_pdf_term / start_var,
            1e-10
        )
        
        # Loserçš„å­˜æ´»æ©Ÿç‡
        loser_survival = pm.math.maximum(1 - safe_normal_cdf(z1_loser), 1e-10)
        
        # è¯åˆä¼¼ç„¶ï¼šwinnerçš„PDF Ã— loserçš„survival
        joint_likelihood = winner_likelihood * loser_survival
        joint_likelihood = pm.math.maximum(joint_likelihood, 1e-12)
        
        # è½‰ç‚ºå°æ•¸ä¼¼ç„¶
        log_likelihood = pm.math.log(joint_likelihood)
        
        # è™•ç†ç„¡æ•ˆå€¼
        is_invalid = (
            pm.math.isnan(log_likelihood) | 
            pm.math.eq(log_likelihood, -np.inf) | 
            pm.math.eq(log_likelihood, np.inf)
        )
        log_likelihood_safe = pm.math.where(is_invalid, -100.0, log_likelihood)
        
        # è£å‰ªæ¥µç«¯å€¼ä¸¦æ±‚å’Œ
        return pm.math.sum(pm.math.clip(log_likelihood_safe, -100.0, 10.0))
    
    def step3_compute_bayes_factors(self, integration_results):
        """
        æ­¥é©Ÿ3: è¨ˆç®—Bayes Factoré€²è¡Œæ¨¡å‹æ¯”è¼ƒ
        """
        
        print("\nğŸ“ æ­¥é©Ÿ3: Bayes Factoræ¨¡å‹æ¯”è¼ƒ")
        print("-" * 50)
        
        coactive_trace = integration_results['coactive_trace']
        parallel_trace = integration_results['parallel_and_trace']
        
        try:
            # è¨ˆç®—WAIC
            coactive_waic = az.waic(coactive_trace)
            parallel_waic = az.waic(parallel_trace)
            
            # è¨ˆç®—LOO
            coactive_loo = az.loo(coactive_trace)
            parallel_loo = az.loo(parallel_trace)
            
            # WAICå·®ç•° (è¿‘ä¼¼Bayes Factor)
            waic_diff = coactive_waic.waic - parallel_waic.waic
            loo_diff = coactive_loo.loo - parallel_loo.loo
            
            # è§£é‡‹çµæœ
            if waic_diff < -2:
                waic_conclusion = "å¼·çƒˆæ”¯æŒ Coactive å‡è¨­"
            elif waic_diff < 0:
                waic_conclusion = "å‚¾å‘æ”¯æŒ Coactive å‡è¨­"
            elif waic_diff > 2:
                waic_conclusion = "å¼·çƒˆæ”¯æŒ Parallel AND å‡è¨­"
            else:
                waic_conclusion = "å‚¾å‘æ”¯æŒ Parallel AND å‡è¨­"
            
            if loo_diff < -2:
                loo_conclusion = "å¼·çƒˆæ”¯æŒ Coactive å‡è¨­"
            elif loo_diff < 0:
                loo_conclusion = "å‚¾å‘æ”¯æŒ Coactive å‡è¨­"
            elif loo_diff > 2:
                loo_conclusion = "å¼·çƒˆæ”¯æŒ Parallel AND å‡è¨­"
            else:
                loo_conclusion = "å‚¾å‘æ”¯æŒ Parallel AND å‡è¨­"
            
            comparison_results = {
                'coactive_waic': coactive_waic.waic,
                'parallel_waic': parallel_waic.waic,
                'waic_diff': waic_diff,
                'waic_conclusion': waic_conclusion,
                'coactive_loo': coactive_loo.loo,
                'parallel_loo': parallel_loo.loo,
                'loo_diff': loo_diff,
                'loo_conclusion': loo_conclusion
            }
            
            print(f"   ğŸ“Š æ¨¡å‹æ¯”è¼ƒçµæœ:")
            print(f"      Coactive WAIC:    {coactive_waic.waic:.2f}")
            print(f"      Parallel WAIC:    {parallel_waic.waic:.2f}")
            print(f"      WAIC å·®ç•°:        {waic_diff:.2f}")
            print(f"      WAIC çµè«–:        {waic_conclusion}")
            print(f"      LOO å·®ç•°:         {loo_diff:.2f}")
            print(f"      LOO çµè«–:         {loo_conclusion}")
            
            return comparison_results
            
        except Exception as e:
            print(f"   âŒ Bayes Factorè¨ˆç®—å¤±æ•—: {e}")
            return None

def run_evidence_integration_analysis(csv_file='GRT_LBA.csv', subject_id=None):
    """
    åŸ·è¡Œå®Œæ•´çš„è­‰æ“šæ•´åˆåˆ†æ
    """
    
    print("ğŸš€ è­‰æ“šæ•´åˆå‡è¨­æª¢é©—åˆ†æ")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        # è¼‰å…¥è³‡æ–™
        processor = DataProcessor()
        df = processor.load_and_clean_data(csv_file)
        
        # é¸æ“‡å—è©¦è€…
        if subject_id is None:
            subject_id = df['participant'].iloc[0]
            print(f"è‡ªå‹•é¸æ“‡å—è©¦è€…: {subject_id}")
        
        # å‰µå»ºåˆ†æå™¨
        analyzer = EvidenceIntegrationComparison()
        
        # æº–å‚™æ•¸æ“š
        subject_data = analyzer.prepare_subject_data(df, subject_id)
        print(f"å—è©¦è€… {subject_id}: {subject_data['n_trials']} trials, æº–ç¢ºç‡ {subject_data['accuracy']:.1%}")
        
        # æ­¥é©Ÿ1: Single LBAä¼°è¨ˆ
        single_model, single_trace, drift_estimates = analyzer.step1_estimate_single_lba(subject_data)
        
        # æ­¥é©Ÿ2: è­‰æ“šæ•´åˆæ¸¬è©¦
        integration_results = analyzer.step2_test_evidence_integration(subject_data, drift_estimates)
        
        # æ­¥é©Ÿ3: Bayes Factoræ¯”è¼ƒ
        bayes_results = analyzer.step3_compute_bayes_factors(integration_results)
        
        total_time = time.time() - start_time
        
        print(f"\n{'='*60}")
        print("ğŸ‰ è­‰æ“šæ•´åˆåˆ†æå®Œæˆ!")
        print(f"â±ï¸ ç¸½æ™‚é–“: {total_time/60:.1f} åˆ†é˜")
        print(f"ğŸ† æœ€çµ‚çµè«–: {bayes_results['waic_conclusion'] if bayes_results else 'ç„¡æ³•åˆ¤æ–·'}")
        print("="*60)
        
        return {
            'subject_id': subject_id,
            'drift_estimates': drift_estimates,
            'integration_results': integration_results,
            'bayes_results': bayes_results,
            'total_time': total_time,
            'success': True
        }
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        
        return {
            'success': False,
            'error': str(e),
            'total_time': time.time() - start_time
        }

if __name__ == "__main__":
    print("ğŸ¯ è­‰æ“šæ•´åˆå‡è¨­æª¢é©—:")
    print("=" * 40)
    print("é€™å€‹åˆ†æå°‡æœƒ:")
    print("1. ç”¨Single LBAä¼°è¨ˆå·¦å³é€šé“drift rate")
    print("2. æ¸¬è©¦Coactive (ç›¸åŠ ) vs Parallel AND (æœ€å¤§å€¼) å‡è¨­")
    print("3. ç”¨Bayes Factoråˆ¤æ–·å“ªå€‹å‡è¨­æ›´ç¬¦åˆæ•¸æ“š")
    
    try:
        choice = input("\næ˜¯å¦é–‹å§‹åˆ†æ? (y/n): ").strip().lower()
        
        if choice == 'y':
            print("\nğŸš€ é–‹å§‹è­‰æ“šæ•´åˆåˆ†æ...")
            result = run_evidence_integration_analysis()
            
            if result['success']:
                print("\nâœ… åˆ†ææˆåŠŸå®Œæˆ!")
            else:
                print("\nâŒ åˆ†æå¤±æ•—")
        else:
            print("åˆ†æå–æ¶ˆ")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ åˆ†æè¢«ä½¿ç”¨è€…ä¸­æ–·")
    except Exception as e:
        print(f"\nğŸ’¥ æœªé æœŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
