# -*- coding: utf-8 -*-
"""
sequential_model_improved.py - æ”¹é€²çš„åºåˆ—è™•ç†ä¸»æ¨¡å‹
åƒè€ƒ Matlab å°ˆæ¡ˆçš„åƒæ•¸è™•ç†æ–¹å¼
"""

import numpy as np
import pymc as pm
import pytensor.tensor as pt
from typing import Dict, Optional, Tuple
from single_side_lba import SingleSideLBA
from four_choice_lba import FourChoiceLBA

class SequentialLBA:
    """åºåˆ—è™•ç†LBAä¸»æ¨¡å‹ - æ”¹é€²ç‰ˆ"""
    
    def __init__(self, first_side='left', time_split_ratio=0.6):
        """
        åˆå§‹åŒ–åºåˆ—è™•ç†LBAæ¨¡å‹
        
        Args:
            first_side: é¦–å…ˆè™•ç†çš„é€šé“ ('left' æˆ– 'right')
            time_split_ratio: ç¬¬ä¸€éšæ®µä½”ç¸½RTçš„æ¯”ä¾‹ (0-1)
        """
        
        self.first_side = first_side
        self.second_side = 'right' if first_side == 'left' else 'left'
        self.time_split_ratio = time_split_ratio
        
        # åˆå§‹åŒ–å­æ¨¡çµ„
        self.first_side_lba = SingleSideLBA(self.first_side)
        self.second_side_lba = SingleSideLBA(self.second_side)
        self.integration_lba = FourChoiceLBA()
        
        # æ”¶é›†æ‰€æœ‰åƒæ•¸åç¨±
        self.all_param_names = (
            self.first_side_lba.param_names + 
            self.second_side_lba.param_names +
            self.integration_lba.param_names
        )
        
        # è¨­å®šåƒæ•¸è½‰æ›å‡½æ•¸ï¼ˆåƒè€ƒ Matlab çš„ transformSamplesï¼‰
        self.param_transforms = self._setup_parameter_transforms()
        
        print(f"âœ… åˆå§‹åŒ–åºåˆ—è™•ç†LBAæ¨¡å‹")
        print(f"   è™•ç†é †åº: {self.first_side} â†’ {self.second_side}")
        print(f"   æ™‚é–“åˆ†å‰²: {self.time_split_ratio:.1%} / {1-self.time_split_ratio:.1%}")
        print(f"   ç¸½åƒæ•¸æ•¸: {len(self.all_param_names)}")
    
    def _setup_parameter_transforms(self):
        """è¨­å®šåƒæ•¸è½‰æ›å‡½æ•¸ï¼ˆåƒè€ƒ Matlab loadParmSettings.mï¼‰"""
        
        transforms = {}
        
        # æ¼‚ç§»ç‡åƒæ•¸ - ä½¿ç”¨å°æ•¸è½‰æ›ç¢ºä¿æ­£å€¼
        for side in [self.first_side, self.second_side]:
            transforms[f'{side}_drift_correct'] = {
                'raw_to_natural': lambda x: pt.exp(x),
                'natural_to_raw': lambda x: pt.log(x),
                'constraint': lambda x: pt.maximum(x, 0.1)
            }
            transforms[f'{side}_drift_incorrect'] = {
                'raw_to_natural': lambda x: pt.exp(x), 
                'natural_to_raw': lambda x: pt.log(x),
                'constraint': lambda x: pt.maximum(x, 0.05)
            }
            
            # é–¾å€¼åƒæ•¸
            transforms[f'{side}_threshold'] = {
                'raw_to_natural': lambda x: pt.exp(x),
                'natural_to_raw': lambda x: pt.log(x),
                'constraint': lambda x: pt.maximum(x, 0.1)
            }
            
            # èµ·å§‹é»è®Šç•°
            transforms[f'{side}_start_var'] = {
                'raw_to_natural': lambda x: pt.exp(x),
                'natural_to_raw': lambda x: pt.log(x),
                'constraint': lambda x: pt.maximum(x, 0.05)
            }
            
            # éæ±ºç­–æ™‚é–“
            transforms[f'{side}_ndt'] = {
                'raw_to_natural': lambda x: pt.exp(x),
                'natural_to_raw': lambda x: pt.log(x),
                'constraint': lambda x: pt.clip(x, 0.05, 0.8)
            }
            
            # å™ªéŸ³åƒæ•¸
            transforms[f'{side}_noise'] = {
                'raw_to_natural': lambda x: pt.exp(x),
                'natural_to_raw': lambda x: pt.log(x),
                'constraint': lambda x: pt.maximum(x, 0.1)
            }
        
        # æ•´åˆå±¤åƒæ•¸
        for i in range(4):
            transforms[f'integration_drift_{i}'] = {
                'raw_to_natural': lambda x: pt.exp(x),
                'natural_to_raw': lambda x: pt.log(x),
                'constraint': lambda x: pt.maximum(x, 0.1)
            }
            
        # å…¶ä»–æ•´åˆå±¤åƒæ•¸
        for param in ['integration_threshold', 'integration_start_var', 
                     'integration_ndt', 'integration_noise']:
            transforms[param] = {
                'raw_to_natural': lambda x: pt.exp(x),
                'natural_to_raw': lambda x: pt.log(x),
                'constraint': lambda x: pt.maximum(x, 0.05)
            }
        
        return transforms
    
    def build_model(self, subject_data):
        """
        å»ºæ§‹å®Œæ•´çš„åºåˆ—è™•ç†PyMCæ¨¡å‹ - æ”¹é€²ç‰ˆ
        """
        
        print(f"ğŸ”§ å»ºæ§‹åºåˆ—è™•ç†æ¨¡å‹...")
        print(f"   å—è©¦è€…: {subject_data['subject_id']}")
        print(f"   è©¦é©—æ•¸: {subject_data['n_trials']}")
        
        with pm.Model() as sequential_model:
            
            # ========================================
            # 1. å®šç¾©åŸå§‹åƒæ•¸å…ˆé©—åˆ†å¸ƒï¼ˆåœ¨è½‰æ›å¾Œçš„ç©ºé–“ï¼‰
            # ========================================
            
            raw_params = {}
            
            # ç¬¬ä¸€é€šé“åƒæ•¸ï¼ˆåœ¨å°æ•¸ç©ºé–“å®šç¾©ï¼‰
            raw_params[f'{self.first_side}_drift_correct_raw'] = pm.Normal(
                f'{self.first_side}_drift_correct_raw', mu=np.log(1.5), sigma=0.2
            )
            raw_params[f'{self.first_side}_drift_incorrect_raw'] = pm.Normal(
                f'{self.first_side}_drift_incorrect_raw', mu=np.log(0.8), sigma=0.2
            )
            raw_params[f'{self.first_side}_threshold_raw'] = pm.Normal(
                f'{self.first_side}_threshold_raw', mu=np.log(1.0), sigma=0.2
            )
            raw_params[f'{self.first_side}_start_var_raw'] = pm.Normal(
                f'{self.first_side}_start_var_raw', mu=np.log(0.3), sigma=0.3
            )
            raw_params[f'{self.first_side}_ndt_raw'] = pm.Normal(
                f'{self.first_side}_ndt_raw', mu=np.log(0.2), sigma=0.2
            )
            raw_params[f'{self.first_side}_noise_raw'] = pm.Normal(
                f'{self.first_side}_noise_raw', mu=np.log(0.3), sigma=0.3
            )
            
            # ç¬¬äºŒé€šé“åƒæ•¸
            raw_params[f'{self.second_side}_drift_correct_raw'] = pm.Normal(
                f'{self.second_side}_drift_correct_raw', mu=np.log(1.5), sigma=0.2
            )
            raw_params[f'{self.second_side}_drift_incorrect_raw'] = pm.Normal(
                f'{self.second_side}_drift_incorrect_raw', mu=np.log(0.8), sigma=0.2
            )
            raw_params[f'{self.second_side}_threshold_raw'] = pm.Normal(
                f'{self.second_side}_threshold_raw', mu=np.log(1.0), sigma=0.2
            )
            raw_params[f'{self.second_side}_start_var_raw'] = pm.Normal(
                f'{self.second_side}_start_var_raw', mu=np.log(0.3), sigma=0.3
            )
            raw_params[f'{self.second_side}_ndt_raw'] = pm.Normal(
                f'{self.second_side}_ndt_raw', mu=np.log(0.2), sigma=0.2
            )
            raw_params[f'{self.second_side}_noise_raw'] = pm.Normal(
                f'{self.second_side}_noise_raw', mu=np.log(0.3), sigma=0.3
            )
            
            # æ•´åˆå±¤åƒæ•¸
            for i in range(4):
                raw_params[f'integration_drift_{i}_raw'] = pm.Normal(
                    f'integration_drift_{i}_raw', mu=np.log(1.0), sigma=0.2
                )
            
            raw_params['integration_threshold_raw'] = pm.Normal(
                'integration_threshold_raw', mu=np.log(0.8), sigma=0.2
            )
            raw_params['integration_start_var_raw'] = pm.Normal(
                'integration_start_var_raw', mu=np.log(0.2), sigma=0.3
            )
            raw_params['integration_ndt_raw'] = pm.Normal(
                'integration_ndt_raw', mu=np.log(0.15), sigma=0.2
            )
            raw_params['integration_noise_raw'] = pm.Normal(
                'integration_noise_raw', mu=np.log(0.25), sigma=0.3
            )
            
            # ========================================
            # 2. è½‰æ›åˆ°è‡ªç„¶åƒæ•¸ç©ºé–“ä¸¦æ‡‰ç”¨ç´„æŸ
            # ========================================
            
            natural_params = {}
            
            # ç¬¬ä¸€é€šé“åƒæ•¸è½‰æ›
            first_side_params = self._transform_side_params(
                raw_params, self.first_side, natural_params
            )
            
            # ç¬¬äºŒé€šé“åƒæ•¸è½‰æ›
            second_side_params = self._transform_side_params(
                raw_params, self.second_side, natural_params
            )
            
            # æ•´åˆå±¤åƒæ•¸è½‰æ›
            integration_params = self._transform_integration_params(
                raw_params, natural_params
            )
            
            # ========================================
            # 3. æº–å‚™è³‡æ–™å¼µé‡
            # ========================================
            
            # è½‰æ›ç‚ºPyTensorå¼µé‡
            final_choices = pt.as_tensor_variable(subject_data['choices'], dtype='int32')
            rt_total = pt.as_tensor_variable(subject_data['rt'], dtype='float64')
            
            first_stimuli = pt.as_tensor_variable(
                subject_data[f'{self.first_side}_stimuli'], dtype='int32'
            )
            first_choices = pt.as_tensor_variable(
                subject_data[f'{self.first_side}_choices'], dtype='int32'
            )
            
            second_stimuli = pt.as_tensor_variable(
                subject_data[f'{self.second_side}_stimuli'], dtype='int32'
            )
            second_choices = pt.as_tensor_variable(
                subject_data[f'{self.second_side}_choices'], dtype='int32'
            )
            
            # ========================================
            # 4. æ™‚é–“åˆ†å‰²
            # ========================================
            
            rt_first = rt_total * self.time_split_ratio
            rt_second = rt_total * (1 - self.time_split_ratio)
            
            # ========================================
            # 5. è¨ˆç®—ä¼¼ç„¶å‡½æ•¸
            # ========================================
            
            # ç¬¬ä¸€é€šé“ä¼¼ç„¶
            first_likelihood = self._compute_side_likelihood(
                first_choices, first_stimuli, rt_first, first_side_params
            )
            
            # ç¬¬äºŒé€šé“ä¼¼ç„¶
            second_likelihood = self._compute_side_likelihood(
                second_choices, second_stimuli, rt_second, second_side_params
            )
            
            # è­‰æ“šæ•´åˆ
            evidence_inputs = self._compute_evidence_combination_improved(
                first_side_params, second_side_params,
                first_stimuli, first_choices,
                second_stimuli, second_choices,
                subject_data['n_trials']
            )
            
            # æ•´åˆå±¤ä¼¼ç„¶
            integration_likelihood = self._compute_integration_likelihood(
                final_choices, evidence_inputs, rt_second, integration_params
            )
            
            # ========================================
            # 6. æ·»åŠ ä¼¼ç„¶åˆ°æ¨¡å‹
            # ========================================
            
            pm.Potential('first_side_likelihood', first_likelihood)
            pm.Potential('second_side_likelihood', second_likelihood)
            pm.Potential('integration_likelihood', integration_likelihood)
            
            # è¨ºæ–·è®Šæ•¸
            pm.Deterministic('total_likelihood',
                           first_likelihood + second_likelihood + integration_likelihood)
        
        print(f"âœ… æ”¹é€²æ¨¡å‹å»ºæ§‹å®Œæˆ")
        print(f"   è‡ªç”±åƒæ•¸: {len(sequential_model.free_RVs)}")
        
        return sequential_model
    
    def _transform_side_params(self, raw_params, side_name, natural_params):
        """è½‰æ›å–®é‚Šåƒæ•¸"""
        
        params = {}
        
        # æ‡‰ç”¨æŒ‡æ•¸è½‰æ›å’Œç´„æŸ
        drift_correct_raw = raw_params[f'{side_name}_drift_correct_raw']
        drift_incorrect_raw = raw_params[f'{side_name}_drift_incorrect_raw']
        
        drift_correct = pt.maximum(pt.exp(drift_correct_raw), 0.1)
        drift_incorrect = pt.maximum(pt.exp(drift_incorrect_raw), 0.05)
        
        # ç¢ºä¿æ­£ç¢ºæ¼‚ç§»ç‡ > éŒ¯èª¤æ¼‚ç§»ç‡
        drift_correct = pt.maximum(drift_correct, drift_incorrect + 0.05)
        
        params[f'{side_name}_drift_correct'] = drift_correct
        params[f'{side_name}_drift_incorrect'] = drift_incorrect
        
        # å…¶ä»–åƒæ•¸
        params[f'{side_name}_threshold'] = pt.maximum(
            pt.exp(raw_params[f'{side_name}_threshold_raw']), 0.1
        )
        params[f'{side_name}_start_var'] = pt.maximum(
            pt.exp(raw_params[f'{side_name}_start_var_raw']), 0.05
        )
        params[f'{side_name}_ndt'] = pt.clip(
            pt.exp(raw_params[f'{side_name}_ndt_raw']), 0.05, 0.8
        )
        params[f'{side_name}_noise'] = pt.maximum(
            pt.exp(raw_params[f'{side_name}_noise_raw']), 0.1
        )
        
        return params
    
    def _transform_integration_params(self, raw_params, natural_params):
        """è½‰æ›æ•´åˆå±¤åƒæ•¸"""
        
        params = {}
        
        # å››å€‹é¸é …çš„æ¼‚ç§»ç‡
        for i in range(4):
            params[f'integration_drift_{i}'] = pt.maximum(
                pt.exp(raw_params[f'integration_drift_{i}_raw']), 0.1
            )
        
        # å…¶ä»–åƒæ•¸
        params['integration_threshold'] = pt.maximum(
            pt.exp(raw_params['integration_threshold_raw']), 0.1
        )
        params['integration_start_var'] = pt.maximum(
            pt.exp(raw_params['integration_start_var_raw']), 0.05
        )
        params['integration_ndt'] = pt.clip(
            pt.exp(raw_params['integration_ndt_raw']), 0.05, 0.3
        )
        params['integration_noise'] = pt.maximum(
            pt.exp(raw_params['integration_noise_raw']), 0.1
        )
        
        return params
    
    def _compute_side_likelihood(self, decisions, stimuli, rt, params):
        """è¨ˆç®—å–®é‚Šä¼¼ç„¶å‡½æ•¸ - æ”¹é€²ç‰ˆ"""
        
        side_name = list(params.keys())[0].split('_')[0]
        
        drift_correct = params[f'{side_name}_drift_correct']
        drift_incorrect = params[f'{side_name}_drift_incorrect']
        threshold = params[f'{side_name}_threshold']
        start_var = params[f'{side_name}_start_var']
        ndt = params[f'{side_name}_ndt']
        noise = params[f'{side_name}_noise']
        
        # è¨ˆç®—æ±ºç­–æ™‚é–“
        decision_time = pt.maximum(rt - ndt, 0.01)
        
        # åˆ¤æ–·æ­£ç¢ºæ€§
        is_correct = pt.eq(decisions, stimuli)
        
        # è¨­å®šwinnerå’Œloseræ¼‚ç§»ç‡
        v_winner = pt.where(is_correct, drift_correct, drift_incorrect)
        v_loser = pt.where(is_correct, drift_incorrect, drift_correct)
        
        # è¨ˆç®—LBAå¯†åº¦
        return self._compute_lba_likelihood(
            decision_time, v_winner, v_loser, threshold, start_var, noise
        )
    
    def _compute_lba_likelihood(self, t, v_winner, v_loser, threshold, start_var, noise):
        """æ”¹é€²çš„LBAä¼¼ç„¶è¨ˆç®—"""
        
        from pytensor.tensor import erf
        
        sqrt_t = pt.sqrt(t)
        
        def safe_normal_cdf(x):
            return 0.5 * (1 + erf(pt.clip(x, -4.5, 4.5) / pt.sqrt(2)))
        
        def safe_normal_pdf(x):
            x_clipped = pt.clip(x, -4.5, 4.5)
            return pt.exp(-0.5 * x_clipped**2) / pt.sqrt(2 * pt.pi)
        
        # Winnerç´¯ç©å™¨
        z1_winner = (v_winner * t - threshold) / (noise * sqrt_t)
        z2_winner = (v_winner * t - start_var) / (noise * sqrt_t)
        
        winner_cdf = safe_normal_cdf(z1_winner) - safe_normal_cdf(z2_winner)
        winner_pdf = (safe_normal_pdf(z1_winner) - safe_normal_pdf(z2_winner)) / (noise * sqrt_t)
        
        winner_density = pt.maximum(
            (v_winner / start_var) * pt.maximum(winner_cdf, 1e-10) + winner_pdf / start_var,
            1e-10
        )
        
        # Loserå­˜æ´»æ©Ÿç‡
        z1_loser = (v_loser * t - threshold) / (noise * sqrt_t)
        loser_survival = pt.maximum(1 - safe_normal_cdf(z1_loser), 1e-10)
        
        # è¯åˆä¼¼ç„¶
        joint_likelihood = winner_density * loser_survival
        joint_likelihood = pt.maximum(joint_likelihood, 1e-12)
        
        log_likelihood = pt.log(joint_likelihood)
        log_likelihood = pt.clip(log_likelihood, -100.0, 10.0)
        
        return pt.sum(log_likelihood)
    
    def _compute_evidence_combination_improved(self, first_params, second_params,
                                             first_stimuli, first_choices,
                                             second_stimuli, second_choices,
                                             n_trials):
        """æ”¹é€²çš„è­‰æ“šçµ„åˆè¨ˆç®—"""
        
        # ä½¿ç”¨åƒæ•¸çš„æœŸæœ›å€¼ä¾†é¿å…å¼µé‡å½¢ç‹€å•é¡Œ
        first_correct_drift = first_params[f'{self.first_side}_drift_correct']
        first_incorrect_drift = first_params[f'{self.first_side}_drift_incorrect']
        second_correct_drift = second_params[f'{self.second_side}_drift_correct']
        second_incorrect_drift = second_params[f'{self.second_side}_drift_incorrect']
        
        # è¨ˆç®—å¹³å‡è­‰æ“šå¼·åº¦
        first_evidence_base = (first_correct_drift + first_incorrect_drift) / 2
        second_evidence_base = (second_correct_drift + second_incorrect_drift) / 2
        
        # è™•ç†é †åºæ¬Šé‡
        if self.first_side == 'left':
            left_evidence = first_evidence_base * 1.1
            right_evidence = second_evidence_base * 1.0
        else:
            left_evidence = second_evidence_base * 1.0
            right_evidence = first_evidence_base * 1.1
        
        # è¨ˆç®—å››å€‹é¸é …çš„è­‰æ“šå¼·åº¦
        evidence_inputs = {
            'choice_0': left_evidence * 0.8 + right_evidence * 0.2,  # å·¦å°è§’å³å‚ç›´
            'choice_1': left_evidence * 0.8 + right_evidence * 0.8,  # å·¦å°è§’å³å°è§’
            'choice_2': left_evidence * 0.2 + right_evidence * 0.2,  # å·¦å‚ç›´å³å‚ç›´
            'choice_3': left_evidence * 0.2 + right_evidence * 0.8   # å·¦å‚ç›´å³å°è§’
        }
        
        return evidence_inputs
    
    def _compute_integration_likelihood(self, choices, evidence_inputs, rt, params):
        """è¨ˆç®—æ•´åˆå±¤ä¼¼ç„¶"""
        
        # åŸºç¤æ¼‚ç§»ç‡
        base_drifts = [
            params[f'integration_drift_{i}'] for i in range(4)
        ]
        
        threshold = params['integration_threshold']
        start_var = params['integration_start_var']
        ndt = params['integration_ndt']
        noise = params['integration_noise']
        
        # èª¿æ•´æ¼‚ç§»ç‡
        adjusted_drifts = []
        for i, base_drift in enumerate(base_drifts):
            evidence_boost = evidence_inputs[f'choice_{i}']
            adjusted_drift = base_drift * (1.0 + evidence_boost * 0.3)
            adjusted_drifts.append(pt.maximum(adjusted_drift, 0.1))
        
        # è¨ˆç®—æ±ºç­–æ™‚é–“
        decision_time = pt.maximum(rt - ndt, 0.01)
        
        # è¨ˆç®—å››é¸ä¸€LBAä¼¼ç„¶
        return self._compute_4choice_lba_likelihood(
            choices, decision_time, adjusted_drifts, threshold, start_var, noise
        )
    
    def _compute_4choice_lba_likelihood(self, choices, t, drifts, threshold, start_var, noise):
        """å››é¸ä¸€LBAä¼¼ç„¶è¨ˆç®—"""
        
        # è¨ˆç®—æ¯å€‹é¸é …çš„å¯†åº¦å’Œå­˜æ´»å‡½æ•¸
        densities = []
        survivals = []
        
        for drift in drifts:
            density = self._compute_single_lba_density(t, drift, threshold, start_var, noise)
            survival = self._compute_single_lba_survival(t, drift, threshold, start_var, noise)
            densities.append(density)
            survivals.append(survival)
        
        # è¨ˆç®—æ¯å€‹é¸é …çš„å®Œæ•´ä¼¼ç„¶
        likelihoods = []
        for i in range(4):
            other_survivals = [survivals[j] for j in range(4) if j != i]
            likelihood = densities[i]
            for survival in other_survivals:
                likelihood = likelihood * survival
            likelihoods.append(likelihood)
        
        # æ ¹æ“šå¯¦éš›é¸æ“‡é¸å–å°æ‡‰çš„ä¼¼ç„¶
        trial_likelihoods = pt.zeros_like(t)
        for i in range(4):
            mask = pt.eq(choices, i)
            trial_likelihoods = trial_likelihoods + mask * likelihoods[i]
        
        # ç¢ºä¿æ­£å€¼ä¸¦å–å°æ•¸
        trial_likelihoods = pt.maximum(trial_likelihoods, 1e-12)
        log_likelihoods = pt.log(trial_likelihoods)
        
        return pt.sum(log_likelihoods)
    
    def _compute_single_lba_density(self, t, drift, threshold, start_var, noise):
        """å–®ä¸€ç´¯ç©å™¨LBAå¯†åº¦"""
        
        from pytensor.tensor import erf
        
        sqrt_t = pt.sqrt(t)
        
        z1 = pt.clip((drift * t - threshold) / (noise * sqrt_t), -4.5, 4.5)
        z2 = pt.clip((drift * t - start_var) / (noise * sqrt_t), -4.5, 4.5)
        
        def safe_normal_cdf(x):
            return 0.5 * (1 + erf(x / pt.sqrt(2)))
        
        def safe_normal_pdf(x):
            return pt.exp(-0.5 * x**2) / pt.sqrt(2 * pt.pi)
        
        cdf_term = safe_normal_cdf(z1) - safe_normal_cdf(z2)
        pdf_term = (safe_normal_pdf(z1) - safe_normal_pdf(z2)) / (noise * sqrt_t)
        
        cdf_term = pt.maximum(cdf_term, 1e-10)
        
        density = pt.maximum(
            (drift / start_var) * cdf_term + pdf_term / start_var,
            1e-10
        )
        
        return density
    
    def _compute_single_lba_survival(self, t, drift, threshold, start_var, noise):
        """å–®ä¸€ç´¯ç©å™¨LBAå­˜æ´»å‡½æ•¸"""
        
        from pytensor.tensor import erf
        
        sqrt_t = pt.sqrt(t)
        z1 = pt.clip((drift * t - threshold) / (noise * sqrt_t), -4.5, 4.5)
        
        def safe_normal_cdf(x):
            return 0.5 * (1 + erf(x / pt.sqrt(2)))
        
        survival = pt.maximum(1 - safe_normal_cdf(z1), 1e-10)
        return survival
    
    def get_model_info(self):
        """ç²å¾—æ¨¡å‹è³‡è¨Šæ‘˜è¦"""
        
        return {
            'model_type': 'sequential_lba_improved',
            'first_side': self.first_side,
            'second_side': self.second_side,
            'time_split_ratio': self.time_split_ratio,
            'total_parameters': len(self.all_param_names),
            'parameter_names': self.all_param_names,
            'has_parameter_transforms': True,
            'transform_functions': list(self.param_transforms.keys())
        }

# ä¾¿åˆ©å‡½æ•¸
def create_improved_sequential_model(first_side='left', time_split_ratio=0.6):
    """å‰µå»ºæ”¹é€²çš„åºåˆ—è™•ç†LBAæ¨¡å‹"""
    return SequentialLBA(first_side, time_split_ratio)

def test_improved_sequential_model():
    """æ¸¬è©¦æ”¹é€²çš„åºåˆ—æ¨¡å‹"""
    
    print("ğŸ§ª æ¸¬è©¦æ”¹é€²çš„åºåˆ—è™•ç†æ¨¡å‹...")
    
    try:
        # å‰µå»ºæ¸¬è©¦è³‡æ–™
        n_trials = 50
        np.random.seed(42)
        
        test_subject_data = {
            'subject_id': 999,
            'n_trials': n_trials,
            'choices': np.random.choice([0, 1, 2, 3], size=n_trials),
            'rt': np.random.uniform(0.3, 1.5, size=n_trials),
            'left_stimuli': np.random.choice([0, 1], size=n_trials),
            'left_choices': np.random.choice([0, 1], size=n_trials),
            'right_stimuli': np.random.choice([0, 1], size=n_trials),
            'right_choices': np.random.choice([0, 1], size=n_trials),
            'accuracy': 0.75
        }
        
        # å‰µå»ºæ”¹é€²çš„åºåˆ—æ¨¡å‹
        seq_model = SequentialLBA(first_side='left', time_split_ratio=0.6)
        
        # ç²å¾—æ¨¡å‹è³‡è¨Š
        model_info = seq_model.get_model_info()
        print(f"   æ¨¡å‹é¡å‹: {model_info['model_type']}")
        print(f"   ç¸½åƒæ•¸æ•¸: {model_info['total_parameters']}")
        print(f"   åƒæ•¸è½‰æ›: {model_info['has_parameter_transforms']}")
        
        # å˜—è©¦å»ºæ§‹PyMCæ¨¡å‹
        print("   æ¸¬è©¦æ”¹é€²çš„PyMCæ¨¡å‹å»ºæ§‹...")
        pymc_model = seq_model.build_model(test_subject_data)
        
        # æª¢æŸ¥æ¨¡å‹åŸºæœ¬æ€§è³ª
        print(f"   è‡ªç”±åƒæ•¸æ•¸é‡: {len(pymc_model.free_RVs)}")
        
        # æ¸¬è©¦æ¨¡å‹ç·¨è­¯
        with pymc_model:
            test_point = pymc_model.initial_point()
            log_prob = pymc_model.compile_logp()(test_point)
            print(f"   æ¸¬è©¦å°æ•¸æ©Ÿç‡: {log_prob:.2f}")
            
            if np.isfinite(log_prob):
                print("   âœ… æ”¹é€²æ¨¡å‹ç·¨è­¯æˆåŠŸ")
            else:
                print("   âš ï¸ è­¦å‘Š: æ¨¡å‹åˆå§‹å°æ•¸æ©Ÿç‡ç„¡æ•ˆ")
        
        print("âœ… æ”¹é€²çš„åºåˆ—æ¨¡å‹æ¸¬è©¦æˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"âŒ æ”¹é€²çš„åºåˆ—æ¨¡å‹æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_improved_sequential_model()
